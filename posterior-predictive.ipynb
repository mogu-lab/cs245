{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Inference: Posterior Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some helper functions (please ignore this!)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** For safety-critical applications of ML, it's important that our model captures two notions of uncertainty. Aleatoric uncertainty captures inherent stochasticity in the system. In contrast, epistemic uncertainty is uncertainty over possible *models* that could have fit the data. Multiple models can fit the data when we have a lack of data and/or a lack of mechanistic understanding of the system. We realized that fitting models using the MLE only captured aleatoric uncertainty. To additionally capture epistemic, we therefore had to rethink our modeling paradigm. Using Bayes' rule, we were able to obtain a *distribution* over model parameters given the data, $p(\\theta \\mathcal{D})$ (the posterior). Using `NumPyro`, we sampled from the posterior to obtain a diversity of models that fit the data. We interpreted a greater diversity of models indicated higher epistemic ucnertainty. \n",
    "\n",
    "**Challenge:** Now that we have a posterior over model parameters, we can capture *epistemic* uncertainty. But how do we use this diverse set of models to (1) make predictions, and (2) compute the log-likelihood (for evaluation)? To do this, we will derive the *posterior predictive*, a distribution that translates a distribution over parameters to a distribution over data. This distributions can then be used to make predictions and evaluate the log-likelihood.\n",
    "\n",
    "**Outline:** \n",
    "* Provide intuition for the posterior predictive\n",
    "* Derive the posterior predictive\n",
    "* Introduce two probability laws used in the derivation\n",
    "* Evaluate the posterior predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition: Model Averaging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the Posterior Predictive\n",
    "\n",
    "**A Graphical Model for the Training *and* Test Data.** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Derivation.** Now we have all we need in order to derive a formula for $p(\\mathcal{D}^* | \\mathcal{D})$. Our first step is to multiply and divide $p(\\mathcal{D}^* | \\mathcal{D})$ by $p(\\mathcal{D})$. \n",
    "\\begin{align}\n",
    "p(\\mathcal{D}^* | \\mathcal{D}) &= \\frac{p(\\mathcal{D}^* | \\mathcal{D}) \\cdot p(\\mathcal{D})}{p(\\mathcal{D})} \n",
    "\\end{align}\n",
    "We do this so that we can write the numerator as the *joint* distribution of $\\mathcal{D}^*$ and $\\mathcal{D}$:\n",
    "\\begin{align}\n",
    "&= \\frac{p(\\mathcal{D}^*, \\mathcal{D})}{p(\\mathcal{D})} \n",
    "\\end{align}\n",
    "Next, we use the law of total probability to re-write the above as a joint distribution over $\\mathcal{D}^*$, $\\mathcal{D}$, and $\\theta$. We do this to introduce $\\theta$ into the equation---since our model's prior, likelihood, and posterior all depend on $\\theta$, it would be weird if the formula for $p(\\mathcal{D}^* | \\mathcal{D})$ depend on it. This gives us:\n",
    "\\begin{align}\n",
    "&= \\frac{\\int p(\\mathcal{D}^*, \\mathcal{D}, \\theta) \\cdot d\\theta}{p(\\mathcal{D})} \n",
    "\\end{align}\n",
    "Now, we can factorize this joint distribution to get one term that's the posterior, $p(\\theta | \\mathcal{D})$, and one term that's the marginal, $p(\\mathcal{D})$:\n",
    "\\begin{align}\n",
    "&= \\frac{\\int p(\\mathcal{D}^* | \\theta, \\mathcal{D}) \\cdot p(\\theta | \\mathcal{D}) \\cdot p(\\mathcal{D}) \\cdot d\\theta}{p(\\mathcal{D})} \n",
    "\\end{align}\n",
    "Since $p(\\mathcal{D})$ doesn't depend on $\\theta$, we can take it out of the integral, thereby canceling it with the $p(\\mathcal{D})$ in the denominator:\n",
    "\\begin{align}\n",
    "&= \\int p(\\mathcal{D}^* | \\theta, \\mathcal{D}) \\cdot p(\\theta | \\mathcal{D}) \\cdot d\\theta\n",
    "\\end{align}\n",
    "Finally, using the laws of conditional independence, we know that $p(\\mathcal{D}^* | \\theta, \\mathcal{D}) = p(\\mathcal{D}^* | \\theta)$. This is because, by conditioning on $\\theta$, we remove all paths connecting $\\mathcal{D}$ to $\\mathcal{D}^*$. In other words, $\\theta$ summarizes all information from $\\mathcal{D}$ needed to predict $\\mathcal{D}^*$. This gives us the following equation:\n",
    "\\begin{align}\n",
    "&= \\int \\underbrace{p(\\mathcal{D}^* | \\theta)}_{\\text{likelihood of new data}} \\cdot \\underbrace{p(\\theta | \\mathcal{D})}_{\\text{posterior}} \\cdot d\\theta\n",
    "\\end{align}\n",
    "As you can see, $p(\\mathcal{D}^* | \\mathcal{D})$ is a function of the posterior and the joint data likelihood of the new data. Adding some syntactic sugar, we can write the above equation as:\n",
    "\\begin{align}\n",
    "&= \\mathbb{E}_{p(\\theta | \\mathcal{D})} \\left[ p(\\mathcal{D}^* | \\theta) \\right]\n",
    "\\end{align}\n",
    "This shows that to evaluate $p(\\mathcal{D}^* | \\mathcal{D})$, we need to:\n",
    "1. Draw posterior samples $\\theta \\sim p(\\theta | \\mathcal{D})$.\n",
    "2. Average the likelihood $p(\\mathcal{D}^* | \\theta)$ across these samples.\n",
    "\n",
    "Interpretation: the posterior predictive *averages* the diversity of models given to us by the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Geometry of Joint Distributions\n",
    "\n",
    "**Bayes' Rule $\\rightarrow$ Cross-Sections.** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Law of Total Probability $\\rightarrow$ Sum of Cross-Sections.** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laws of Conditional Independence\n",
    "\n",
    "Recall that the slope and intercept of a model may have been sampled independently under the prior, but as soon as we condition on data, they are no longer independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the Posterior Predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Exercise: Derive the Posterior Predictive Distribution\n",
    "\n",
    "For each of the models below, draw the directed graphical model, and then derive the posterior predictive formula.\n",
    "\n",
    "**Part 1:** Bayesian predictive model. \n",
    "\\begin{align}\n",
    "\\theta &\\sim p_\\theta(\\cdot) \\\\\n",
    "y_n | x_n, \\theta &\\sim p_{Y | X}(\\cdot | x_n, \\theta)\n",
    "\\end{align}\n",
    "\n",
    "**Part 2:** Bayesian Concept-Bottlebeck model (CBM).\n",
    "\\begin{align}\n",
    "\\theta &\\sim p_\\theta(\\cdot) \\\\\n",
    "\\phi &\\sim p_\\phi(\\cdot) \\\\\n",
    "c_n | x_n, \\theta &\\sim p_{C | X}(\\cdot | x_n, \\theta) = \\mathrm{Cat}(\\pi(x_n; \\theta)) \\\\\n",
    "y_n | c_n, \\phi &\\sim p_{Y | C}(\\cdot | c_n, \\phi),\n",
    "\\end{align}\n",
    "where $\\pi(\\cdot; \\theta)$ is a function that maps $x_n$ to the parameters of a categorical distribution. CBMs aim to make it easier to interpret model predictions. They do this by combining two models:\n",
    "1. CMBs first learning to predict \"concepts\" $c_n$ associated, associated with input $x_n$. In a CBM, a concept is just a discrete attribute associated with the input; for example, if $x_n$ is an image of wildlife, a concept could be rain, grass, dog, etc. You can think of $p_{C | X}$ as a classifier. \n",
    "2. After having predicted the concept $c_n$ from the input $x_n$, CBMs attempt to predict the final output $y_n$ from the concept only. In this way, predictions of $y_n$ can be analyzed in terms of the concepts, which as easier to understand, instead of with respect to the inputs, which could be high dimensional and difficult to reason about.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation using the Posterior Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
