

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>9. Optimization &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'optimization';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/optimization.html" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="10. Continuous Probability" href="probability-continuous.html" />
    <link rel="prev" title="8. Maximum Likelihood: Practice" href="mle-practice.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="help.html">Academic Support &amp; Office Hours</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. What is Probabilistic ML?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Vectorization in <code class="docutils literal notranslate"><span class="pre">Jax</span></code>: An Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization in <code class="docutils literal notranslate"><span class="pre">Jax</span></code></a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">7. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-practice.html">8. Maximum Likelihood: Practice</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">9. Optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">10. Continuous Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="regression-linear.html">11. Regression (Linear)</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification-linear.html">12. Classification (Linear)</a></li>
<li class="toctree-l1"><a class="reference internal" href="regression-and-classification-nonlinear.html">13. Regression and Classification (Non-Linear)</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluation-metrics.html">14. Evaluation Metrics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dimensionality-reduction.html">15. Dimensionality Reduction (Factor Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">16. Clustering (Gaussian Mixture Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="time-series.html">17. Time Series (Dynamical Systems)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bayesian-inference-theory.html">18. Bayesian Inference: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian-inference-practice.html">19. Bayesian Inference: Practice</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/mogu-lab/cs349-fall-2024/blob/master/optimization.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li><a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fmogu-lab%2Fcs349-fall-2024%2Fblob%2Fmaster%2Foptimization.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onDeepnote"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_deepnote.svg">
  </span>
<span class="btn__text-container">Deepnote</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/optimization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analytic-solutions-to-optimization-problems">9.1. Analytic Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-analytically-solving-for-the-mle">9.2. An Example: Analytically Solving for the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-with-analytic-solutions-to-optimization-problems">9.3. Challenges with Analytic Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numeric-solutions-to-optimization-problems">9.4. Numeric Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-with-numeric-optimization">9.5. Challenges with Numeric Optimization</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimization">
<h1><span class="section-number">9. </span>Optimization<a class="headerlink" href="#optimization" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span> 
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> We can use <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> to perform the MLE on a class of models, composed of discrete distributions. The MLE involves solving an optimization problem—finding the parameters that maximize the joint data likelihood. So far, we’ve let <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> take care of this optimization problem for us. But what is <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> doing under the hood?</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Introduce (exact) analytical optimization, which rely on person to do most of the work via pen-and-paper math.</p></li>
<li><p>Introduce (approximate) numerical optimization, which rely on the machine to do the work for us, like <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>.</p></li>
<li><p>Discuss tradeoffs between the two.</p></li>
</ul>
<section id="analytic-solutions-to-optimization-problems">
<h2><span class="section-number">9.1. </span>Analytic Solutions to Optimization Problems<a class="headerlink" href="#analytic-solutions-to-optimization-problems" title="Permalink to this heading">#</a></h2>
<p><strong>Goal:</strong> Recall that our goal is to maximize the joint data log-likelihood:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8e20116d-a121-45f3-9c74-abee99deb2ce">
<span class="eqno">(9.1)<a class="headerlink" href="#equation-8e20116d-a121-45f3-9c74-abee99deb2ce" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmax}_\theta \text{ } p(\mathcal{D}; \theta),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> represents our model parameters. We will remain consistent with the convention of ML literature; we re-write the above problem as a minimization problem of our <em>negative</em> log-likelihood.</p>
<div class="amsmath math notranslate nohighlight" id="equation-9b5e999f-bc67-461f-b443-114a5aacb4e7">
<span class="eqno">(9.2)<a class="headerlink" href="#equation-9b5e999f-bc67-461f-b443-114a5aacb4e7" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta^\text{MLE} &amp;= \mathrm{argmin}_\theta \underbrace{-p(\mathcal{D}; \theta)}_{\text{Our loss function, } \mathcal{L}(\theta)}
\end{align}\]</div>
<p>We call the <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> our “loss function,” since our goal is now to minimize our losses.</p>
<p><strong>Intuition:</strong> So how can we identify the minima of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>? A good place to start is to determine what makes <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> different at its minima. Let’s see if we can get some intuition by looking at some intuition by looking at loss functions we made up.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_example_loss_functions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7aac645797545a3aef2ddac29c6a572ef665ecb662bf2e35d0e205080ccf9767.png" src="_images/7aac645797545a3aef2ddac29c6a572ef665ecb662bf2e35d0e205080ccf9767.png" />
</div>
</div>
<p>Looking at the left plot above, we see that the minimum has a unique property: the loss function is <em>flat at the minimum</em>. In other words, at the minimum, the derivative of the loss function equals zero:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1d4f1ce1-bb12-46f2-bd03-1d34ecc1fafd">
<span class="eqno">(9.3)<a class="headerlink" href="#equation-1d4f1ce1-bb12-46f2-bd03-1d34ecc1fafd" title="Permalink to this equation">#</a></span>\[\begin{align}
\frac{d \mathcal{L}(\theta)}{d \theta} = 0
\end{align}\]</div>
<p>Does the same hold for the right plot? Not exactly… At the minimum, we do have <span class="math notranslate nohighlight">\(\frac{d \mathcal{L}(\theta)}{d \theta} = 0\)</span>. But there are other points for which we also have <span class="math notranslate nohighlight">\(\frac{d \mathcal{L}(\theta)}{d \theta} = 0\)</span>.</p>
<p>Nonetheless, for these two functions, it seems that looking at the points for which the loss is flat is helpful. Instead of looking at every possible value of <span class="math notranslate nohighlight">\(\theta\)</span> (in these plots, <span class="math notranslate nohighlight">\(\theta\)</span> can take on infinite different values), we just need to examine the points for which the loss is flat. For the plot on the left, this strategy directly found the minimum. For the plot on the right, this strategy found a small set of points that <em>includes</em> the minimum. To find the minimum within this set, all we need to do is evaluate the loss at each point and select the one that yields the smallest loss.</p>
<p><strong>Procedure:</strong> We can turn this intuition into the following four-step process.</p>
<ol class="arabic simple">
<li><p>Compute the derivative of the loss function: <span class="math notranslate nohighlight">\(\frac{d \mathcal{L}(\theta)}{d \theta}\)</span></p></li>
<li><p>Set the derivative of the loss function equal to <span class="math notranslate nohighlight">\(0\)</span>: <span class="math notranslate nohighlight">\(\frac{d \mathcal{L}(\theta)}{d \theta} = 0\)</span></p></li>
<li><p>Solve the equation for <em>all possible values</em> of <span class="math notranslate nohighlight">\(\theta\)</span> analytically (i.e. on pen-and-paper)—this is the difficult part!</p></li>
<li><p>Plug each possible value of <span class="math notranslate nohighlight">\(\theta\)</span> into our loss function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> and select the one(s) that minimize it.</p></li>
</ol>
</section>
<section id="an-example-analytically-solving-for-the-mle">
<h2><span class="section-number">9.2. </span>An Example: Analytically Solving for the MLE<a class="headerlink" href="#an-example-analytically-solving-for-the-mle" title="Permalink to this heading">#</a></h2>
<p><strong>The Model.</strong> Let’s see how this works by analytically performing the MLE on a simple example. Suppose we want to model the probability of a patient being hospitalized overnight. We can do this using a Bernoulli distribution:</p>
<div class="amsmath math notranslate nohighlight" id="equation-afacb693-b117-42f7-9bcc-9116496c4f63">
<span class="eqno">(9.4)<a class="headerlink" href="#equation-afacb693-b117-42f7-9bcc-9116496c4f63" title="Permalink to this equation">#</a></span>\[\begin{align}
H \sim p_H(\cdot; \rho) = \mathrm{Bern}(\rho).
\end{align}\]</div>
<p>Recall that the PMF of a Bernoulli RV is,</p>
<div class="amsmath math notranslate nohighlight" id="equation-9e0e646b-4b73-45e8-bcce-cae86c181045">
<span class="eqno">(9.5)<a class="headerlink" href="#equation-9e0e646b-4b73-45e8-bcce-cae86c181045" title="Permalink to this equation">#</a></span>\[\begin{align}
p_H(h; \rho) = \rho^{\mathbb{I}(h = 1)} \cdot (1 - \rho)^{\mathbb{I}(h = 0)},
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{I}(\cdot)\)</span> is an <em>indicator variable</em>—it evaluates to 1 if the condition in parentheses is true and 0 otherwise.</p>
<p><strong>The Joint Data Likelihood.</strong> Now, let’s write the joint data log-likelihood for our model:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f845bcab-92bd-40ad-b673-4b0f385ae508">
<span class="eqno">(9.6)<a class="headerlink" href="#equation-f845bcab-92bd-40ad-b673-4b0f385ae508" title="Permalink to this equation">#</a></span>\[\begin{align}
\log p(\mathcal{D}; \rho) &amp;= \log \prod\limits_{n=1}^N p(\mathcal{D}_n; \rho) \quad (\text{since observations are i.i.d}) \\
&amp;= \log \prod\limits_{n=1}^N p_H(h_n; \rho) \\
&amp;= \log \prod\limits_{n=1}^N \rho^{\mathbb{I}(h_n = 1)} \cdot (1 - \rho)^{\mathbb{I}(h_n = 0)} \quad (\text{using the definition of Bernoulli PMF}) \\
&amp;= \sum\limits_{n=1}^N \log \rho^{\mathbb{I}(h_n = 1)} + \log (1 - \rho)^{\mathbb{I}(h_n = 0)} \quad (\text{using the fact that } \log (x \cdot y) = \log x + \log y) \\
&amp;= \sum\limits_{n=1}^N \mathbb{I}(h_n = 1) \cdot \log \rho + \mathbb{I}(h_n = 0) \cdot \log (1 - \rho) \quad (\text{using the fact that } \log x^y = y \cdot \log x)  \\
&amp;= \underbrace{\left( \sum\limits_{n=1}^N \mathbb{I}(h_n = 1) \right)}_{\text{Total number of times $H = 1$}} \cdot \log \rho + \underbrace{\left( \sum\limits_{n=1}^N \mathbb{I}(h_n = 0) \right)}_{\text{Total number of times $H = 0$}} \cdot \log (1 - \rho) \quad (\text{moving terms that do not depend on the sums out}) \\
&amp;= T \cdot \log \rho + (N - T) \cdot \log (1 - \rho)
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(T = \sum\limits_{n=1}^N \mathbb{I}(h_n = 1)\)</span> is the total number of hospitalizations.</p>
<p><strong>The MLE Objective.</strong> Our MLE objective is therefore:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7d031d56-76ac-44da-8eca-09f65feb50f6">
<span class="eqno">(9.7)<a class="headerlink" href="#equation-7d031d56-76ac-44da-8eca-09f65feb50f6" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho^\text{MLE} &amp;= \mathrm{argmax}_{\rho} \text{ } \log p(\mathcal{D}; \rho) \\
&amp;= \mathrm{argmax}_{\rho} \left( T \cdot \log \rho - (N - T) \cdot \log (1 - \rho) \right) \\
&amp;= \mathrm{argmin}_{\rho} \underbrace{\left( -T \cdot \log \rho + (N - T) \cdot \log (1 - \rho) \right)}_{\text{Our loss function: } \mathcal{L}(\rho)} \quad (\text{maximizing a function is equivalent to minimizing its negative})
\end{align}\]</div>
<p><strong>Analytic Optimization.</strong> We take the gradient of the above loss <span class="math notranslate nohighlight">\(\mathcal{L}(\rho)\)</span> with respect to <span class="math notranslate nohighlight">\(\rho\)</span>, set it to <span class="math notranslate nohighlight">\(0\)</span> and solve:</p>
<div class="amsmath math notranslate nohighlight" id="equation-01e73b51-3b4c-418a-bdda-9dda4bed3dd5">
<span class="eqno">(9.8)<a class="headerlink" href="#equation-01e73b51-3b4c-418a-bdda-9dda4bed3dd5" title="Permalink to this equation">#</a></span>\[\begin{align}
0 &amp;= \frac{d \mathcal{L}(\rho)}{d \rho} \\
&amp;= -\frac{T}{\rho} + \frac{N - T}{1 - \rho} \quad (\text{taking the derivative of } \mathcal{L}(\rho)) \\
&amp;= \frac{T - N \cdot \rho}{\rho \cdot (\rho - 1)} \quad (\text{bringing fractions under common denominator}) \\
&amp;= T - N \cdot \rho \quad (\text{multiplying both sides by } \rho \cdot (\rho - 1))
\end{align}\]</div>
<p>Solving the above gives us the solution,</p>
<div class="amsmath math notranslate nohighlight" id="equation-a69d2a3b-2c8e-44d9-9ef0-dea5b5e5bac8">
<span class="eqno">(9.9)<a class="headerlink" href="#equation-a69d2a3b-2c8e-44d9-9ef0-dea5b5e5bac8" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho &amp;= \frac{T}{N}.
\end{align}\]</div>
<p>The solution is exactly the proportion of hospitalizations out of the total number of hospital visits!</p>
<p><strong>A Note on Constraint Optimization.</strong> Oftentimes, when performing the MLE analytically, we need to constrain the parameters to lie within valid ranges. For example, <span class="math notranslate nohighlight">\(\rho\)</span> should only be allowed to take on values between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Formally, we express such a constraint as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-67ec1f16-ca87-4370-a4fc-f00553360ddb">
<span class="eqno">(9.10)<a class="headerlink" href="#equation-67ec1f16-ca87-4370-a4fc-f00553360ddb" title="Permalink to this equation">#</a></span>\[\begin{align}
\rho^\text{MLE} &amp;= \mathrm{argmax}_{\rho} \text{ } \log p(\mathcal{D}; \rho) \quad \text{subject to} \quad 0 \leq \rho \leq 1 \\
\end{align}\]</div>
<p>In our derivation, it just so happens that the solution already satisfies this constraint. However, for different models, we may have to enforce such constraints explicitly. To enforce these constraints, one can use <a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="noopener noreferrer" target="_blank">lagrange multipliers</a>. We will not cover this method in here, but just want to point out that this class of methods exists.</p>
</section>
<section id="challenges-with-analytic-solutions-to-optimization-problems">
<h2><span class="section-number">9.3. </span>Challenges with Analytic Solutions to Optimization Problems<a class="headerlink" href="#challenges-with-analytic-solutions-to-optimization-problems" title="Permalink to this heading">#</a></h2>
<p>As you can see from the above example, performing optimization analytically suffers from two challenges:</p>
<ol class="arabic simple">
<li><p><strong>Analytic optimization needs a specialized solution for every model.</strong> As you can see from the above example, performing the optimization analytically will require a new derivation for each model. However, when working with real data, we rarely know what’s the “right” model a priori. We typically start with an exploratory data analysis, try different models, evaluate them using different metrics, make hypotheses for why the models don’t fit well, revise the models to fit better, and repeat. If we had to derive a new solution for every model we wish to test, our modeling process will become quite cumbersome. Moreover, we are more prone to make errors in the derivation and write bugs in our code.</p></li>
<li><p><strong>Analytic optimization cannot solve for the parameters of every model.</strong> Since the above example is for a simple Bernoulli model, an analytic solution to the MLE exists. However, for more complex problems, that may not be the case. In fact, for modern ML models, it is <em>rare</em> for there to exist an analytic solution.</p></li>
</ol>
<p>This motivates us to look into alternative optimization methods: numeric optimization.</p>
</section>
<section id="numeric-solutions-to-optimization-problems">
<h2><span class="section-number">9.4. </span>Numeric Solutions to Optimization Problems<a class="headerlink" href="#numeric-solutions-to-optimization-problems" title="Permalink to this heading">#</a></h2>
<p>Numeric optimizations algorithms address both shortcomings of analytic optimization:</p>
<ol class="arabic simple">
<li><p>They can be easily applied to different models without cumbersome derivations. This is because they can be conveniently implemented behind abstractions. Moreover, these abstractions allow us to pair different numeric optimization algorithms with different models <em>without having to write much code</em>. They even make it easy to incorporate constraints over the parameters into the optimization. This helps us write bug-free, error-free optimization code.</p></li>
<li><p>They can be applied to optimization problems for which there exists no analytic optimization solution. They are also fast and work well in practice, making them extremely popular for complicated modern ML models.</p></li>
</ol>
<p>Of course, these numerical algorithms have their own challenges—we’ll look into some challenges they face in a bit. Let us introduce the simplest and most popular numerical optimization algorithm: <em>gradient descent</em>.</p>
<p><strong>Gradient Descent.</strong> The gradient (or “multivariate derivative”) is the direction of the steepest ascent. Similarly, the negative gradient is the direction of the steepest descent. The idea behind the <em>gradient descent</em> algorithm is to take little steps in the direction of the negative gradient in hope that, after taking enough steps, we’ll reach the minimum. Let’s illustrate this with an animation:</p>
<center>
    <p>
        <img src="https://miro.medium.com/v2/resize:fit:875/1*vchQOUUW_RPVjkb4pQF-1A.gif" width="500px" />
    </p>
    <p>
        <small>GIF from <a href="https://towardsai.net/p/machine-learning/deep-learning-from-scratch-in-modern-c-gradient-descent">Deep Learning from Scratch in Modern C++: Gradient Descent</a></small>
    </p>
</center>
<p>In the animation, the vertical axis represents <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>. The two horizontal axes represent the dimensions of <span class="math notranslate nohighlight">\(\theta\)</span> (in this case, it’s 2-dimensional). Each arrow represents the negative gradient. As you can see, in each iteration of the algorithm, <span class="math notranslate nohighlight">\(\theta\)</span> (the red point) moves in the direction of the gradient, progressively minimizing the loss.</p>
<p>Now, let’s write the gradient descent algorithm. For clarity, we’ll write it out with the notation for a 1-dimensional <span class="math notranslate nohighlight">\(\theta\)</span> (the multivariate version is pretty much the same).</p>
<div class="proof algorithm admonition" id="gradient-descent">
<p class="admonition-title"><span class="caption-number">Algorithm 9.1 </span> (Gradient Descent)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs.</strong> A loss function, <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>, a choice for the learning rate, <span class="math notranslate nohighlight">\(\alpha\)</span>, and an initialization for the parameters, <span class="math notranslate nohighlight">\(\theta^\text{current} \leftarrow \text{initial value}\)</span>.</p>
<p><strong>Output.</strong> Return a parameter <span class="math notranslate nohighlight">\(\theta\)</span> that (hopefully) minimizes the loss <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>.</p>
<p><strong>Algorithm.</strong> Repeat until the loss doesn’t change much from iteration to iteration:</p>
<ol class="arabic simple">
<li><p>Compute the gradient of the loss with respect to the parameters, evaluated at the current value of the parameters:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-329eec5a-fdce-4adb-9516-88dfead33a0f">
<span class="eqno">(9.11)<a class="headerlink" href="#equation-329eec5a-fdce-4adb-9516-88dfead33a0f" title="Permalink to this equation">#</a></span>\[\begin{align}
    u &amp;\leftarrow \frac{d \mathcal{L}(\theta)}{d \theta} \Bigg|_{\theta = \theta^\text{current}}
  \end{align}\]</div>
<ol class="arabic simple" start="2">
<li><p>Take a step in the direction of the negative gradient (steepest descent):</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-999426e9-1807-4af7-89f2-9e39832842a6">
<span class="eqno">(9.12)<a class="headerlink" href="#equation-999426e9-1807-4af7-89f2-9e39832842a6" title="Permalink to this equation">#</a></span>\[\begin{align}
    \theta^\text{next} &amp;\leftarrow \theta^\text{current} - \alpha \cdot u
  \end{align}\]</div>
<ol class="arabic simple" start="3">
<li><p>Update the model parameters:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-17f2e5f0-3f31-4532-bf43-bee0ad1be0cb">
<span class="eqno">(9.13)<a class="headerlink" href="#equation-17f2e5f0-3f31-4532-bf43-bee0ad1be0cb" title="Permalink to this equation">#</a></span>\[\begin{align}
    \theta^\text{current} &amp;\leftarrow \theta^\text{next}
  \end{align}\]</div>
</section>
</div><p>In this algorithm, notice that there’s one variable we have yet to define: <span class="math notranslate nohighlight">\(\alpha\)</span>. Here, <span class="math notranslate nohighlight">\(\alpha\)</span> represents the size of the step we plan to take in the direction of the gradient. It is typically called the <em>learning rate</em>. You will have to play with this parameter to determine what value works best for minimizing your loss. According to lore, a good place to start is with <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>. Why? Who knows…</p>
<p><strong>Simple Implementation of Gradient Descent.</strong> Even though <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> already comes with <a class="reference external" href="https://num.pyro.ai/en/stable/optimizers.html" rel="noopener noreferrer" target="_blank">several different gradient-based optimization algorithms</a>, let’s implement the above univariate gradient descent algorithm. While we’re at it, let’s have the algorithm keep track of how our parameters <span class="math notranslate nohighlight">\(\theta\)</span> change with each iteration to get some more intuition:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>

<span class="k">def</span> <span class="nf">univariate_gradient_descent</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">theta_init</span><span class="p">):</span>
    <span class="c1"># Initialize theta     </span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_init</span>

    <span class="c1"># For each iteration of the algorithm...</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="c1"># Use Jax to compute the gradient of the loss with respect to theta</span>
        <span class="n">gradient_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)</span>

        <span class="c1"># Evaluate the gradient at the current value of theta</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">gradient_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Take a step in the direction of the gradient</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">u</span>

    <span class="k">return</span> <span class="n">theta</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s see how it does on minimizing a quadratic loss function: <span class="math notranslate nohighlight">\(\mathcal{L}(\theta) = \theta^2\)</span>. We know the minimum of this formula should be at <span class="math notranslate nohighlight">\(\theta = 0\)</span>. Will our algorithm find it?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define our loss function</span>
<span class="k">def</span> <span class="nf">quadratic_loss</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">theta</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="c1"># Run gradient descent</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Minimum =&#39;</span><span class="p">,</span> <span class="n">univariate_gradient_descent</span><span class="p">(</span><span class="n">quadratic_loss</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minimum = -4.0740736e-10
</pre></div>
</div>
</div>
</div>
<p>As you can see, gradient descent successfully found the minima, approximately—the resultant number is very close to 0.</p>
<center>
    <img src="figs/gradient_descent_quadratic_fn_lr0p1.gif" width="600px" />
    <img src="figs/gradient_descent_quadratic_fn_lr0p01.gif" width="600px" />
</center><p><strong>Automatic Differentiation.</strong> So how does <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> know to compute gradients automatically for us? TODO YANIV</p>
</section>
<section id="challenges-with-numeric-optimization">
<h2><span class="section-number">9.5. </span>Challenges with Numeric Optimization<a class="headerlink" href="#challenges-with-numeric-optimization" title="Permalink to this heading">#</a></h2>
<p><strong>Local optima.</strong></p>
<p><strong>Hyper-parameters.</strong></p>
<p><strong>Exercise:</strong> Do gradient descent by hand on a function with different learning rates to see where you end up.</p>
<center>
    <img src="figs/gradient_descent_quadratic_plus_sin_fn_lr0p1.gif" width="600px" />
    <img src="figs/gradient_descent_quadratic_plus_sin_fn_lr0p01.gif" width="600px" />
</center></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="mle-practice.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Maximum Likelihood: Practice</p>
      </div>
    </a>
    <a class="right-next"
       href="probability-continuous.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Continuous Probability</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analytic-solutions-to-optimization-problems">9.1. Analytic Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-analytically-solving-for-the-mle">9.2. An Example: Analytically Solving for the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-with-analytic-solutions-to-optimization-problems">9.3. Challenges with Analytic Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numeric-solutions-to-optimization-problems">9.4. Numeric Solutions to Optimization Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-with-numeric-optimization">9.5. Challenges with Numeric Optimization</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-6">
        &copy; Copyright 2024	
      </div>      
      <div class="col-6">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="width: 49%; max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="width: 49%; max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>