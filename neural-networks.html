

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>13. Neural Networks &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'neural-networks';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/neural-networks.html" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="14. Evaluation Metrics" href="evaluation-metrics.html" />
    <link rel="prev" title="12. Classification" href="classification.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="help.html">Academic Support &amp; Office Hours</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. What is Probabilistic ML?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Vectorization in <code class="docutils literal notranslate"><span class="pre">Jax</span></code>: An Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization in <code class="docutils literal notranslate"><span class="pre">Jax</span></code></a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">7. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">8. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">9. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">10. Probability (Continuous)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">11. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">12. Classification</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">13. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluation-metrics.html">14. Evaluation Metrics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bayesian-inference-theory.html">15. Bayesian Inference: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian-inference-code.html">16. Bayesian Inference: Code</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dimensionality-reduction.html">17. Dimensionality Reduction (Factor Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">18. Clustering (Gaussian Mixture Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="time-series.html">19. Time Series (Dynamical Systems)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/neural-networks.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-shortcomings-of-polynomials">13.1. The Shortcomings of Polynomials</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expressive-functions-from-simple-building-blocks">13.2. Expressive Functions from Simple Building Blocks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-transforms">13.3. Multivariate Linear Transforms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">13.4. Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connecting-the-pictures-to-the-math">13.5. Connecting the Pictures to the Math</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="neural-networks">
<h1><span class="section-number">13. </span>Neural Networks<a class="headerlink" href="#neural-networks" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> So far, we’ve focused on translating our IHH colleague’s goals into probabilistic models, and then fitting these models to data to help them answer scientific questions. In each model’s conditional distributions, we’ve had to make two choices: what distribution to use, and how the distributions parameter should depend on the condition. For example, in regression, recall we picked the following conditional distribution:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d19d4996-4044-4e3e-ac75-143e6207de97">
<span class="eqno">(13.1)<a class="headerlink" href="#equation-d19d4996-4044-4e3e-ac75-143e6207de97" title="Permalink to this equation">#</a></span>\[\begin{align}
p_{Y | X}(\cdot | x_n; \underbrace{W, \sigma}_{\theta}) = \mathcal{N}( \underbrace{\mu(x_n; W)}_{\text{trend}}, \underbrace{\sigma^2}_{\text{noise}} ),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu(x_n; W)\)</span> represents the “trend” of the data. We’ve had to decide whether <span class="math notranslate nohighlight">\(\mu(x_n; W)\)</span> should be linear, polynomial, or some other function. As our data grows in complexity—for example, as <span class="math notranslate nohighlight">\(x_n\)</span> becomes high-dimensional—it becomes increasingly difficult to make up functions that are, expressive, fast, and easy to code. We will show you why below.</p>
<p><strong>Challenge:</strong> So what functions should be use in our probabilistic models? Here, we will introduce a new type of function—a <em>neural network</em>. As we will show here, neural networks are expressive, fast, and easy to code.</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Shortcomings of other expressive functions, likely polynomials</p></li>
<li><p>The idea behind neural networks: using function composition to create expressive functions</p></li>
<li><p>Introduce <em>a little bit</em> of linear algebra to help introduce neural networks</p></li>
<li><p>Introduce neural networks, implement them in <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code> and fit them to IHH data</p></li>
<li><p>Connect the math behind neural networks to the pictures used to represent them in popular media</p></li>
</ul>
<section id="the-shortcomings-of-polynomials">
<h2><span class="section-number">13.1. </span>The Shortcomings of Polynomials<a class="headerlink" href="#the-shortcomings-of-polynomials" title="Permalink to this heading">#</a></h2>
<p><strong>The Universality of Polynomials.</strong> In both chapters about regression and classification, we observed the benefits of using non-linear functions for data with non-linear trends. In regression, we’ve focused on polynomials as our primary tool for creating non-linear functions, and for our low-dimensional data, they seemed to work great! So you may be wondering, why not apply them to high-dimensional data as well? In fact, polynomials boast a very powerful property: they are <em>universal function approximators</em>. By this, we mean that for any continuous function on some bounded interval <span class="math notranslate nohighlight">\([a, b]\)</span>, we can find a polynomial that approximates it arbitrarily well (this is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem" rel="noopener noreferrer" target="_blank">Stone–Weierstrass theorem</a>). This means that for <em>any</em> data set that consists of continuous trends, <em>theoretically speaking</em>, polynomials can capture it. This is a huge deal! So let’s see how polynomials measure up against a neural network:</p>
<figure class="align-center" id="fig-regression-inductive-bias-closeup">
<img alt="_images/example_regression_inductive_bias_percent_ood_0.png" src="_images/example_regression_inductive_bias_percent_ood_0.png" />
<figcaption>
<p><span class="caption-number">Fig. 13.1 </span><span class="caption-text">Examples of polynomial and neural network regression on IHH data.</span><a class="headerlink" href="#fig-regression-inductive-bias-closeup" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>As you can see, polynomial regression seems to capture the trend in these regression data sets super well—so why aren’t they as famous as neural networks? Polynomial regression actually comes with several challenges that make it inappropriate in many contexts.</p>
<p><strong>Challenge 1: Numerical Instability.</strong> Polynomials are numerically unstable. Imagine you want to use a degree-20 polynomial in your regression model. This means that, in fitting your model, you will have to evaluate <span class="math notranslate nohighlight">\(x^20\)</span>. When <span class="math notranslate nohighlight">\(x = 0.1\)</span> and when <span class="math notranslate nohighlight">\(x = 10.0\)</span>, you’re asking your computer to represent numbers like <span class="math notranslate nohighlight">\(0.000000000000000000001\)</span> and <span class="math notranslate nohighlight">\(1000000000000000000000\)</span>. Because your computer only has finite precision, very small numbers are at risk of being rounded down to <span class="math notranslate nohighlight">\(0\)</span> and very large numbers may overflow.</p>
<p><strong>Challenge 2: Inductive Bias.</strong> Oftentimes, we’re less interested in seeing what our model does on data we’ve already observed. Instead, we want to know what it might do for a <em>new</em> data point. For example, suppose we’re asked to develop a model to predict telekinetic ability and glow from age (like we did in the regression chapter). We aren’t interested in seeing the model’s predictions on patients included in our data set—we’ve already observed these patients’ age, telekinetic ability, and glow. What we’re we’re interested in is the model’s predictions for <em>new</em> patients. For example, what happens if we get an input that we’ve never seen before, like a patient that’s much older or younger than the rest of the patients in the data.</p>
<p>We call the trend of the model away from the data its “inductive bias.” Different models that fit the data equally well may actually have different inductive biases. Let’s illustrate what we mean visually. In the plot above, you see that the 5th and 6th-degree polynomials both fit the data equally well. But what would they predict for points away from our data? And will their predictions be medically reasonable? Let’s have a look: the plot below shows the very same models from the plot above, but this time the plots are zoomed out. In this way you can see each model’s behavior away from the data. As you can see, each model’s inductive bias is different.</p>
<figure class="align-center" id="fig-regression-inductive-bias">
<img alt="_images/example_regression_inductive_bias_percent_ood_30.png" src="_images/example_regression_inductive_bias_percent_ood_30.png" />
<figcaption>
<p><span class="caption-number">Fig. 13.2 </span><span class="caption-text">A zoomed-out plot of the same models from the figure above.</span><a class="headerlink" href="#fig-regression-inductive-bias" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>What’s interesting in the above plot is that the polynomial regression’s inductive bias tends towards positive or negative infinity very quickly. Looking at the math, this makes sense: a 5th or 6th-degree polynomial will include terms like <span class="math notranslate nohighlight">\(\text{age}^5\)</span>, which grow quickly with age.</p>
<p>While this may be the desired behavior for some data sets, for our data sets, it’s inappropriate. For example, look at the left-middle plot, in which the 5th-degree polynomial regression predicts telekinetic ability from age. Even though we’ve generally seen that as age increases, telekinetic ability decreases, this plot suggests the opposite. After age 100, the patient’s telekinetic ability <em>skyrockets</em>; in fact, it increases so quickly it’s larger than the ability of all other patients.</p>
<p><strong>Conclusion:</strong> When picking a function-class to work with (like the class of polynomials), it’s important to consider numerical stability (as well as ease of optimization). Without these properties, it doesn’t matter how expressive your function is, since you’ll never be able to practically fit it to data. Second, it’s important to think about the function class’s inductive bias, or in other words, to think about how it will generalize in regions of the space where data is scarce.</p>
</section>
<section id="expressive-functions-from-simple-building-blocks">
<h2><span class="section-number">13.2. </span>Expressive Functions from Simple Building Blocks<a class="headerlink" href="#expressive-functions-from-simple-building-blocks" title="Permalink to this heading">#</a></h2>
<p><strong>Idea.</strong> Instead of using polynomials, let’s see if we can build an expressive function-class, <span class="math notranslate nohighlight">\(\mu(\cdot; W)\)</span>, from small building blocks. Each block will be simple and numerically stable. And when combined, will give us an expressive function, capable of adapting to any trend we observe in the data. This is the mechanism underlying neural networks.</p>
<p>Let’s import some libraries so we can plot as we go.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">jrandom</span>
<span class="kn">import</span> <span class="nn">jax.nn</span> <span class="k">as</span> <span class="nn">jnn</span>
<span class="kn">import</span> <span class="nn">numpyro.distributions</span> <span class="k">as</span> <span class="nn">D</span>
</pre></div>
</div>
</div>
</div>
<p><strong>A Simple Block.</strong> For the simple block, let’s use a sigmoid. A sigmoid looks like a smoothed-out step function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">15.0</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jnn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;A Sigmoid Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b1bc71888fca882eb7756c392ba0f514bfbb5b08c9c4266948b41cf178fd4d5e.png" src="_images/b1bc71888fca882eb7756c392ba0f514bfbb5b08c9c4266948b41cf178fd4d5e.png" />
</div>
</div>
<p>We will give the sigmoid two parameters, which we will learn from data, giving us our simple building block:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6f5aff24-21c4-474a-9dfa-e32862fd8c71">
<span class="eqno">(13.2)<a class="headerlink" href="#equation-6f5aff24-21c4-474a-9dfa-e32862fd8c71" title="Permalink to this equation">#</a></span>\[\begin{align}
\underbrace{u(x; w, b)}_{\text{building block}} &amp;= \mathrm{sigmoid}(\underbrace{w \cdot x + b}_{\text{scale and shift $x$}})
\end{align}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(b\)</span> shifts the sigmoid left/right, and <span class="math notranslate nohighlight">\(w\)</span> stretches/shrinks the overall sigmoid horizontally. Let’s see what this looks like for different choices of choices of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Choose some scales and shifts</span>
<span class="n">scale</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">shift</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>

<span class="c1"># Plot!</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">15.0</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">shift</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jnn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$w = </span><span class="si">{}</span><span class="s1">, b = </span><span class="si">{}</span><span class="s1">$&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Building Blocks with Different Parameters&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/677fb0f765b4485f253544815c07f9df7fad3a13551d18daec9588964df15c0e.png" src="_images/677fb0f765b4485f253544815c07f9df7fad3a13551d18daec9588964df15c0e.png" />
</div>
</div>
<p><strong>Combining Building Blocks via Addition.</strong> While on their own, each building block can’t really model anything interesting—it can’t model anything that’s not a sigmoid. However, by adding these building blocks with different parameters, we can start making more interesting-looking functions:</p>
<div class="amsmath math notranslate nohighlight" id="equation-71406d2f-8fce-4f2a-9f73-cbb57bb948cb">
<span class="eqno">(13.3)<a class="headerlink" href="#equation-71406d2f-8fce-4f2a-9f73-cbb57bb948cb" title="Permalink to this equation">#</a></span>\[\begin{align}
h(x; w_1, \dots, w_L, b_1, \dots, b_L) &amp;= \sum\limits_{i=1}^L u(x; w_i, b_i) \\
&amp;= \sum\limits_{i=1}^L \text{sigmoid}(w_i \cdot x + b_i)
\end{align}\]</div>
<p>Let’s see what function we get by adding the three building blocks above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">shift</span><span class="p">):</span>  
    <span class="n">y</span> <span class="o">+=</span> <span class="n">jnn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># Plot!</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sum of the Building Blocks Above&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f11998671c2ab2091514a710479008c48683f25c32a994f0c602d50d437e7b95.png" src="_images/f11998671c2ab2091514a710479008c48683f25c32a994f0c602d50d437e7b95.png" />
</div>
</div>
<p>As you can see, this function already looks a lot more interesting than a sigmoid on its own. So what do you think will happen if we add <em>a bunch</em> of these together? Let’s design a little experiment to see:</p>
<ol class="arabic simple">
<li><p>Let’s plot functions composed of <span class="math notranslate nohighlight">\(L = 10\)</span> building blocks.</p></li>
<li><p>To make our lives easier, let’s draw <span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(b_i\)</span> from some distribution (so we don’t have to pick those values by hand). Here, we’ll go with a Normal distribution.</p></li>
<li><p>We’ll repeat the process <span class="math notranslate nohighlight">\(N\)</span> times to get a sense for the types of functions we can represent.</p></li>
</ol>
<p>Let’s see what happens:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of building blocks to add</span>
<span class="n">L</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Number of functions to plot</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Create one random key per function</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jrandom</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">key_per_function</span> <span class="o">=</span> <span class="n">jrandom</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">key_per_function</span><span class="p">:</span>
    <span class="n">key_slope</span><span class="p">,</span> <span class="n">key_intercept</span> <span class="o">=</span> <span class="n">jrandom</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Generate the parameters of the block from a Normal distribution</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">key_slope</span><span class="p">,</span> <span class="p">(</span><span class="n">L</span><span class="p">,))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">key_intercept</span><span class="p">,</span> <span class="p">(</span><span class="n">L</span><span class="p">,))</span>

    <span class="c1"># Use some &quot;broadcasting&quot; magic to efficiently sum the building blocks</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">jnn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Plot!</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Functions Generated by Adding Simple Blocks&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b95e1934311923efe151d68df2f3a5dafa46b9a9b66c7c3ccc13f408b9b8db58.png" src="_images/b95e1934311923efe151d68df2f3a5dafa46b9a9b66c7c3ccc13f408b9b8db58.png" />
</div>
</div>
<p>Now we’re getting some wiggly functions! You can imagine that if we were to learn the parameters <span class="math notranslate nohighlight">\(h\)</span> from the data, we’d be able to fit many different functions.</p>
</section>
<section id="multivariate-linear-transforms">
<h2><span class="section-number">13.3. </span>Multivariate Linear Transforms<a class="headerlink" href="#multivariate-linear-transforms" title="Permalink to this heading">#</a></h2>
<p>Suppose we’re developing a regression model for high-dimensional data and need to pick which <span class="math notranslate nohighlight">\(\mu(\cdot; W)\)</span> to use. Let our input data be <span class="math notranslate nohighlight">\(D\)</span> dimensional, meaning that our inputs are an array (or vector): <span class="math notranslate nohighlight">\(x_n = [ x_n^{(1)}, x_n^{(2)}, \dots, x_n^{(D)} ]\)</span>. We use the superscript to denote the dimension of the data.</p>
</section>
<section id="id1">
<h2><span class="section-number">13.4. </span>Neural Networks<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
</section>
<section id="connecting-the-pictures-to-the-math">
<h2><span class="section-number">13.5. </span>Connecting the Pictures to the Math<a class="headerlink" href="#connecting-the-pictures-to-the-math" title="Permalink to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="classification.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Classification</p>
      </div>
    </a>
    <a class="right-next"
       href="evaluation-metrics.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Evaluation Metrics</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-shortcomings-of-polynomials">13.1. The Shortcomings of Polynomials</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expressive-functions-from-simple-building-blocks">13.2. Expressive Functions from Simple Building Blocks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-transforms">13.3. Multivariate Linear Transforms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">13.4. Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connecting-the-pictures-to-the-math">13.5. Connecting the Pictures to the Math</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-6">
        &copy; Copyright 2024 Yaniv Yacoby
      </div>      
      <div class="col-6">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="width: 49%; max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="width: 49%; max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>