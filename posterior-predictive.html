

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>18. Bayesian Inference: Posterior Predictive &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/main.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/course_schedule.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'posterior-predictive';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/posterior-predictive.html" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="17. Bayesian Inference: Prior and Posterior" href="prior-and-posterior.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probabilistic Foundations of ML
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="help.html">Academic Support &amp; Office Hours</a></li>
<li class="toctree-l1"><a class="reference internal" href="skills-check.html">Skills Check</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. What is Probabilistic ML?</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-basics.html">2. Vectorization: An Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="vectorization-advanced.html">3. Advanced Vectorization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Directed Graphical Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability-discrete.html">4. Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-conditional.html">5. Conditional Probability (Discrete)</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-joint.html">6. Joint Probability (Discrete)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frequentist Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mle-theory.html">7. Maximum Likelihood: Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle-code.html">8. Maximum Likelihood: Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">9. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-continuous.html">10. Probability (Continuous)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictive Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regression.html">11. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">12. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">13. Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-selection.html">14. Model Selection &amp; Evaluation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gmms.html">15. Gaussian Mixture Models (Clustering)</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor-analysis.html">16. Factor Analysis (Dimensionality Reduction)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="prior-and-posterior.html">17. Bayesian Inference: Prior and Posterior</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">18. Bayesian Inference: Posterior Predictive</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/posterior-predictive.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bayesian Inference: Posterior Predictive</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-model-averaging">18.1. Intuition: Model Averaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-the-posterior-predictive">18.2. Derivation of the Posterior Predictive</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laws-of-conditional-independence">18.3. Laws of Conditional Independence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-predictive-of-different-models">18.4. Posterior Predictive of Different Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation-using-the-posterior-predictive">18.5. Model Evaluation using the Posterior Predictive</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-inference-posterior-predictive">
<h1><span class="section-number">18. </span>Bayesian Inference: Posterior Predictive<a class="headerlink" href="#bayesian-inference-posterior-predictive" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some helper functions (please ignore this!)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Context:</strong> For safety-critical applications of ML, it’s important that our model captures two notions of uncertainty. Aleatoric uncertainty captures inherent stochasticity in the system. In contrast, epistemic uncertainty is uncertainty over possible <em>models</em> that could have fit the data. Multiple models can fit the data when we have a lack of data and/or a lack of mechanistic understanding of the system. We realized that fitting models using the MLE only captured aleatoric uncertainty. To additionally capture epistemic, we therefore had to rethink our modeling paradigm. Using Bayes’ rule, we were able to obtain a <em>distribution</em> over model parameters given the data, <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span> (the posterior). Using <code class="docutils literal notranslate"><span class="pre">NumPyro</span></code>, we sampled from the posterior to obtain a diversity of models that fit the data. We interpreted a greater diversity of models indicated higher epistemic ucnertainty.</p>
<p><strong>Challenge:</strong> Now that we have a posterior over model parameters, we can capture <em>epistemic</em> uncertainty. But how do we use this diverse set of models to (1) make predictions, and (2) compute the log-likelihood (for evaluation)? To do this, we will derive the <em>posterior predictive</em>, a distribution that translates a distribution over parameters to a distribution over data. This distributions can then be used to make predictions and evaluate the log-likelihood.</p>
<p><strong>Outline:</strong></p>
<ul class="simple">
<li><p>Provide intuition for the posterior predictive</p></li>
<li><p>Derive the posterior predictive</p></li>
<li><p>Introduce laws of conditional independence</p></li>
<li><p>Evaluate the posterior predictive</p></li>
</ul>
<section id="intuition-model-averaging">
<h2><span class="section-number">18.1. </span>Intuition: Model Averaging<a class="headerlink" href="#intuition-model-averaging" title="Permalink to this heading">#</a></h2>
<p><strong>Bayesian Modeling as Ensembling.</strong> Recall in the previous chapter, we initially introduced <em>ensembling</em> as a way to capture epistemic uncertainty. In ensembling, we train a collection of models independently and hope that, due to quirks in optimization, we end up with a diverse collection of models. In a sense, doesn’t our Bayesian approach provide us with an ensemble as well? After all, each set of parameters <span class="math notranslate nohighlight">\(\theta\)</span> from the posterior <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span> represents a different model. Based on this analogy, we can create a “Bayesian” ensemble as follows:</p>
<ol class="arabic">
<li><p>We draw <span class="math notranslate nohighlight">\(S\)</span> samples from the posterior. Each sample <span class="math notranslate nohighlight">\(\theta_s\)</span> now represents a member of our ensemble.</p></li>
<li><p>Each posterior sample represents a different model, <span class="math notranslate nohighlight">\(p(\mathcal{D} | \theta_s)\)</span>.</p>
<blockquote>
<div><p>For regression, we have <span class="math notranslate nohighlight">\(p_{Y | X}(y | x, \theta_s)\)</span>.</p>
</div></blockquote>
</li>
</ol>
<p><strong>Predicting.</strong> Using this ensemble, we can predict by <em>averaging</em> the predictions of the ensemble members:</p>
<ol class="arabic">
<li><p>We draw <span class="math notranslate nohighlight">\(\mathcal{D}_s \sim p(\cdot | \theta_s)\)</span> for each <span class="math notranslate nohighlight">\(\theta_s\)</span>.</p>
<blockquote>
<div><p>For regression, we draw <span class="math notranslate nohighlight">\(y_s \sim p_{Y | X}(\cdot | x, \theta_s)\)</span>.</p>
</div></blockquote>
</li>
<li><p>We average: <span class="math notranslate nohighlight">\(\frac{1}{S} \sum\limits_{s=1}^S \mathcal{D}_s\)</span>.</p>
<blockquote>
<div><p>For regression, we average <span class="math notranslate nohighlight">\(\frac{1}{S} \sum\limits_{s=1}^S y_s\)</span>.</p>
</div></blockquote>
</li>
</ol>
<p><strong>Evaluating Log-Likelihood.</strong> Given test data, <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span>, we can use the ensemble to evaluate the model’s log-likelihood:</p>
<ol class="arabic">
<li><p>We evaluate <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \theta_s)\)</span> for each <span class="math notranslate nohighlight">\(\theta_s\)</span>.</p>
<blockquote>
<div><p>For regression, we evaluate <span class="math notranslate nohighlight">\(p_{Y | X}(y | x, \theta_s)\)</span> for each <span class="math notranslate nohighlight">\(\theta_s\)</span>.</p>
</div></blockquote>
</li>
<li><p>We average and take the log: <span class="math notranslate nohighlight">\(\log \frac{1}{S} \sum\limits_{s=1}^S p(\mathcal{D}^* | \theta_s)\)</span>.</p>
<blockquote>
<div><p>For regression, we average and take the log: <span class="math notranslate nohighlight">\(\log \frac{1}{S} \sum\limits_{s=1}^S p_{Y | X}(y^* | x^*, \theta_s)\)</span>.</p>
</div></blockquote>
</li>
</ol>
<p><strong>Formalizing Intuition.</strong> As we will show next, this intuition is actually correct.</p>
</section>
<section id="derivation-of-the-posterior-predictive">
<h2><span class="section-number">18.2. </span>Derivation of the Posterior Predictive<a class="headerlink" href="#derivation-of-the-posterior-predictive" title="Permalink to this heading">#</a></h2>
<p><strong>Goal.</strong> We want to derive a formula for <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span>, which represents the distribution of new data <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span> given the observed data, <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
<blockquote>
<div><p>For a regression model, this distribution is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0e366175-4ea0-44dd-a15a-66b98bdf732a">
<span class="eqno">(18.1)<a class="headerlink" href="#equation-0e366175-4ea0-44dd-a15a-66b98bdf732a" title="Permalink to this equation">#</a></span>\[\begin{align}
    p_{Y^* | X^*, \mathcal{D}}(y^* | x^*, \mathcal{D}) &amp;= p_{Y^* | X^*, \mathcal{D}}(y^* | x^*, x_1, \dots, x_N, y_1, \dots, y_N),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(x^*\)</span> is a <em>new</em> input for which we’d like to make a prediction, <span class="math notranslate nohighlight">\(y^*\)</span>.</p>
</div></blockquote>
<p><strong>A Graphical Model for the Training <em>and</em> Test Data.</strong> Notice that our posterior predictive includes a new random variable, <span class="math notranslate nohighlight">\(\mathcal{D}*\)</span>. Let’s incorporate it into our graphical model. This will help us reason about the conditional dependencies (below), needed in the derivation of the posterior predictive.</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGLeQTM6_s&#x2F;gr7qf2eGP4X0wfKa1NZFnw&#x2F;view?embed">
    </iframe>
  </div>
</div>
<p>As you can see, the original graphical model (for training data) is on the left. We then added a second component on the right for <span class="math notranslate nohighlight">\(M\)</span> test points we have not yet observed. We can similarly create a diagram for regression as follows:</p>
<div class="canva-centered-embedding">
  <div class="canva-iframe-container">
    <iframe loading="lazy" class="canva-iframe"
      src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGLeBNwO1Q&#x2F;aZpvVjlBvUw6FQOHfzk5qA&#x2F;view?embed">
    </iframe>
  </div>
</div><p><strong>Derivation.</strong> Now we have all we need in order to derive a formula for <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span>. Our first step is to multiply and divide <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span> by <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-82fda503-39ad-474c-ae46-a5a0285093f5">
<span class="eqno">(18.2)<a class="headerlink" href="#equation-82fda503-39ad-474c-ae46-a5a0285093f5" title="Permalink to this equation">#</a></span>\[\begin{align}
p(\mathcal{D}^* | \mathcal{D}) &amp;= \frac{p(\mathcal{D}^* | \mathcal{D}) \cdot p(\mathcal{D})}{p(\mathcal{D})} 
\end{align}\]</div>
<p>We do this so that we can write the numerator as the <em>joint</em> distribution of <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span> and <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7879098f-138c-431c-af32-333db9bb9f25">
<span class="eqno">(18.3)<a class="headerlink" href="#equation-7879098f-138c-431c-af32-333db9bb9f25" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \frac{p(\mathcal{D}^*, \mathcal{D})}{p(\mathcal{D})} 
\end{align}\]</div>
<p>Next, we use the law of total probability to re-write the above as a joint distribution over <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, and <span class="math notranslate nohighlight">\(\theta\)</span>. We do this to introduce <span class="math notranslate nohighlight">\(\theta\)</span> into the equation—since our model’s prior, likelihood, and posterior all depend on <span class="math notranslate nohighlight">\(\theta\)</span>, it would be weird if the formula for <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span> didn’t depend on it. This gives us:</p>
<div class="amsmath math notranslate nohighlight" id="equation-12256be7-9ee3-4eb3-a193-2ce6e5be2220">
<span class="eqno">(18.4)<a class="headerlink" href="#equation-12256be7-9ee3-4eb3-a193-2ce6e5be2220" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \frac{\int p(\mathcal{D}^*, \mathcal{D}, \theta) \cdot d\theta}{p(\mathcal{D})} 
\end{align}\]</div>
<p>Now, we can factorize this joint distribution to get one term that’s the posterior, <span class="math notranslate nohighlight">\(p(\theta | \mathcal{D})\)</span>, and one term that’s the marginal, <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7b4ccc7e-cc87-4319-b8af-40124ed52883">
<span class="eqno">(18.5)<a class="headerlink" href="#equation-7b4ccc7e-cc87-4319-b8af-40124ed52883" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \frac{\int p(\mathcal{D}^* | \theta, \mathcal{D}) \cdot p(\theta | \mathcal{D}) \cdot p(\mathcal{D}) \cdot d\theta}{p(\mathcal{D})} 
\end{align}\]</div>
<p>Since <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\theta\)</span>, we can take it out of the integral, thereby canceling it with the <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span> in the denominator:</p>
<div class="amsmath math notranslate nohighlight" id="equation-83f448f3-94e6-40ef-abb2-378e1e18ae8c">
<span class="eqno">(18.6)<a class="headerlink" href="#equation-83f448f3-94e6-40ef-abb2-378e1e18ae8c" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \int p(\mathcal{D}^* | \theta, \mathcal{D}) \cdot p(\theta | \mathcal{D}) \cdot d\theta
\end{align}\]</div>
<p>Finally, using the laws of conditional independence, we know that <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \theta, \mathcal{D}) = p(\mathcal{D}^* | \theta)\)</span>. This is because, by conditioning on <span class="math notranslate nohighlight">\(\theta\)</span>, we remove all paths connecting <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> to <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span>. In other words, <span class="math notranslate nohighlight">\(\theta\)</span> summarizes all information from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> needed to predict <span class="math notranslate nohighlight">\(\mathcal{D}^*\)</span>. This gives us the following equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f5e39904-695b-4839-ae98-01466c0b2fa0">
<span class="eqno">(18.7)<a class="headerlink" href="#equation-f5e39904-695b-4839-ae98-01466c0b2fa0" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \int \underbrace{p(\mathcal{D}^* | \theta)}_{\text{likelihood of new data}} \cdot \underbrace{p(\theta | \mathcal{D})}_{\text{posterior}} \cdot d\theta
\end{align}\]</div>
<p>As you can see, <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span> is a function of the posterior and the joint data likelihood of the new data. Adding some syntactic sugar, we can write the above equation as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3b2e0a78-c52c-4ae8-97b3-16161f56afcf">
<span class="eqno">(18.8)<a class="headerlink" href="#equation-3b2e0a78-c52c-4ae8-97b3-16161f56afcf" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;= \mathbb{E}_{p(\theta | \mathcal{D})} \left[ p(\mathcal{D}^* | \theta) \right]
\end{align}\]</div>
<p>This shows that to evaluate <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \mathcal{D})\)</span>, we need to:</p>
<ol class="arabic simple">
<li><p>Draw posterior samples <span class="math notranslate nohighlight">\(\theta \sim p(\theta | \mathcal{D})\)</span>.</p></li>
<li><p>Average the likelihood <span class="math notranslate nohighlight">\(p(\mathcal{D}^* | \theta)\)</span> across these samples.</p></li>
</ol>
<p>As you can see this matches our intuition exactly!</p>
</section>
<section id="laws-of-conditional-independence">
<h2><span class="section-number">18.3. </span>Laws of Conditional Independence<a class="headerlink" href="#laws-of-conditional-independence" title="Permalink to this heading">#</a></h2>
<p>Recall that the slope and intercept of a model may have been sampled independently under the prior, but as soon as we condition on data, they are no longer independent.</p>
</section>
<section id="posterior-predictive-of-different-models">
<h2><span class="section-number">18.4. </span>Posterior Predictive of Different Models<a class="headerlink" href="#posterior-predictive-of-different-models" title="Permalink to this heading">#</a></h2>
<div class="admonition-exercise-derive-the-posterior-predictive-distribution admonition">
<p class="admonition-title">Exercise: Derive the Posterior Predictive Distribution</p>
<p>For each of the models below, draw the directed graphical model (that captures both the train and test data). Then, derive the posterior predictive formula.</p>
<p><strong>Part 1:</strong> Bayesian predictive model.</p>
<div class="amsmath math notranslate nohighlight" id="equation-017d092f-c03a-446d-bb49-5117c84ccd31">
<span class="eqno">(18.9)<a class="headerlink" href="#equation-017d092f-c03a-446d-bb49-5117c84ccd31" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;\sim p_\theta(\cdot) \\
y_n | x_n, \theta &amp;\sim p_{Y | X}(\cdot | x_n, \theta)
\end{align}\]</div>
<p><strong>Part 2:</strong> Bayesian Concept-Bottlebeck model (CBM). CBMs aim to make it easier to interpret model predictions. They do this by combining two models:</p>
<ul class="simple">
<li><p>CMBs first learning to predict “concepts” <span class="math notranslate nohighlight">\(c_n\)</span> associated, associated with input <span class="math notranslate nohighlight">\(x_n\)</span>. In a CBM, a concept is just a discrete attribute associated with the input; for example, if <span class="math notranslate nohighlight">\(x_n\)</span> is an image of wildlife, a concept could be rain, grass, dog, etc. You can think of <span class="math notranslate nohighlight">\(p_{C | X}\)</span> as a classifier.</p></li>
<li><p>After having predicted the concept <span class="math notranslate nohighlight">\(c_n\)</span> from the input <span class="math notranslate nohighlight">\(x_n\)</span>, CBMs attempt to predict the final output <span class="math notranslate nohighlight">\(y_n\)</span> from the concept only. In this way, predictions of <span class="math notranslate nohighlight">\(y_n\)</span> can be analyzed in terms of the concepts, which as easier to understand, instead of with respect to the inputs, which could be high dimensional and difficult to reason about.</p></li>
</ul>
<p>A Bayesian CBM has the following generative process:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8c84ec7b-e833-43f0-9a5c-d76c5f762a33">
<span class="eqno">(18.10)<a class="headerlink" href="#equation-8c84ec7b-e833-43f0-9a5c-d76c5f762a33" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;\sim p_\theta(\cdot) \\
\phi &amp;\sim p_\phi(\cdot) \\
c_n | x_n, \theta &amp;\sim p_{C | X}(\cdot | x_n, \theta) = \mathrm{Cat}(\pi(x_n; \theta)) \\
y_n | c_n, \phi &amp;\sim p_{Y | C}(\cdot | c_n, \phi),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi(\cdot; \theta)\)</span> is a function that maps <span class="math notranslate nohighlight">\(x_n\)</span> to the parameters of a categorical distribution.</p>
<p><strong>Part 3:</strong> Bayesian Factor Analysis.</p>
<div class="amsmath math notranslate nohighlight" id="equation-fe1d90a0-16f0-4f1a-9cf3-97a6a909a614">
<span class="eqno">(18.11)<a class="headerlink" href="#equation-fe1d90a0-16f0-4f1a-9cf3-97a6a909a614" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;\sim p_\theta(\cdot) \\
z_n &amp;\sim p_Z(\cdot) \\
x_n | z_n, \theta &amp;\sim p_{X | Z, \theta}(\cdot | z_n, \theta) 
\end{align}\]</div>
<p><strong>Part 4:</strong> Bayesian predictive model with input noise.</p>
<div class="amsmath math notranslate nohighlight" id="equation-5147295a-cf71-45db-8e24-cc95408c64b1">
<span class="eqno">(18.12)<a class="headerlink" href="#equation-5147295a-cf71-45db-8e24-cc95408c64b1" title="Permalink to this equation">#</a></span>\[\begin{align}
\theta &amp;\sim p_\theta(\cdot) \\
z_n &amp;\sim p_Z(\cdot) \\
y_n | x_n, z_n, \theta &amp;\sim p_{Y | X, Z, \theta}(\cdot | x_n, z_n, \theta) 
\end{align}\]</div>
</div>
</section>
<section id="model-evaluation-using-the-posterior-predictive">
<h2><span class="section-number">18.5. </span>Model Evaluation using the Posterior Predictive<a class="headerlink" href="#model-evaluation-using-the-posterior-predictive" title="Permalink to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="prior-and-posterior.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">17. </span>Bayesian Inference: Prior and Posterior</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-model-averaging">18.1. Intuition: Model Averaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivation-of-the-posterior-predictive">18.2. Derivation of the Posterior Predictive</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laws-of-conditional-independence">18.3. Laws of Conditional Independence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-predictive-of-different-models">18.4. Posterior Predictive of Different Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation-using-the-posterior-predictive">18.5. Model Evaluation using the Posterior Predictive</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <footer>
  <div class="flex-shrink-0 container">
    <div class="row align-items-center">
      <div class="col-6">
        &copy; Copyright 2024 Yaniv Yacoby
      </div>      
      <div class="col-6">
        <img src="_static/img/wc-logo-blue.png" alt="Wellesley College Logo" class="only-light" style="width: 49%; max-width: 120px; float: right; display: block;"/>
        <img src="_static/img/wc-logo-white.png" alt="Wellesley College Logo" class="only-dark" style="width: 49%; max-width: 120px; float: right; display: block;"/>
      </div>
    </div>    
  </div>  
</footer>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>