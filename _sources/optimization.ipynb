{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some helper functions (please ignore this!)\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** We can use `NumPyro` to perform the MLE on a class of models, composed of discrete distributions. The MLE involves solving an optimization problem---finding the parameters that maximize the joint data likelihood. So far, we've let `NumPyro` take care of this optimization problem for us. But what is `NumPyro` doing under the hood? \n",
    "\n",
    "**Outline:**\n",
    "* Introduce (exact) analytical optimization, which rely on person to do most of the work via pen-and-paper math.\n",
    "* Introduce (approximate) numerical optimization, which rely on the machine to do the work for us, like `NumPyro`.\n",
    "* Discuss tradeoffs between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic Solutions to Optimization Problems\n",
    "\n",
    "**Goal:** Recall that our goal is to maximize the joint data log-likelihood:\n",
    "\\begin{align}\n",
    "\\theta^\\text{MLE} &= \\mathrm{argmax}_\\theta \\text{ } p(\\mathcal{D}; \\theta),\n",
    "\\end{align}\n",
    "where $\\theta$ represents our model parameters. We will remain consistent with the convention of ML literature; we re-write the above problem as a minimization problem of our *negative* log-likelihood. \n",
    "\\begin{align}\n",
    "\\theta^\\text{MLE} &= \\mathrm{argmin}_\\theta \\underbrace{-p(\\mathcal{D}; \\theta)}_{\\text{Our loss function, } \\mathcal{L}(\\theta)}\n",
    "\\end{align}\n",
    "We call the $\\mathcal{L}(\\theta)$ our \"loss function,\" since our goal is now to minimize our losses.\n",
    "\n",
    "**Intuition:** So how can we identify the minima of $\\mathcal{L}(\\theta)$? A good place to start is to determine what makes $\\mathcal{L}(\\theta)$ different at its minima. Let's see if we can get some intuition by looking at some intuition by looking at loss functions we made up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} _static/figs/example_loss_functions.png\n",
    "---\n",
    "width: 700px\n",
    "name: fig-example-loss-functions\n",
    "align: center\n",
    "---\n",
    "Two \"made-up\" loss functions---what properties do the minima share?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the left plot above, we see that the minimum has a unique property: the loss function is *flat at the minimum*. In other words, at the minimum, the derivative of the loss function equals zero: \n",
    "\\begin{align}\n",
    "\\frac{d \\mathcal{L}(\\theta)}{d \\theta} = 0\n",
    "\\end{align}\n",
    "Does the same hold for the right plot? Not exactly... At the minimum, we do have $\\frac{d \\mathcal{L}(\\theta)}{d \\theta} = 0$. But there are other points for which we also have $\\frac{d \\mathcal{L}(\\theta)}{d \\theta} = 0$. \n",
    "This shows us that there are two *types* of optima: *global* and *local*. The *global minima* are the $\\theta$'s that make $\\mathcal{L}(\\theta)$ smaller than all other $\\theta$'s. On the other hand, *local minima* are $\\theta$'s that make $\\mathcal{L}(\\theta)$ smaller than all other $\\theta$'s in their neighborhood. Let's make this clear with the following illustration.\n",
    "\n",
    "```{figure} _static/figs/global-vs-local-optima.png\n",
    "---\n",
    "width: 400px\n",
    "name: fig-global-vs-local-optima\n",
    "align: center\n",
    "---\n",
    "Types of optima. Adapted from [this website](https://ludovicarnold.com/teaching/optimization/problem-statement/).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if our idea of setting the gradient of the loss to zero gives us both global- and local- minima and maxima, is it still useful? Yes, looking at the $\\theta$'s for which the loss is flat is still helpful! Instead of looking at every possible value of $\\theta$ (in these plots, $\\theta$ can take on infinite different values), we just need to examine the points for which the loss is flat. For the plot on the left, this strategy directly found the global minimum. For the plot on the right, this strategy found a small set of points that *includes* the global minimum. To find the global minimum within this set, all we need to do is evaluate the loss at each point and select the one that yields the smallest loss.\n",
    "\n",
    "**Procedure:** We can turn this intuition into the following four-step process.\n",
    "1. Compute the derivative of the loss function: $\\frac{d \\mathcal{L}(\\theta)}{d \\theta}$\n",
    "2. Set the derivative of the loss function equal to zero: $\\frac{d \\mathcal{L}(\\theta)}{d \\theta} = 0$\n",
    "3. Solve the equation for *all possible values* of $\\theta$ analytically (i.e. on pen-and-paper)---this is the difficult part!\n",
    "4. Plug each possible value of $\\theta$ into our loss function $\\mathcal{L}(\\theta)$ and select the one(s) that minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example: Analytically Solving for the MLE\n",
    "\n",
    "**The Model.** Let's see how this works by analytically performing the MLE on a simple example. Suppose we want to model the probability of a patient being hospitalized overnight. We can do this using a Bernoulli distribution:\n",
    "\\begin{align}\n",
    "H \\sim p_H(\\cdot; \\rho) = \\mathrm{Bern}(\\rho).\n",
    "\\end{align}\n",
    "Recall that the PMF of a Bernoulli RV is,\n",
    "\\begin{align}\n",
    "p_H(h; \\rho) = \\rho^{\\mathbb{I}(h = 1)} \\cdot (1 - \\rho)^{\\mathbb{I}(h = 0)},\n",
    "\\end{align}\n",
    "where $\\mathbb{I}(\\cdot)$ is an *indicator variable*---it evaluates to 1 if the condition in parentheses is true and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Joint Data Likelihood.** Now, let's write the joint data log-likelihood for our model:\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}; \\rho) &= \\log \\prod\\limits_{n=1}^N p(\\mathcal{D}_n; \\rho) \\quad (\\text{since observations are i.i.d}) \\\\\n",
    "&= \\log \\prod\\limits_{n=1}^N p_H(h_n; \\rho) \\\\\n",
    "&= \\log \\prod\\limits_{n=1}^N \\rho^{\\mathbb{I}(h_n = 1)} \\cdot (1 - \\rho)^{\\mathbb{I}(h_n = 0)} \\quad (\\text{using the definition of Bernoulli PMF}) \\\\\n",
    "&= \\sum\\limits_{n=1}^N \\log \\rho^{\\mathbb{I}(h_n = 1)} + \\log (1 - \\rho)^{\\mathbb{I}(h_n = 0)} \\quad (\\text{using the fact that } \\log (x \\cdot y) = \\log x + \\log y) \\\\\n",
    "&= \\sum\\limits_{n=1}^N \\mathbb{I}(h_n = 1) \\cdot \\log \\rho + \\mathbb{I}(h_n = 0) \\cdot \\log (1 - \\rho) \\quad (\\text{using the fact that } \\log x^y = y \\cdot \\log x)  \\\\\n",
    "&= \\underbrace{\\left( \\sum\\limits_{n=1}^N \\mathbb{I}(h_n = 1) \\right)}_{\\text{Total number of times $H = 1$}} \\cdot \\log \\rho + \\underbrace{\\left( \\sum\\limits_{n=1}^N \\mathbb{I}(h_n = 0) \\right)}_{\\text{Total number of times $H = 0$}} \\cdot \\log (1 - \\rho) \\quad (\\text{moving terms that do not depend on the sums out}) \\\\\n",
    "&= T \\cdot \\log \\rho + (N - T) \\cdot \\log (1 - \\rho)\n",
    "\\end{align}\n",
    "where $T = \\sum\\limits_{n=1}^N \\mathbb{I}(h_n = 1)$ is the total number of hospitalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The MLE Objective.** Our MLE objective is therefore:\n",
    "\\begin{align}\n",
    "\\rho^\\text{MLE} &= \\mathrm{argmax}_{\\rho} \\text{ } \\log p(\\mathcal{D}; \\rho) \\\\\n",
    "&= \\mathrm{argmax}_{\\rho} \\left( T \\cdot \\log \\rho - (N - T) \\cdot \\log (1 - \\rho) \\right) \\\\\n",
    "&= \\mathrm{argmin}_{\\rho} \\underbrace{\\left( -T \\cdot \\log \\rho + (N - T) \\cdot \\log (1 - \\rho) \\right)}_{\\text{Our loss function: } \\mathcal{L}(\\rho)} \\quad (\\text{maximizing a function is equivalent to minimizing its negative})\n",
    "\\end{align}\n",
    "\n",
    "**Analytic Optimization.** We take the gradient of the above loss $\\mathcal{L}(\\rho)$ with respect to $\\rho$, set it to $0$ and solve:\n",
    "\\begin{align}\n",
    "0 &= \\frac{d \\mathcal{L}(\\rho)}{d \\rho} \\\\\n",
    "&= -\\frac{T}{\\rho} + \\frac{N - T}{1 - \\rho} \\quad (\\text{taking the derivative of } \\mathcal{L}(\\rho)) \\\\\n",
    "&= \\frac{T - N \\cdot \\rho}{\\rho \\cdot (\\rho - 1)} \\quad (\\text{bringing fractions under common denominator}) \\\\\n",
    "&= T - N \\cdot \\rho \\quad (\\text{multiplying both sides by } \\rho \\cdot (\\rho - 1))\n",
    "\\end{align}\n",
    "Solving the above gives us the solution,\n",
    "\\begin{align}\n",
    "\\rho &= \\frac{T}{N}.\n",
    "\\end{align}\n",
    "The solution is exactly the proportion of hospitalizations out of the total number of hospital visits!\n",
    "\n",
    "**A Note on Constraint Optimization.** Oftentimes, when performing the MLE analytically, we need to constrain the parameters to lie within valid ranges. For example, $\\rho$ should only be allowed to take on values between $0$ and $1$. Formally, we express such a constraint as follows:\n",
    "\\begin{align}\n",
    "\\rho^\\text{MLE} &= \\mathrm{argmax}_{\\rho} \\text{ } \\log p(\\mathcal{D}; \\rho) \\quad \\text{subject to} \\quad 0 \\leq \\rho \\leq 1 \\\\\n",
    "\\end{align}\n",
    "In our derivation, it just so happens that the solution already satisfies this constraint. However, for different models, we may have to enforce such constraints explicitly. To enforce these constraints, one can use [lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier). We will not cover this method in here, but just want to point out that this class of methods exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with Analytic Solutions to Optimization Problems\n",
    "\n",
    "As you can see from the above example, performing optimization analytically suffers from two challenges:\n",
    "1. **Analytic optimization needs a specialized solution for every model.** As you can see from the above example, performing the optimization analytically will require a new derivation for each model. However, when working with real data, we rarely know what's the \"right\" model a priori. We typically start with an exploratory data analysis, try different models, evaluate them using different metrics, make hypotheses for why the models don't fit well, revise the models to fit better, and repeat. If we had to derive a new solution for every model we wish to test, our modeling process will become quite cumbersome. Moreover, we are more prone to make errors in the derivation and write bugs in our code. \n",
    "2. **Analytic optimization cannot solve for the parameters of every model.** Since the above example is for a simple Bernoulli model, an analytic solution to the MLE exists. However, for more complex problems, that may not be the case. In fact, for modern ML models, it is *rare* for there to exist an analytic solution.\n",
    "\n",
    "This motivates us to look into alternative optimization methods: numeric optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Solutions to Optimization Problems\n",
    "\n",
    "Numeric optimizations algorithms address both shortcomings of analytic optimization:\n",
    "1. They can be easily applied to different models without cumbersome derivations. This is because they can be conveniently implemented behind abstractions. Moreover, these abstractions allow us to pair different numeric optimization algorithms with different models *without having to write much code*. They even make it easy to incorporate constraints over the parameters into the optimization. This helps us write bug-free, error-free optimization code.\n",
    "2. They can be applied to optimization problems for which there exists no analytic optimization solution. They are also fast and work well in practice, making them extremely popular for complicated modern ML models.\n",
    "\n",
    "Of course, these numerical algorithms have their own challenges---we'll look into some challenges they face in a bit. Let us introduce the simplest and most popular numerical optimization algorithm: *gradient descent*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent.** The gradient (or \"multivariate derivative\") is the direction of the steepest ascent. Similarly, the negative gradient is the direction of the steepest descent. The idea behind the *gradient descent* algorithm is to take little steps in the direction of the negative gradient in hope that, after taking enough steps, we'll reach the minimum. Let's illustrate this with an animation:\n",
    "\n",
    "```{figure} https://miro.medium.com/v2/resize:fit:875/1*vchQOUUW_RPVjkb4pQF-1A.gif\n",
    "---\n",
    "width: 500px\n",
    "name: fig-gradient-descent-gif\n",
    "align: center\n",
    "---\n",
    "Animation of gradient descent on a 2-dimensional function. GIF from [this website](https://towardsai.net/p/machine-learning/deep-learning-from-scratch-in-modern-c-gradient-descent).\n",
    "```\n",
    "\n",
    "In the animation, the vertical axis represents $\\mathcal{L}(\\theta)$. The two horizontal axes represent the dimensions of $\\theta$ (in this case, it's 2-dimensional). Each arrow represents the negative gradient. As you can see, in each iteration of the algorithm, $\\theta$ (the red point) moves in the direction of the gradient, progressively minimizing the loss.\n",
    "\n",
    "Now, let's write the gradient descent algorithm. For clarity, we'll write it out with the notation for a 1-dimensional $\\theta$ (the multivariate version is pretty much the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:algorithm} Gradient Descent\n",
    ":label: gradient-descent\n",
    "\n",
    "**Inputs.** A loss function, $\\mathcal{L}(\\theta)$, a choice for the learning rate, $\\alpha$, and an initialization for the parameters, $\\theta^\\text{current} \\leftarrow \\text{initial value}$.\n",
    "\n",
    "**Output.** Return a parameter $\\theta$ that (hopefully) minimizes the loss $\\mathcal{L}(\\theta)$.\n",
    "\n",
    "**Algorithm.** Repeat until the loss doesn't change much from iteration to iteration:\n",
    "1. Compute the gradient of the loss with respect to the parameters, evaluated at the current value of the parameters:\n",
    "  \\begin{align}\n",
    "    u &\\leftarrow \\frac{d \\mathcal{L}(\\theta)}{d \\theta} \\Bigg|_{\\theta = \\theta^\\text{current}}\n",
    "  \\end{align}\n",
    "2. Take a step in the direction of the negative gradient (steepest descent):\n",
    "  \\begin{align}\n",
    "    \\theta^\\text{next} &\\leftarrow \\theta^\\text{current} - \\alpha \\cdot u\n",
    "  \\end{align}\n",
    "3. Update the model parameters:\n",
    "  \\begin{align}\n",
    "    \\theta^\\text{current} &\\leftarrow \\theta^\\text{next}\n",
    "  \\end{align}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this algorithm, notice that there's one variable we have yet to define: $\\alpha$. Here, $\\alpha$ represents the size of the step we plan to take in the direction of the gradient. It is typically called the *learning rate*. You will have to play with this parameter to determine what value works best for minimizing your loss. According to lore, a good place to start is with $\\alpha = 0.01$. Why? Who knows...\n",
    "\n",
    "**Simple Implementation of Gradient Descent.** Even though `NumPyro` already comes with [several different gradient-based optimization algorithms](https://num.pyro.ai/en/stable/optimizers.html), let's implement the above univariate gradient descent algorithm. While we're at it, let's have the algorithm keep track of how our parameters $\\theta$ change with each iteration to get some more intuition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "def univariate_gradient_descent(loss_fn, num_iterations, learning_rate, theta_init):\n",
    "    # Initialize theta     \n",
    "    theta = theta_init\n",
    "\n",
    "    # For each iteration of the algorithm...\n",
    "    for i in range(num_iterations):\n",
    "        # Use Jax to compute the gradient of the loss with respect to theta\n",
    "        gradient_fn = jax.grad(loss_fn)\n",
    "\n",
    "        # Evaluate the gradient at the current value of theta\n",
    "        u = gradient_fn(theta)\n",
    "\n",
    "        # Take a step in the direction of the gradient\n",
    "        theta = theta - learning_rate * u\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if it can successfully find the minimum of a quadratic loss function: $\\mathcal{L}(\\theta) = \\theta^2$. Since this is a parabola, we know the minimum of this formula should be at $\\theta = 0$. Will our algorithm find it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum at theta = -4.0740736e-10\n"
     ]
    }
   ],
   "source": [
    "# Define our loss function\n",
    "def quadratic_loss(theta):\n",
    "    return theta ** 2.0\n",
    "\n",
    "# Run gradient descent\n",
    "minimum = univariate_gradient_descent(quadratic_loss, 100, 0.1, jnp.array(-2.0))\n",
    "\n",
    "# Print out the minimum\n",
    "print('Minimum at theta =', minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, gradient descent successfully found the minima, approximately---the resultant number is very close to 0. Let's animate the algorithm to see how it finds the minima. We'll do this using two different learning rates: first with $\\alpha = 0.1$, and then with $\\alpha = 0.01$. \n",
    "\n",
    "```{figure} _static/figs/gradient_descent_quadratic_fn_lr0p1.gif\n",
    "---\n",
    "width: 500px\n",
    "name: fig-dg-quad-lr0p1\n",
    "align: center\n",
    "---\n",
    "For this loss function, a large learning rate causes gradient descent to converge quickly to the minimum.\n",
    "```\n",
    "\n",
    "```{figure} _static/figs/gradient_descent_quadratic_fn_lr0p01.gif\n",
    "---\n",
    "width: 500px\n",
    "name: fig-dg-quad-lr0p01\n",
    "align: center\n",
    "---\n",
    "For this loss function, a small learning rate causes gradient descent to converge quickly to the minimum.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensuring Parameters Satisfy Constraints.** Sometimes we need our parameters to satisfy certain properties. For example, in the above example, in which we computed the MLE for the Bernoulli model, we need to ensure our parameter, $\\rho$, lies on the interval $[0, 1]$. How can we satisfy this constraint using gradient descent? We define $\\rho$ as a *function* of another variable, $\\theta$, such that the unconstrained $\\theta$ (i.e. $\\theta \\in \\mathbb{R}$) is transformed into a constrained $\\rho$ (i.e. $\\rho \\in [0, 1]$):\n",
    "\\begin{align}\n",
    "\\rho &= g(\\theta).\n",
    "\\end{align}\n",
    "We then perform the optimization over $\\theta$. To make this possible, we need a function $g(\\cdot)$ that maps the real-line to the unit interval (i.e. $g: \\mathbb{R} \\rightarrow [0, 1]$). For this, we can use a function known as a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function):\n",
    "\\begin{align}\n",
    "g(\\theta) = \\frac{1}{1 + e^{-\\theta}}.\n",
    "\\end{align}\n",
    "This is how constraints (such as `C.unit_interval`) in `NumPyro` are implemented internally. Each constraint relies on a different transform, $g(\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatic Differentiation.** So how does `NumPyro` know to compute gradients automatically for us? The magic is in the function `jax.grad`, which uses an algorithm called \"automatic differentiation\" or \"backpropagation\" to compute exact (not approximate) gradients. We will not get into this topic in the course, but if you're interested, check out [this video](https://www.youtube.com/watch?v=_-D0DiN49fc&t=415s) to learn more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with Numeric Optimization\n",
    "\n",
    "While so far, gradient descent seems like magic, it definitely has its own drawbacks. Let's look at how gradient descent performs on a more \"wiggly\" loss function---what do you notice about it?\n",
    "\n",
    "```{figure} _static/figs/gradient_descent_quadratic_plus_sin_fn_lr0p01.gif\n",
    "---\n",
    "width: 500px\n",
    "name: fig-dg-quad-plus-sin-lr0p01\n",
    "align: center\n",
    "---\n",
    "For this loss function, a small learning rate causes gradient descent to get stuck in a local minimum near its initialization. \n",
    "```\n",
    "\n",
    "```{figure} _static/figs/gradient_descent_quadratic_plus_sin_fn_lr0p1.gif\n",
    "---\n",
    "width: 500px\n",
    "name: fig-dg-quad-plus-sin-lr0p1\n",
    "align: center\n",
    "---\n",
    "For this loss function, a large learning rate helps step out of local minima, but prevents gradient descent from honing in on and ultimately \"sticking\" to the minimum.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from these animations, gradient descent struggles with a few challenges:\n",
    "1. **Gradient Descent is Prone to Local Optima.** Gradient descent is prone to getting stuck in local optima. Moreover, it's not possible for us to determine whether an optima is local or global. For some complicated ML models, such local optima can be \"good enough\" for use, and for others they are not.\n",
    "2. **Gradient Descent is Sensitive to Initialization.** Since gradient descent tends to get stuck in local optima that are relatively close to its initialization, it's important that we try many different initializations to empirically find one that works well. This can be time-consuming.\n",
    "3. **Gradient Descent is Sensitive to Hyper-parameters.** As you saw from the above animations, the choice of learning rate and number of iterations change the behavior of gradient descent drastically. This means than whenever we use such numeric optimization algorithm, we will have to try a bunch of different settings of these \"hyper-parameters\" to empirically see what works best. This can sometimes be time-consuming and cumbersome.\n",
    "4. **Gradient Descent Adds Diagnosic Challenges.** Now that we've introduced an approximate component to our modeling toolkit---numeric optimization---it will be difficult to diagnose *why* our ML method performs poorly. For example, if our model fits our data poorly, is it because our modeling assumptions (e.g. our choice of distributions) are inappropriate, or is it because our optimizer got stuck in some local minima?\n",
    "\n",
    "Given these challenges, it is important for us to use numeric optimization algorithms responsibly: to be aware of all the ways they might fail us, and to think critically about how these failures impact the downstream effects of our ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
