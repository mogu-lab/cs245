{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some helper functions (please ignore this!)\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** We can perform MLE on a class of models, composed of discrete distributions. \n",
    "\n",
    "**Challenge:**\n",
    "\n",
    "**Outline:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic Solutions to Optimization Problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example of Analytically Solving for the MLE\n",
    "\n",
    "**The Model.** Let's see how this works by analytically performing the MLE on a simple example. Suppose we want to model the probability of a patient being hospitalized overnight. We can do this using a Bernoulli distribution:\n",
    "\\begin{align}\n",
    "H \\sim p_H(\\cdot; \\rho) = \\mathrm{Bern}(\\rho).\n",
    "\\end{align}\n",
    "Recall that the PMF of a Bernoulli RV is,\n",
    "\\begin{align}\n",
    "p_H(h; \\rho) = \\rho^{\\mathbb{I}(h = 1)} \\cdot (1 - \\rho)^{\\mathbb{I}(h = 0)},\n",
    "\\end{align}\n",
    "where $\\mathbb{I}(\\cdot)$ is an *indicator variable*---it evaluates to 1 if the condition in parentheses is true and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Joint Data Likelihood.** Now, let's write the joint data log-likelihood for our model:\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}; \\rho) &= \\log \\prod\\limits_{n=1}^N p(\\mathcal{D}_n; \\rho) \\quad (\\text{since observations are i.i.d}) \\\\\n",
    "&= \\log \\prod\\limits_{n=1}^N p_H(h_n; \\rho) \\\\\n",
    "&= \\log \\prod\\limits_{n=1}^N \\rho^{\\mathbb{I}(h_n = 1)} \\cdot (1 - \\rho)^{\\mathbb{I}(h_n = 0)} \\quad (\\text{using the definition of Bernoulli PMF}) \\\\\n",
    "&= \\sum\\limits_{n=1}^N \\log \\rho^{\\mathbb{I}(h_n = 1)} + \\log (1 - \\rho)^{\\mathbb{I}(h_n = 0)} \\quad (\\text{using the fact that } \\log (x \\cdot y) = \\log x + \\log y) \\\\\n",
    "&= \\sum\\limits_{n=1}^N \\mathbb{I}(h_n = 1) \\cdot \\log \\rho + \\mathbb{I}(h_n = 0) \\cdot \\log (1 - \\rho) \\quad (\\text{using the fact that } \\log x^y = y \\cdot \\log x)  \\\\\n",
    "&= \\underbrace{\\left( \\sum\\limits_{n=1}^N \\mathbb{I}(h_n = 1) \\right)}_{\\text{Total number of times $H = 1$}} \\cdot \\log \\rho + \\underbrace{\\left( \\sum\\limits_{n=1}^N \\mathbb{I}(h_n = 0) \\right)}_{\\text{Total number of times $H = 0$}} \\cdot \\log (1 - \\rho) \\quad (\\text{moving terms that do not depend on the sums out}) \\\\\n",
    "&= T \\cdot \\log \\rho + (N - T) \\cdot \\log (1 - \\rho)\n",
    "\\end{align}\n",
    "where $T = \\sum\\limits_{n=1}^N \\mathbb{I}(h_n = 1)$ is the total number of hospitalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The MLE Objective.** Our MLE objective is therefore:\n",
    "\\begin{align}\n",
    "\\rho^\\text{MLE} &= \\mathrm{argmax}_{\\rho} \\text{ } \\log p(\\mathcal{D}; \\rho) \\\\\n",
    "&= \\mathrm{argmax}_{\\rho} \\left( T \\cdot \\log \\rho - (N - T) \\cdot \\log (1 - \\rho) \\right) \\\\\n",
    "&= \\mathrm{argmin}_{\\rho} \\underbrace{\\left( -T \\cdot \\log \\rho + (N - T) \\cdot \\log (1 - \\rho) \\right)}_{\\text{Our loss function: } \\mathcal{L}(\\rho)} \\quad (\\text{maximizing a function is equivalent to minimizing its negative})\n",
    "\\end{align}\n",
    "\n",
    "**Analytic Optimization.** We take the gradient of the above loss $\\mathcal{L}(\\rho)$ with respect to $\\rho$, set it to $0$ and solve:\n",
    "\\begin{align}\n",
    "0 &= \\frac{d \\mathcal{L}(\\rho)}{d \\rho} \\\\\n",
    "&= -\\frac{T}{\\rho} + \\frac{N - T}{1 - \\rho} \\quad (\\text{taking the derivative of } \\mathcal{L}(\\rho)) \\\\\n",
    "&= \\frac{T - N \\cdot \\rho}{\\rho \\cdot (\\rho - 1)} \\quad (\\text{bringing fractions under common denominator}) \\\\\n",
    "&= T - N \\cdot \\rho \\quad (\\text{multiplying both sides by } \\rho \\cdot (\\rho - 1))\n",
    "\\end{align}\n",
    "Solving the above gives us the solution,\n",
    "\\begin{align}\n",
    "\\rho &= \\frac{T}{N}.\n",
    "\\end{align}\n",
    "The solution is exactly the proportion of hospitalizations out of the total number of hospital visits!\n",
    "\n",
    "**A Note on Constraint Optimization.** Oftentimes, when performing the MLE analytically, we need to constrain the parameters to lie within valid ranges. For example, $\\rho$ should only be allowed to take on values between $0$ and $1$:\n",
    "\\begin{align}\n",
    "\\rho^\\text{MLE} &= \\mathrm{argmax}_{\\rho} \\text{ } \\log p(\\mathcal{D}; \\rho) \\text{  subject to  } 0 \\leq \\rho \\leq 1 \\\\\n",
    "\\end{align}\n",
    "In the our derivation, it just so happens that the solution satisfies this constraint. However, for different models, we may have to enforce such constraints explicitly. This is typically done using [lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier). We will not cover this method in class, but just want to point out that it exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with Analytic Optimization\n",
    "\n",
    "**Needs a specialized solution for every model.** As you can see from the above example, performing the optimization analytically will require a new derivation for each model. However, when working with real data, we rarely know what's the \"right\" model a priori. We typically start with an exploratory data analysis, try different models, evaluate them using different metrics, make hypotheses for why the models don't fit well, revise the models to fit better, and repeat. If we had to derive a new solution for every model we wish to test, our modeling process will become quite cumbersome. Moreover, we are more prone to make errors in the derivation and write bugs in our code. \n",
    "\n",
    "**Cannot solve for the parameters for every model.** Since the above example is for a simple Bernoulli model, an analytic solution to the MLE exists. However, for more complex problems, that may not be the case. In fact, for modern ML models, it is *rare* for there to exist an analytic solution.\n",
    "\n",
    "**What can we do to overcome these challenges?** We can use numeric optimization algorithms. Numeric algorithms can be conveniently implemented behind abstractions, allowing us to pair different numeric optimization algorithms with different models *without having to write much code*. They even make it easy to incorporate constraints over the parameters into the optimization. This helps us write bug-free, error-free optimization code. Moreover, in practice, numerical optimization algorithms often achieve good approximate solutions for modern, complex models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Solutions to Optimization Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with Numeric Optimization\n",
    "\n",
    "**Local optima.**\n",
    "\n",
    "**Hyper-parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
