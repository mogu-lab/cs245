{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4896889-43a8-4e1c-b5f8-a0d012c463fe",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b63666-0869-4b9a-82d6-e44d2a6407fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some helper functions (please ignore this!)\n",
    "from utils import * \n",
    "from cs349 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a516c-738e-43df-b107-be3696893e09",
   "metadata": {},
   "source": [
    "**Context:** At this point, our modeling toolkit is already getting quite expressive. \n",
    "1. We can develop simple *predictive models* using *conditional distributions*: we can specify models of the form $p_{A | B}(a | b)$, which allow us to predict the probability that $A = a$ given that $B = b$. We do this by specifying a distribution over random variable (RV) $A$, whose parameters are a *function* of $b$.  \n",
    "2. We can develop simple *generative models* using *joint distributions*: we can specify models of the form $p_{A, B}(a, b)$, which allow us to sample (or generate) data. We do this by factorizing this joint probability into a product of conditional and marginal distributions, e.g. $p_{A, B}(a, b) = p_{A | B}(a | b) \\cdot p_B(b)$, which we already know how to specify.\n",
    "\n",
    "Of course, the predictive and generative models you may have heard about in the news are capable of doing more than the instances we've covert so far---we will build up to these fancy models over the course of the semester. What's important for now, though, is that you understand how such models can be represented using probability distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb748b3f-7e0a-486a-b581-392fdbdb478c",
   "metadata": {},
   "source": [
    "**Challenge:** So what stands in our way of applying our modeling tools to real-world data? First, we've only instantiated our models with *discrete* distributions. Many real-world data, however, requires *continuous* distributions; that is, distributions over real numbers (e.g. blood pressure, body-mass index, time spent in REM sleep, etc.). We'll get more into the details of continuous modeling a bit later. Our second obstacle is: we still don't have a way of *automatically* fitting a model to data. So far, you've fit all models to data by hand via inspection---you looked at the data and tried to match the model to the data. With increasing model and data complexity, it becomes prohibitively difficult to fit the model to the data by hand. Today, we'll introduce one technique for doing this: maximum likelihood estimation (MLE). This is the first algorithm that truly allows the \"machine to learn.\"\n",
    "\n",
    "The idea behind MLE is to find a model under which the probability of the data is highest. The intuition behind the MLE is that a model that scores the observed data as likely could have reasonably generated the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de18b8a-66d0-4b4e-b80a-35c1c08743b5",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "* Formally introduce and motivate the MLE.\n",
    "* Extend notation of directed graphical models to represent a full data-set instead of just one observation.\n",
    "* Implement MLE in `NumPyro`.\n",
    "\n",
    "Let's load our IHH ER data again so we remember what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c080d5-af0b-4dc0-a4e8-e14e43009508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a bunch of libraries we'll be using below\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpyro\n",
    "import numpyro.distributions as D\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "csv_fname = 'data/IHH-ER.csv'\n",
    "data = pd.read_csv(csv_fname, index_col='Patient ID')\n",
    "\n",
    "# Print a random sample of 5 patients, just to see what's in the data\n",
    "data.sample(15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952641d-67cc-4d9e-a375-fee3b0756d84",
   "metadata": {},
   "source": [
    "## MLE: Notation and Formalism\n",
    "\n",
    "The idea behind the MLE is to find the model parameters that maximize the probability of the data. Let's introduce some notation to help us formalize what this means mathematically.\n",
    "\n",
    "**Notation for Data.** Let $\\mathcal{D}$ denote our *all* of our observed data ($\\mathcal{D}$ represents the entirety of the above table). Let $\\mathcal{D}_n$ represent observation number $n$ (i.e. row $n$) from the table. $\\mathcal{D}_n$ is a tuple of values at each of the columns: $\\mathcal{D}_n = (d_n, c_n, h_n, a_n, m_n)$. Recall that we define:\n",
    "* $D$: Day-of-Week\t\n",
    "* $C$: Condition\t\n",
    "* $H$: Hospitalized\t\n",
    "* $A$: Antibiotics\n",
    "* $M$: Attempts-to-Disentangle\n",
    "\n",
    "**Notation for Parameters.** For simplicity, we've omitted the notation for each distribution's parameters from the notation so far. From now on, we'll explicitly write out the parameters as arguments to the distribution by listing them after a semi-colon. \n",
    "> For example, we can denote that a joint distribution over RVs $A$ and $B$ depends on a parameter $\\theta$ as follows: $p_{A, B}(a, b; \\theta)$. Similarly, we can write a conditional that depends on $\\theta$ using $p_{A | B}(A | B; \\theta)$.\n",
    ">\n",
    "> If different components of the distribution depend on different parameters, we can list them. For example, in a joint distribution over $A$ and $B$, we can have the conditional depend on $\\theta$ and the marginal depend on $\\phi$: $p_{A, B}(a, b; \\theta, \\phi) = p_{A | B}(a | b; \\theta) \\cdot p_B(b; \\phi)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12211fe-7912-48bc-9a62-cdfa503efabe",
   "metadata": {},
   "source": [
    "**The MLE Objective.**\n",
    "Let $\\theta$ denote the set of all parameters used in our model for the IHH ER data. Using the above notation, $p_{D, C, H, A, M}(\\mathcal{D}; \\theta)$ denotes the probability of the observed data. Our goal is then to find the parameters $\\theta$ that maximize the probability of having observed $\\mathcal{D}$:\n",
    "\\begin{align}\n",
    "\\theta^\\text{MLE} &= \\mathrm{argmax}_{\\theta} \\quad p(\\mathcal{D}; \\theta),\n",
    "\\end{align}\n",
    "wherein \"argmax\" denotes the value of $\\theta$ that maximizes the joint probability. So what does it mean to evaluate the probability of the *whole data*, $\\mathcal{D}$, under our model, $p_{D, C, H, A, M}$? It means evaluating the *joint distribution of all observations*, $\\mathcal{D}_n = (d_n, c_n, h_n, a_n, m_n)$ for every $n \\in [1, N]$:\n",
    "\\begin{align}\n",
    "\\theta^\\text{MLE} &= \\mathrm{argmax}_{\\theta} \\quad  p(\\mathcal{D}; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_{\\theta} \\quad p(\\mathcal{D}_1, \\cdots, \\mathcal{D}_N; \\theta),\n",
    "\\end{align}\n",
    "where $N$ is the total number of observations. \n",
    "\n",
    "Now, recall that every joint distribution can be factorized into a product of conditional and marginal distributions, and that the number of possible factorizations grows unwieldy very quickly with the number of variables. Since the number of variables in this joint distribution is a function of the number of observations, $N$, which is large (e.g. thousands), we need some way to select a reasonable factorization. As typical, we are going to assume that the observations are independent, and identically distributed (i.i.d). This means that one patient coming to the ER does not tell us anything about how likely other patients are to come to the ER. Now, recall that when two RVs are independent, their joint distribution equals a product of their marginals. We can therefore factorize the joint distribution as follows:\n",
    "\\begin{align}\n",
    "\\theta^\\text{MLE} &= \\mathrm{argmax}_{\\theta} \\quad  p(\\mathcal{D}; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_{\\theta} \\quad p(\\mathcal{D}_1, \\cdots, \\mathcal{D}_N; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_{\\theta} \\quad p(\\mathcal{D}_1; \\theta) \\cdot p(\\mathcal{D}_2; \\theta) \\cdots p(\\mathcal{D}_N; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_{\\theta} \\quad \\prod\\limits_{n=1}^N p(\\mathcal{D}_n; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_{\\theta} \\quad \\prod\\limits_{n=1}^N \\underbrace{p_{D, C, H, A, M}(d_n, c_n, h_n, a_n, m_n; \\theta)}_{\\text{We already know how to compute this!}}\n",
    "\\end{align}\n",
    "We have now arrived at a formula for the joint distribution that we know how to compute---we've even written code to evaluate it in `NumPyro`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f2bec-48cf-4aa6-81b7-767c5a20ba95",
   "metadata": {},
   "source": [
    "**Numerical Stability:** Notice that since our joint is a discrete probability distribution, it outputs probabilities between 0 and 1: $p_{D, C, H, A, M}(d_n, c_n, h_n, a_n, m_n; \\theta) \\in [0, 1]$. In other words, it outputs *fractions*. In the above formula, we then multiply these fractions times one another $N$ times. But what happens when you multiply fractions together many times? Answer: the results shrinks towards 0 very quickly (try it yourself!). This is a problem, because our computer can only represent small numbers up to a finite precision. For a large $N$, our computer will round down the answer to $0$, which will prevent us from performing the argmax. Because of this issue, we have to transform our original MLE objective into a problem that a computer can numerically solve. \n",
    "\n",
    "We do this by maximizing the $\\log$ of the joint probability for two reasons:\n",
    "1. Logs turn products into sums: $\\log X \\cdot Y = \\log X + \\log Y$. Applying this formula to our MLE objective results in a *sum* of fractions, which is numerically stable:\n",
    "\\begin{align}\n",
    "\\log \\prod\\limits_{n=1}^N p_{D, C, H, A, M}(d_n, c_n, h_n, a_n, m_n; \\theta) = \\sum\\limits_{n=1}^N \\log p_{D, C, H, A, M}(d_n, c_n, h_n, a_n, m_n; \\theta)\n",
    "\\end{align}\n",
    "2. But by maximizing the $\\log$ of the joint probability instead, will we get the wrong answer? Because the $\\log$ function is a *strictly increasing function*, our maxima will remain in the same location. That is:\n",
    "\\begin{align}\n",
    "\\theta^\\text{MLE} &= \\mathrm{argmax}_{\\theta} \\prod\\limits_{n=1}^N p_{D, C, H, A, M}(d_n, c_n, h_n, a_n, m_n; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_{\\theta} \\sum\\limits_{n=1}^N \\log p_{D, C, H, A, M}(d_n, c_n, h_n, a_n, m_n; \\theta)\n",
    "\\end{align}\n",
    "To illustrate point (2), check out the graph below, which shows that the argmax of a function doesn't change if a $\\log$ is applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28857fb4-5326-4ea0-9500-6e57ab510f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_invariance_of_argmax_under_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab29e7-286b-429e-9c27-6c070dcce0c4",
   "metadata": {},
   "source": [
    "**Optimization:** So at this point, we can compute the MLE objective for specific choices of $\\theta$, but we don't know yet how to perform the argmax operation. We'll introduce this concept a bit later in the course. For now, we'll provide you with a function that can perform the maximization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c66098-9474-4d91-ab56-70c764d6491c",
   "metadata": {},
   "source": [
    "## Graphically Representing I.I.D Observations and Model Parameters \n",
    "\n",
    "Before implementing the MLE in `NumPyro`, we will extend our Directed Graphical Model representation (DGM) to include i.i.d observations and model parameters. This will help us in the translation process from math to code. Suppose we have a simple joint distribution over two RVs, $A$ and $B$, whose conditional controlled by a parameter, $\\theta$, as follows:\n",
    "\\begin{align}\n",
    "p_{A, B}(a, b; \\theta) &= p_{B | A}(b | a; \\theta) \\cdot p_A(a)\n",
    "\\end{align}\n",
    "Suppose further that we have $N$ i.i.d observations from this joint distribution. That is, we have $\\mathcal{D}_n = (a_n, b_n)$ for $n \\in [1, N]$. This gives us the following joint distribution over the entire data:\n",
    "\\begin{align}\n",
    "p(\\mathcal{D}; \\theta) &= \\prod\\limits_{n=1}^N p(\\mathcal{D}_n; \\theta) \\quad \\text{since the observations are i.i.d} \\\\\n",
    "&= \\prod\\limits_{n=1}^N p_{A, B}(a_n, b_n; \\theta) \\\\\n",
    "&= \\prod\\limits_{n=1}^N p_{B | A}(b_n | a_n; \\theta) \\cdot p_A(a_n)\n",
    "\\end{align}\n",
    "How would we represent this graphically? The answer is a little messy:\n",
    "<img align=\"center\" width=\"500px\" src=\"figs/mle-plate-and-parameter-example-bad.png\" />\n",
    "\n",
    "Each pair $(A_n, B_n)$ get its own arrow to signify the conditional dependence of $B_n$ on $A_n$. And since every pair depends on the same parameter, $\\theta$ has an arrow pointing into every $B_n$. \n",
    "\n",
    "**Representing Parameters.** In the above, notice that circles are only used for RVs. Since $\\theta$ is not an RV, it is not inside a circle---it's represented by a dot instead.\n",
    "\n",
    "**Representing I.I.D Observations:** For more complicated models, like the IHH ER you've already developed, this graphical representation becomes too difficult to read. As a result, we use the following short-hand:\n",
    "<img align=\"center\" width=\"500px\" src=\"figs/mle-plate-and-parameter-example-good.png\" />\n",
    "\n",
    "In this representation, we introduce a \"plate\" (the rectangle surrounding $A_n$ and $B_n$). The plate denotes that what's inside should be repeated $N$ times, where $N$ is written in the bottom-right corner. Why is this called a plate? Do you eat off of rectangular plates at home? This shall remain a mystery to us all...\n",
    "\n",
    "**A note on conditional independence:** We note that in this example model, for the IHH ER model you've developed, and generally for the models we consider in this class, the observations are only i.i.d given the model parameters. That is, given $\\theta$, we can factorize $p(\\mathcal{D}; \\theta)$ into $\\prod_{n=1}^N p(\\mathcal{D}_n; \\theta)$. However, if we do not \"condition\" on the parameters, $\\theta$, the observations do carry knowledge about one another. That is, having observed $\\mathcal{D}_1$ can tell me something about $\\mathcal{D}_2$ because it tells me something about $\\theta$, which is shared across all observations. More on that later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c147f-4eb0-4d02-9deb-4bc3d3bab597",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    "**Part 1:** Extend the graphical model for the IHH ER below to represent the entirety of the data. Additionally, include all parameters of all distributions.\n",
    "\n",
    "<img align=\"center\" width=\"500px\" src=\"figs/joint-probability-ihh-er-dgm.png\" />\n",
    "\n",
    "**Part 2:** TODO translate more between equations and DGMs!\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419c594-5a47-429d-96fb-f0a48b1751cc",
   "metadata": {},
   "source": [
    "## MLE in `NumPyro`\n",
    "\n",
    "We're now ready to translate our model into `NumPyro` code that we can then use to fit to data. To do this, we will rely on three `NumPyro` primitives to code the joint distribution of the whole data:\n",
    "* `numpyro.param`, which represents model parameters\n",
    "* `numpyro.sample`, which represents an RV\n",
    "* `numpyro.plate`, which represents i.i.d sampling using the \"plate\" notation\n",
    "\n",
    "We'll walk you through how to use these primitives to build the model, but we additionally recommend you get in the habit of reading the [relevant documentation](https://num.pyro.ai/en/stable/primitives.html).\n",
    "\n",
    "**Concept:** In `NumPyro`, models are implemented as Python functions, which use the above primitives to sample from the joint distribution. We do this by factorizing the joint distribution and sampling from its components in some valid ordering. This will suffice in providing `NumPyro` with enough information to do the heavy lifting of the MLE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5903403-779e-447f-8be0-bfb4df0ddcd3",
   "metadata": {},
   "source": [
    "**Example:** We'll illustrate how to do this using an example. Suppose we want to model the joint distribution of the day-of-the-week, $D$, and whether a patient is intorixated, $I$. We can do this via the model,\n",
    "\\begin{align}\n",
    "p(\\mathcal{D}; \\pi, \\rho) &= \\prod\\limits_{n=1}^N p_{I | D}(i_n | d_n; \\rho) \\cdot p_D(d_n; \\pi).\n",
    "\\end{align}\n",
    "In this model,\n",
    "* *Marginal:* $p_D(\\cdot; \\pi)$ is a distribution over the days of the week. We will therefore define it to be a Categorical distribution:\n",
    "\\begin{align}\n",
    "p_D(\\cdot; \\pi) = \\mathrm{Cat}(\\pi).\n",
    "\\end{align} Here, the parameter $\\pi$ is a 7-dimensional array describing the probability of an observation coming from each of the 7 days of the week. That is, the probability of $D = d_n$ is given by the $d_n$-th entry of $\\pi$: i.e. $p_{D}(d_n; \\pi) = \\pi_{d_n}$.\n",
    "* *Conditional:* $p_{I | D}(\\cdot | d_n; \\rho)$ is the probability that a patient arrives at the ER with intoxication given that $D = d_n$. Since \"intoxication\" is a binary outcome, we will model it using a Bernoulli. However, recall that the probability of intoxication *changes with the day of the week*---on some days, we're more likely to treat patients with intoxication. The parameter of the Bernoulli distribution therefore needs to change depending on the day of the week. We therefore define:\n",
    "\\begin{align}\n",
    "p_{I | D}(i_n | d_n; \\rho) = \\mathrm{Ber}(\\rho_{d_n}).\n",
    "\\end{align} By this, we mean that $\\rho$ is a 7-dimensional vector, where the $d_n$-th entry, denoted by $\\rho_{d_n}$, is the probability of intoxication on day $d_n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c59e1-9460-4b80-9aec-bfab166a2178",
   "metadata": {},
   "source": [
    "**Valid Parameter Values:** Since each entry of $\\rho$ represents a Bernoulli distribution, each entry must be on between 0 and 1 (i.e. on the \"unit interval\"). And since $\\pi$ is represents a Categorical distribution, its entries must sum to 1 (a fancy name of this is that $\\pi$ lies on a \"simplex\"). We note this briefly because we will have to communicate to `NumPyro` the valid values these parameters may take on (but don't worry, this isn't hard to do)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9a3bb-c7b4-436d-b637-8dd2d9b91eee",
   "metadata": {},
   "source": [
    "**Generative Process:** For this model, we assume the data was sampled (or generated) as follows:\n",
    "* *Step 1:* We choose an initial guess for $\\pi$ and $\\rho$. Our guess doesn't need to be good, since `NumPyro` help us find the right values.\n",
    "* *Step 2:* For every $n \\in [1, N]$:\n",
    "  * *Step 2a:* We sample $d_n$ from the marginal, $d_n \\sim p_D(\\cdot; \\pi) = \\mathrm{Cat}(\\pi)$. \n",
    "  * *Step 2b:* Given the specific value of $d_n$ from Step 2a, we sample from the conditional, $i_n | d_n \\sim p_{I | D}(\\cdot | d_n; \\rho) = \\mathrm{Ber}(\\rho_{d_n})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef364a-5817-404e-8001-6b3e07eaa026",
   "metadata": {},
   "source": [
    "**Code:** Now that we have our generative process spelled out, let's translate it directly into code.\n",
    "We start by creating a function to represent our model:\n",
    "```\n",
    "def model_of_intoxication_given_day(N, d=None, i=None):\n",
    "    pass\n",
    "```\n",
    "This function takes in several arguments:\n",
    "* The total number of observations, $N$.\n",
    "* Optional arrays describing the observations. That is, `d` represents an array of all $N$ days of the week, and `i` represents all $N$ binary values intoxication-yes/no. When sampling data from the model, we do not pass in `d` and `i`, but when performing the MLE, we do need to pass them in. This is because the MLE objective will try to find settings of $\\pi, \\rho$ that will make our observed data most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31464a-84e1-4450-855b-5174655590fc",
   "metadata": {},
   "source": [
    "Next, we can go through each step of the generative process above and translate it into `NumPyro`, line by line:\n",
    "* *Step 1:* We define our parameters using the `numpyro.param` primitive.\n",
    "```\n",
    "pi = numpyro.param(\n",
    "    'pi',                          # A name used by NumPyro\n",
    "    init_value=jnp.ones(7) / 7.0,  # Initial value for pi: [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7]\n",
    "    constraint=C.simplex,          # Entries of pi must sum to 1 (i.e. lie on a \"simplex\")\n",
    ")\n",
    "\n",
    "rho = numpyro.param(\n",
    "    'rho',                         # A name used by NumPyro\n",
    "    init_value=jnp.ones(7) * 0.5,  # Initial value for rho: [0.5, 0.5, 0.5, 0.5, 0.5. 0.5, 0.5]\n",
    "    constraint=C.unit_interval,    # Each entry of rho must be in [0, 1] (i.e. lie in \"unit interval\")\n",
    ")\n",
    "```\n",
    "You can find more types of constraints listed [here](https://num.pyro.ai/en/stable/_modules/numpyro/distributions/constraints.html).\n",
    "* *Step 2:* We create a \"plate\" to indicate $N$ i.i.d observations using `numpyro.plate`.\n",
    "```\n",
    "with numpyro.plate('data', N):\n",
    "    pass\n",
    "```\n",
    "  * *Step 2a:* Inside the plate, we sample $d_n$ from the marginal, $d_n \\sim p_D(\\cdot; \\pi) = \\mathrm{Cat}(\\pi)$, using `numpyro.sample`.\n",
    "```\n",
    "# Define marginal as Categorical using pi\n",
    "p_D = D.Categorical(pi)\n",
    "\n",
    "# Sample from the marginal\n",
    "d = numpyro.sample('d', p_D, obs=d)  \n",
    "```\n",
    "  * *Step 2b:* Inside the plate, we use the specific value of $d_n$ from Step 2a to sample from the conditional, $i_n | d_n \\sim p_{I | D}(\\cdot | d_n; \\rho) = \\mathrm{Ber}(\\rho_{d_n})$ using `numpyro.sample`.\n",
    "```\n",
    "# Define conditional as Bernoulli. Notice rho[d] to access the d-th entry of rho\n",
    "p_I_given_D = D.Bernoulli(rho[d])\n",
    "\n",
    "# Sample from the conditional \n",
    "i = numpyro.sample('i', p_I_given_D, obs=i)\n",
    "```\n",
    "\n",
    "Putting everything together, we arrive at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f9542-bde8-472a-a576-9f65a9455f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpyro\n",
    "import numpyro.distributions as D\n",
    "import numpyro.distributions.constraints as C\n",
    "\n",
    "\n",
    "def model_of_intoxication_given_day(N, d=None, i=None):\n",
    "    pi = numpyro.param(\n",
    "        'pi', \n",
    "        init_value=jnp.ones(7) / 7.0, \n",
    "        constraint=C.simplex,\n",
    "    )\n",
    "    \n",
    "    rho = numpyro.param(\n",
    "        'rho', \n",
    "        init_value=jnp.ones(7) * 0.5, \n",
    "        constraint=C.unit_interval,\n",
    "    )\n",
    "    \n",
    "    with numpyro.plate('data', N):\n",
    "        p_D = D.Categorical(pi)\n",
    "        d = numpyro.sample('d', p_D, obs=d)\n",
    "\n",
    "        p_I_given_D = D.Bernoulli(rho[d])\n",
    "        i = numpyro.sample('i', p_I_given_D, obs=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4457fe-5ac4-4087-960e-6f2f58003b35",
   "metadata": {},
   "source": [
    "Let's check to see if we coded the model correctly by asking `NumPyro` to visualize the directed graphical model for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7375356-a83d-43d2-8860-329a65e398de",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "\n",
    "numpyro.render_model(\n",
    "    model_of_intoxication_given_day, \n",
    "    model_args=(N,), \n",
    "    render_distributions=True, \n",
    "    render_params=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255e5fc-b28a-476f-aea1-26649fff858c",
   "metadata": {},
   "source": [
    "**Sampling from the Model:** We created a helper function for you in order to sample from this model. Note that we haven't fit the model yet to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e3f1a-ee76-4c34-bbf7-8769931565a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20 # Number of samples to generate\n",
    "key = jrandom.PRNGKey(seed=0) # Random generator key to use\n",
    "cs349_sample(model_of_intoxication_given_day, key, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03667c28-a5bf-41a0-8ed1-61cc981bec04",
   "metadata": {},
   "source": [
    "**Performing the MLE:** We will now perform the MLE using another helper function we've created. Recall that the MLE for this model is:\n",
    "\\begin{align}\n",
    "\\pi^\\text{MLE}, \\rho^\\text{MLE} &= \\mathrm{argmax}_{\\pi, \\rho} \\prod\\limits_{n=1}^N p_{I | D}(i_n | d_n; \\rho) \\cdot p_D(d_n; \\pi) \\\\\n",
    "&= \\mathrm{argmax}_{\\pi, \\rho} \\log \\prod\\limits_{n=1}^N p_{I | D}(i_n | d_n; \\rho) \\cdot p_D(d_n; \\pi) \\\\\n",
    "&= \\mathrm{argmax}_{\\pi, \\rho} \\sum\\limits_{n=1}^N \\log p_{I | D}(i_n | d_n; \\rho) + \\log p_D(d_n; \\pi)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cec164-fa8a-463e-89b3-56f69df0f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from the data frame. \n",
    "# Use our helper function, convert_day_of_week_to_int, to convert the days of the week to ints\n",
    "d = jnp.array(convert_day_of_week_to_int(data['Day-of-Week']).values)\n",
    "i = jnp.array((data['Condition'] == 'Intoxication').astype(int).values)\n",
    "\n",
    "N = len(d)\n",
    "NUM_STEPS = 2000\n",
    "result = cs349_mle(\n",
    "    model_of_intoxication_given_day, \n",
    "    jrandom.PRNGKey(0),\n",
    "    NUM_STEPS,\n",
    "    N, \n",
    "    d=d, \n",
    "    i=i,\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8a9f1-4b20-4b69-9697-0388ba3bd455",
   "metadata": {},
   "source": [
    "**Visualizing the Convergence of the MLE:** We can plot the model's log-likelihood with each iteration of the optimizer to see whether (1) the log-likelihood is increasing, as it should, and (2) whether the optimization converged (i.e. do we think there's no more room to improve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2d921-4107-40ba-a9be-51832cdf3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(jnp.arange(NUM_STEPS), result.log_likelihood)\n",
    "plt.xlabel('Optimization Step')\n",
    "plt.ylabel('Log Likelihood')\n",
    "plt.title('Convergence of MLE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae48cf-b3fa-4655-8bf3-9922e962eccb",
   "metadata": {},
   "source": [
    "**Visualizing the Model's Parameters:** The learned model's parameters can be accessed as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5071e3-9ffb-4d2e-ad2d-bd5b3de410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.parameters_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1b93b-33db-4c41-b300-bb33058d1fb8",
   "metadata": {},
   "source": [
    "We can then visualize these learned probabilities to see if they match our observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da984f4-f5cd-4d9f-80f1-eeaf2400335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "marginal_probabilities_from_data = []\n",
    "conditional_probabilities_from_data = []\n",
    "\n",
    "# Iterate over the days of the week\n",
    "for day in days_of_week:\n",
    "    # Select all patients that came in on the specific day of the week\n",
    "    patients_on_day = data[(data['Day-of-Week'] == day)]\n",
    "\n",
    "    # Of the selected patients, further select patients with intoxication\n",
    "    patient_intoxicated_on_day = patients_on_day[patients_on_day['Condition'] == 'Intoxication']\n",
    "\n",
    "    # Compute the portion of patients with intoxication on this day\n",
    "    portion_intoxicated_on_day = float(len(patient_intoxicated_on_day)) / float(len(patients_on_day))\n",
    "\n",
    "    # Compute the portion of patients arriving on this day\n",
    "    portion_arriving_on_day = float(len(patients_on_day)) / float(len(data))\n",
    "    \n",
    "    marginal_probabilities_from_data.append(portion_arriving_on_day)\n",
    "    conditional_probabilities_from_data.append(portion_intoxicated_on_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523adb76-05a8-4ade-a53c-bbe607ff191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot marginal\n",
    "width = 0.3\n",
    "plt.bar(jnp.arange(7) - 0.5 * width, marginal_probabilities_from_data, width, label='Data')\n",
    "plt.bar(jnp.arange(7) + 0.5 * width, result.parameters_mle['pi'], width, label='MLE')\n",
    "\n",
    "# Add axis labels and titles\n",
    "plt.xticks(jnp.arange(7), days_of_week, rotation=30)\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Probability of Day-of-Week')\n",
    "plt.title('Marginal Probability of Being Arriving on Each Day')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d30d9d-024f-4bb4-a31b-c4e3943adb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot conditional\n",
    "width = 0.3\n",
    "plt.bar(jnp.arange(7) - 0.5 * width, conditional_probabilities_from_data, width, label='Data')\n",
    "plt.bar(jnp.arange(7) + 0.5 * width, result.parameters_mle['rho'], width, label='MLE')\n",
    "\n",
    "# Add axis labels and titles\n",
    "plt.xticks(jnp.arange(7), days_of_week, rotation=30)\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Probability of Intoxication')\n",
    "plt.title('Conditional Probability of Intoxication Given Day')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3bbbc-fdf2-4b27-9175-ca7ecbb3d31c",
   "metadata": {},
   "source": [
    "**Sampling from the Fitted Model:** We can sample from the fitted model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633b1ef-dc08-4d0c-8483-3bdc914d945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20 # Number of samples to generate\n",
    "key = jrandom.PRNGKey(seed=0) # Random generator key to use\n",
    "cs349_sample(result.model_mle, key, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7abf329-a566-451f-a4cf-ea304d1d7a30",
   "metadata": {},
   "source": [
    "## Theoretical Properties of the MLE\n",
    "\n",
    "* Consistency\n",
    "* Unbiasedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7db26-bacb-4f9a-b10f-9932e21962f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
