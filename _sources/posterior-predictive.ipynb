{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Inference: Posterior Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some helper functions (please ignore this!)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** For safety-critical applications of ML, it's important that our model captures two notions of uncertainty. Aleatoric uncertainty captures inherent stochasticity in the system. In contrast, epistemic uncertainty is uncertainty over possible *models* that could have fit the data. Multiple models can fit the data when we have a lack of data and/or a lack of mechanistic understanding of the system. We realized that fitting models using the MLE only captured aleatoric uncertainty. To additionally capture epistemic, we therefore had to rethink our modeling paradigm. Using Bayes' rule, we were able to obtain a *distribution* over model parameters given the data, $p(\\theta | \\mathcal{D})$ (the posterior). Using `NumPyro`, we sampled from the posterior to obtain a diversity of models that fit the data. We interpreted a greater diversity of models indicated higher epistemic ucnertainty. \n",
    "\n",
    "**Challenge:** Now that we have a posterior over model parameters, we can capture *epistemic* uncertainty. But how do we use this diverse set of models to (1) make predictions, and (2) compute the log-likelihood (for evaluation)? To do this, we will derive the *posterior predictive*, a distribution that translates a distribution over parameters to a distribution over data. This distributions can then be used to make predictions and evaluate the log-likelihood.\n",
    "\n",
    "**Outline:** \n",
    "* Provide intuition for the posterior predictive\n",
    "* Derive the posterior predictive\n",
    "* Introduce laws of conditional independence\n",
    "* Evaluate the posterior predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition: Model Averaging\n",
    "\n",
    "**Bayesian Modeling as Ensembling.** Recall in the previous chapter, we initially introduced *ensembling* as a way to capture epistemic uncertainty. In ensembling, we train a collection of models independently and hope that, due to quirks in optimization, we end up with a diverse collection of models. In a sense, doesn't our Bayesian approach provide us with an ensemble as well? After all, each set of parameters $\\theta$ from the posterior $p(\\theta | \\mathcal{D})$ represents a different model. Based on this analogy, we can create a \"Bayesian\" ensemble as follows:\n",
    "1. We draw $S$ samples from the posterior. Each sample $\\theta_s$ now represents a member of our ensemble.\n",
    "2. Each posterior sample represents a different model, $p(\\mathcal{D} | \\theta_s)$.\n",
    "    > For regression, we have $p_{Y | X}(y | x, \\theta_s)$.\n",
    "\n",
    "**Predicting.** Using this ensemble, we can predict by *averaging* the predictions of the ensemble members:\n",
    "1. We draw $\\mathcal{D}_s \\sim p(\\cdot | \\theta_s)$ for each $\\theta_s$.\n",
    "    > For regression, we draw $y_s \\sim p_{Y | X}(\\cdot | x, \\theta_s)$.\n",
    "2. We average: $\\frac{1}{S} \\sum\\limits_{s=1}^S \\mathcal{D}_s$.\n",
    "    > For regression, we average $\\frac{1}{S} \\sum\\limits_{s=1}^S y_s$.\n",
    "\n",
    "**Evaluating Log-Likelihood.** Given test data, $\\mathcal{D}^*$, we can use the ensemble to evaluate the model's log-likelihood:\n",
    "1. We evaluate $p(\\mathcal{D}^* | \\theta_s)$ for each $\\theta_s$.\n",
    "    > For regression, we evaluate $p_{Y | X}(y | x, \\theta_s)$ for each $\\theta_s$.\n",
    "2. We average and take the log: $\\log \\frac{1}{S} \\sum\\limits_{s=1}^S p(\\mathcal{D}^* | \\theta_s)$.\n",
    "    > For regression, we average and take the log: $\\log \\frac{1}{S} \\sum\\limits_{s=1}^S p_{Y | X}(y^* | x^*, \\theta_s)$.\n",
    "\n",
    "**Formalizing Intuition.** As we will show next, this intuition is actually correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the Posterior Predictive\n",
    "\n",
    "**Goal.** We want to derive a formula for $p(\\mathcal{D}^* | \\mathcal{D})$, which represents the distribution of new data $\\mathcal{D}^*$ given the observed data, $\\mathcal{D}$. \n",
    "> For a regression model, this distribution is:\n",
    "> \\begin{align}\n",
    "    p_{Y^* | X^*, \\mathcal{D}}(y^* | x^*, \\mathcal{D}) &= p_{Y^* | X^*, \\mathcal{D}}(y^* | x^*, x_1, \\dots, x_N, y_1, \\dots, y_N),\n",
    "\\end{align}\n",
    "> where $x^*$ is a *new* input for which we'd like to make a prediction, $y^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Graphical Model for the Training *and* Test Data.** Notice that our posterior predictive includes a new random variable, $\\mathcal{D}*$. Let's incorporate it into our graphical model so we can better reason about it. \n",
    "\n",
    "TODO depict DGM for abstract model and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivation.** Now we have all we need in order to derive a formula for $p(\\mathcal{D}^* | \\mathcal{D})$. Our first step is to multiply and divide $p(\\mathcal{D}^* | \\mathcal{D})$ by $p(\\mathcal{D})$:\n",
    "\\begin{align}\n",
    "p(\\mathcal{D}^* | \\mathcal{D}) &= \\frac{p(\\mathcal{D}^* | \\mathcal{D}) \\cdot p(\\mathcal{D})}{p(\\mathcal{D})} \n",
    "\\end{align}\n",
    "We do this so that we can write the numerator as the *joint* distribution of $\\mathcal{D}^*$ and $\\mathcal{D}$:\n",
    "\\begin{align}\n",
    "&= \\frac{p(\\mathcal{D}^*, \\mathcal{D})}{p(\\mathcal{D})} \n",
    "\\end{align}\n",
    "Next, we use the law of total probability to re-write the above as a joint distribution over $\\mathcal{D}^*$, $\\mathcal{D}$, and $\\theta$. We do this to introduce $\\theta$ into the equation---since our model's prior, likelihood, and posterior all depend on $\\theta$, it would be weird if the formula for $p(\\mathcal{D}^* | \\mathcal{D})$ didn't depend on it. This gives us:\n",
    "\\begin{align}\n",
    "&= \\frac{\\int p(\\mathcal{D}^*, \\mathcal{D}, \\theta) \\cdot d\\theta}{p(\\mathcal{D})} \n",
    "\\end{align}\n",
    "Now, we can factorize this joint distribution to get one term that's the posterior, $p(\\theta | \\mathcal{D})$, and one term that's the marginal, $p(\\mathcal{D})$:\n",
    "\\begin{align}\n",
    "&= \\frac{\\int p(\\mathcal{D}^* | \\theta, \\mathcal{D}) \\cdot p(\\theta | \\mathcal{D}) \\cdot p(\\mathcal{D}) \\cdot d\\theta}{p(\\mathcal{D})} \n",
    "\\end{align}\n",
    "Since $p(\\mathcal{D})$ doesn't depend on $\\theta$, we can take it out of the integral, thereby canceling it with the $p(\\mathcal{D})$ in the denominator:\n",
    "\\begin{align}\n",
    "&= \\int p(\\mathcal{D}^* | \\theta, \\mathcal{D}) \\cdot p(\\theta | \\mathcal{D}) \\cdot d\\theta\n",
    "\\end{align}\n",
    "Finally, using the laws of conditional independence, we know that $p(\\mathcal{D}^* | \\theta, \\mathcal{D}) = p(\\mathcal{D}^* | \\theta)$. This is because, by conditioning on $\\theta$, we remove all paths connecting $\\mathcal{D}$ to $\\mathcal{D}^*$. In other words, $\\theta$ summarizes all information from $\\mathcal{D}$ needed to predict $\\mathcal{D}^*$. This gives us the following equation:\n",
    "\\begin{align}\n",
    "&= \\int \\underbrace{p(\\mathcal{D}^* | \\theta)}_{\\text{likelihood of new data}} \\cdot \\underbrace{p(\\theta | \\mathcal{D})}_{\\text{posterior}} \\cdot d\\theta\n",
    "\\end{align}\n",
    "As you can see, $p(\\mathcal{D}^* | \\mathcal{D})$ is a function of the posterior and the joint data likelihood of the new data. Adding some syntactic sugar, we can write the above equation as:\n",
    "\\begin{align}\n",
    "&= \\mathbb{E}_{p(\\theta | \\mathcal{D})} \\left[ p(\\mathcal{D}^* | \\theta) \\right]\n",
    "\\end{align}\n",
    "This shows that to evaluate $p(\\mathcal{D}^* | \\mathcal{D})$, we need to:\n",
    "1. Draw posterior samples $\\theta \\sim p(\\theta | \\mathcal{D})$.\n",
    "2. Average the likelihood $p(\\mathcal{D}^* | \\theta)$ across these samples.\n",
    "\n",
    "As you can see this matches our intuition exactly! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laws of Conditional Independence\n",
    "\n",
    "Recall that the slope and intercept of a model may have been sampled independently under the prior, but as soon as we condition on data, they are no longer independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive of Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Exercise: Derive the Posterior Predictive Distribution\n",
    "\n",
    "For each of the models below, draw the directed graphical model, and then derive the posterior predictive formula.\n",
    "\n",
    "**Part 1:** Bayesian predictive model. \n",
    "\\begin{align}\n",
    "\\theta &\\sim p_\\theta(\\cdot) \\\\\n",
    "y_n | x_n, \\theta &\\sim p_{Y | X}(\\cdot | x_n, \\theta)\n",
    "\\end{align}\n",
    "\n",
    "**Part 2:** Bayesian Concept-Bottlebeck model (CBM). CBMs aim to make it easier to interpret model predictions. They do this by combining two models:\n",
    "* CMBs first learning to predict \"concepts\" $c_n$ associated, associated with input $x_n$. In a CBM, a concept is just a discrete attribute associated with the input; for example, if $x_n$ is an image of wildlife, a concept could be rain, grass, dog, etc. You can think of $p_{C | X}$ as a classifier. \n",
    "* After having predicted the concept $c_n$ from the input $x_n$, CBMs attempt to predict the final output $y_n$ from the concept only. In this way, predictions of $y_n$ can be analyzed in terms of the concepts, which as easier to understand, instead of with respect to the inputs, which could be high dimensional and difficult to reason about.\n",
    "\n",
    "A Bayesian CBM has the following generative process:\n",
    "\\begin{align}\n",
    "\\theta &\\sim p_\\theta(\\cdot) \\\\\n",
    "\\phi &\\sim p_\\phi(\\cdot) \\\\\n",
    "c_n | x_n, \\theta &\\sim p_{C | X}(\\cdot | x_n, \\theta) = \\mathrm{Cat}(\\pi(x_n; \\theta)) \\\\\n",
    "y_n | c_n, \\phi &\\sim p_{Y | C}(\\cdot | c_n, \\phi),\n",
    "\\end{align}\n",
    "where $\\pi(\\cdot; \\theta)$ is a function that maps $x_n$ to the parameters of a categorical distribution. \n",
    "\n",
    "**Part 3:** Bayesian Factor Analysis.\n",
    "\\begin{align}\n",
    "\\theta &\\sim p_\\theta(\\cdot) \\\\\n",
    "z_n &\\sim p_Z(\\cdot) \\\\\n",
    "x_n | z_n, \\theta &\\sim p_{X | Z, \\theta}(\\cdot | z_n, \\theta) \n",
    "\\end{align}\n",
    "\n",
    "**Part 4:** Bayesian predictive model with input noise.\n",
    "\\begin{align}\n",
    "\\theta &\\sim p_\\theta(\\cdot) \\\\\n",
    "z_n &\\sim p_Z(\\cdot) \\\\\n",
    "y_n | x_n, z_n, \\theta &\\sim p_{Y | X, Z, \\theta}(\\cdot | x_n, z_n, \\theta) \n",
    "\\end{align}\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation using the Posterior Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
