{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78da4845-6ced-4d35-a00c-d7e163841f23",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746438c9-b754-47cf-ada3-1283dec45b2f",
   "metadata": {},
   "source": [
    "## Helpful Identities\n",
    "\n",
    "* $I$ is the identity matrix\n",
    "* $\\log(A \\cdot B) = \\log A + \\log B$\n",
    "* A **neural network** $f(\\cdot; \\theta)$ is a function (defined by a composition of simple functions), parameterized by weights $\\theta$.\n",
    "* **Marginalization:** given $p(x, z)$, the marginal is $p(x) = \\mathbb{E}_{p(z)} [ p(x | z) ]$\n",
    "* **KL-divergence** a measure of \"distance\" between distributions: $\\text{KL}[q(z) || p(z)] = \\mathbb{E}_{q(z)} \\left[ \\log \\frac{q(z)}{p(z)} \\right] \\geq 0$\n",
    "* **Jensen's Inequality:** $\\log \\mathbb{E}_{p(z)} [f(z)] \\geq \\mathbb{E}_{p(z)} [\\log f(z)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc4a35-4a96-4598-bd11-3fd1570ae2e4",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "**Motivation:**\n",
    "* Want to be able to generate realistic-looking data (e.g. airport security)\n",
    "* Want to be able to evaluate the probability of a data-point\n",
    "\n",
    "**Question:** How can we design model + inference to accomplish this task?\n",
    "\n",
    "**Today:** We are going to combine several topics from the course with some tools from deep learning to define a model + inference capable of expressing very complicated distributions, like a distribution of images. \n",
    "\n",
    "The topics we rely on today are:\n",
    "1. Latent variable models\n",
    "2. Maximum Likelihood (MLE)\n",
    "3. Expectation-Maximization (EM)\n",
    "4. Bayesian Inference (i.e. inferring posterior)\n",
    "5. Gradient descent\n",
    "6. Sampling \n",
    "7. Monte Carlo (MC) estimation of expectations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da867ddd-547a-4c30-bf82-0ed279748b24",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "1. Introduce the model\n",
    "2. Demo\n",
    "3. Inference: why EM doesn't work\n",
    "4. Variational Inference\n",
    "5. Ticks for scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7e130-c988-4870-b2c1-5155105383b0",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "**Model:** Variational Autoencoder / Non-linear Factor Analysis\n",
    "\\begin{align}\n",
    "z &\\sim p(z) = \\mathcal{N}(0, I) \\\\\n",
    "x | z &\\sim p(x | z; \\theta) = \\mathcal{N}(f(z; \\theta), \\sigma^2 \\cdot I)\n",
    "\\end{align} in which $x_1, \\dots, x_N \\in \\mathbb{R}^D$ is observed, $z_1, \\dots, z_N \\in \\mathbb{R}^L$ is latent, $f(\\cdot; \\theta)$ is some function (e.g. neural network). \n",
    "\n",
    "When $f(z; \\theta)$ is a linear function, this model is known as **factor analysis**. \n",
    "\n",
    "**Picture for intuition:** TODO.\n",
    "\n",
    "**Inference Goal:** maximize the likelihood of the data under the model (MLE)\n",
    "\\begin{align}\n",
    "\\max\\limits_\\theta \\frac{1}{N} \\sum\\limits_{n=1}^N \\log p(x_n; \\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab98dd4-56d4-4a29-aa86-4299f54b676f",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "**Links:** \n",
    "* [Demo](http://taylordenouden.com/VAE-Latent-Space-Explorer/)\n",
    "* [Cool visualization](https://arxiv.org/pdf/1704.03477.pdf)\n",
    "\n",
    "**What do we see in the demo?**\n",
    "* The demo consists of a model trained to generate hand-drawn digits.\n",
    "* It visualizes the latent space of the model -- that is, the projection of each image $x$ onto the 2D $z$-space.\n",
    "\n",
    "**Questions:**\n",
    "1. What does each color in the latent space represent?\n",
    "2. What happens when you move from a region with one color to another?\n",
    "3. What do you notice about how $x$ transforms as you move your cursor (and how would it differ from a naive interpolation between images)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930788d-1018-4b52-839c-e7e92af6d82b",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "**Can we use the naive MLE?** MLE for this model is intractable:\n",
    "\\begin{align}\n",
    "\\max\\limits_\\theta \\frac{1}{N} \\sum\\limits_{n=1}^N \\log p(x_n; \\theta)\n",
    "&= \\max\\limits_\\theta \\frac{1}{N} \\sum\\limits_{n=1}^N \\log \\underbrace{\\mathbb{E}_{p(z_n)} [ p(x_n | z_n; \\theta) ]}_{\\text{intractable}}\n",
    "\\end{align}\n",
    "\n",
    "Why is it intractable? (draw picture).\n",
    "* Case 1: high variance MC-estimate\n",
    "* Case 2: low variance computationally expensive MC-estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ebf59-0550-4445-ab83-17c7edf8a8ac",
   "metadata": {},
   "source": [
    "### Can we apply EM?\n",
    "\n",
    "Let's derive it:\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(x_n; \\theta) \n",
    "&= \\log \\mathbb{E}_{p(z_n)} [ p(x_n | z_n; \\theta) ] \\\\\n",
    "&= \\log \\int\\limits_{z_n} p(x_n | z_n; \\theta) p(z_n) d z_n \\\\\n",
    "&= \\log \\int\\limits_{z_n} q(z_n) \\cdot \\frac{p(x_n | z_n; \\theta) p(z_n)}{q(z_n)} d z_n \\\\\n",
    "&= \\log \\mathbb{E}_{q(z_n)} \\left[ \\frac{p(x_n | z_n; \\theta) p(z_n)}{q(z_n)} \\right] \\quad \\text{Importance Sampling: how to select } q(z_n)? \\\\\n",
    "&\\geq \\mathbb{E}_{q(z_n)} \\left[ \\log \\frac{p(x_n | z_n; \\theta) p(z_n)}{q(z_n)} \\right] \\quad \\text{Soln: optimize lower bound jointly w.r.t } \\theta, q(z_n)\n",
    "\\end{align} Plugging this back into our MLE goal, we get:\n",
    "\\begin{align}\n",
    "\\underbrace{\\frac{1}{N} \\sum\\limits_{n=1}^N \\log p(x_n; \\theta)}_{\\text{log marginal/incomplete likelihood}} \\geq \\underbrace{\\frac{1}{N} \\sum\\limits_{n=1}^N \\mathbb{E}_{q(z_n)} \\left[ \\log \\frac{p(x_n | z_n; \\theta) p(z_n)}{q(z_n)} \\right]}_{\\text{ELBO}(\\theta, q(z_1), \\dots, q(z_N))}\n",
    "\\end{align}\n",
    "\n",
    "In EM, this is the objective we optimize via coordinate ascent. Before applying coordinate ascent, let's understand the **tightness** of this bound.\n",
    "\\begin{align}\n",
    "\\frac{1}{N} \\sum\\limits_{n=1}^N \\log p(x_n; \\theta) \n",
    "&= \\underbrace{\\frac{1}{N} \\sum\\limits_{n=1}^N  \\mathbb{E}_{q(z_n)} \\left[ \\log \\frac{p(x_n | z_n; \\theta) p(z_n)}{q(z_n)} \\right]}_{\\text{ELBO}(\\theta, q(z_1), \\dots, q(z_N))} + \\underbrace{\\frac{1}{N} \\sum\\limits_{n=1}^N \\text{KL} \\left[ q(z_n) || p(z_n | x_n; \\theta) \\right]}_{\\geq 0 \\quad \\text{\"gap\"}}\n",
    "\\end{align} The above equation shows that maximizing the ELBO is **equivalent** to maximizing the likelihood when the \"gap\" is 0 -- i.e. when we can set $q(z_n)$ equal to the posterior exactly.\n",
    "\n",
    "We apply **coordinate ascent** as follows:\n",
    "* **E-step:**\n",
    "\\begin{align}\n",
    "q^\\text{new}(z_1) &= \\mathrm{argmax}_{q(z_1)} \\quad \\text{ELBO}(\\theta^\\text{old}, q(z_1), \\dots, q^\\text{old}(z_N)) \\\\\n",
    "&\\vdots \\\\\n",
    "q^\\text{new}(z_N) &= \\mathrm{argmax}_{q(z_N)} \\quad \\text{ELBO}(\\theta^\\text{old}, q^\\text{old}(z_1), \\dots, q(z_N))\n",
    "\\end{align}\n",
    "* **M-step:**\n",
    "\\begin{align}\n",
    "\\theta^\\text{new} = \\mathrm{argmax}_{\\theta} \\quad &\\text{ELBO}(\\theta, q^\\text{old}(z_1), \\dots, q^\\text{old}(z_N)) \n",
    "\\end{align}\n",
    "* **Repeat!**\n",
    "\n",
    "What's the solution to the E-step? $q^\\text{new}(z_n) = p(z_n | x_n; \\theta^\\text{old})$. But this requires a **closed-form** solution, which we do not have in general. How can we get around this?\n",
    "* Sampling (e.g. MH)? (take a long time, scales poorly with $N$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045d893-ebf2-4973-aba7-7f793ccf99d2",
   "metadata": {},
   "source": [
    "### Variational Inference\n",
    "\n",
    "Since **coordinate ascent** relies on conjugacy, what other optimization techniques can we use to maximize the ELBO? Can we use gradient descent?\n",
    "\n",
    "We can! But we'll need to make a few changes to our inference (in 3 parts).\n",
    "\n",
    "**Part 1: Select form for $q(z_n)$.** Previously, we set $q^\\text{new}(z_n) = p(z_n | x_n; \\theta^\\text{old})$ using an analytic formula. However, when such a formula does not exist, what can we do? Let's approximate the posterior.\n",
    "\n",
    "That is, we'll set $q(z_n)$ to come from some family of distributions that's easy to work with (i.e. that we can sample from and for which we can evaluate the PDF). This distribution will not capture the posterior perfectly, but will hopefully capture it well enough. We'll call this family of distributions a **variational family**. One common choice is a diagonal Gaussian:\n",
    "\\begin{align}\n",
    "q(z_n) = \\mathcal{N}(\\mu_n, \\sigma^2_n)\n",
    "\\end{align}\n",
    "\n",
    "Now, when we maximize the ELBO, we do so by taking gradients with respect to the parameters of $q(z_n)$, as opposed to relative to $q(z_n)$:\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta, \\mu_1, \\sigma_1, \\dots, \\mu_N, \\sigma_N} \\text{ELBO}(\\theta, \\mu_1, \\sigma_1, \\dots, \\mu_N, \\sigma_N) \n",
    "\\end{align}\n",
    "\n",
    "**Part 2: Reparameterization Trick.**\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta, \\mu_1, \\sigma_1, \\dots, \\mu_N, \\sigma_N} \\text{ELBO}(\\theta, \\mu_1, \\sigma_1, \\dots, \\mu_N, \\sigma_N)\n",
    "&= \\nabla_{\\theta, \\mu_1, \\sigma_1, \\dots, \\mu_N, \\sigma_N} \\frac{1}{N} \\sum\\limits_{n=1}^N \\mathbb{E}_{q(z_n)} \\left[ \\log \\frac{p(x_n | z_n; \\theta) p(z_n)}{q(z_n)} \\right] \\\\\n",
    "&\\approx \\nabla_{\\theta, \\mu_1, \\sigma_1, \\dots, \\mu_N, \\sigma_N} \\frac{1}{N} \\sum\\limits_{n=1}^N \\frac{1}{S} \\sum\\limits_{s=1}^S \\left[ \\log \\frac{p(x_n | z_n^{(s)}; \\theta) p(z_n^{(s)})}{q(z_n^{(s)})} \\right]\n",
    "\\end{align} where $z_n^{(s)} \\sim q(z_n) = \\mathcal{N}(\\mu_n, \\sigma^2_n)$. \n",
    "\n",
    "But how can we take a gradient with respect to a **sample**, $z_n^{(s)}$? We can use the following identity (know as the **reparameterization trick**):\n",
    "\\begin{align}\n",
    "z_n^{(s)} &= r(\\eta_n^{(s)}, \\sigma_n, \\mu_n) \\\\\n",
    "&= \\eta_n^{(s)} \\cdot \\sigma_n + \\mu_n, \\quad \\eta_n^{(s)} \\sim \\mathcal{N}(0, I)\n",
    "\\end{align} Plugging this in, we get:\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta, \\mu_1, \\sigma_1, \\dots, \\mu_N, \\sigma_N} \\text{ELBO}(\\theta, \\mu_1, \\sigma_1, \\dots, \\mu_N, \\sigma_N)\n",
    "&= \\nabla_{\\theta, \\mu_1, \\sigma_1, \\dots, \\mu_N, \\sigma_N} \\frac{1}{N} \\sum\\limits_{n=1}^N \\mathbb{E}_{p(\\eta)} \\left[ \\log \\frac{p(x_n | r(\\eta_n^{(s)}, \\sigma_n, \\mu_n); \\theta) p(r(\\eta_n^{(s)}, \\sigma_n, \\mu_n))}{q(r(\\eta_n^{(s)}, \\sigma_n, \\mu_n))} \\right] \\\\\n",
    "\\end{align} in which we do not need to take derivative with respect to the samples.\n",
    "\n",
    "**Part 3: Amortize Inference.** The above gradients are over a very large number of parameters (specifically $N$), which does not scale well. Can we make this more efficient?\n",
    "\n",
    "Yes! By **amortizing**: we set $q(z_n) = \\mathcal{N}(\\mu(x_n; \\phi), \\sigma^2(x_n; \\phi))$, where $\\mu(\\cdot; \\phi), \\sigma^2(\\cdot; \\phi)$ are neural networks parameterized by $\\phi$. Now, we only need to compute gradients with respect to $\\theta$ and $\\phi$.\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta, \\phi} \\text{ELBO}(\\theta, \\phi)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8f2d5-1b39-4ee3-b6aa-706c5f99f04f",
   "metadata": {},
   "source": [
    "## Putting it all together \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/982/1*6MxYa1CLA4Kuy0i65Wc4sw.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305a6d4-e202-44b7-af51-6231f938cf5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
