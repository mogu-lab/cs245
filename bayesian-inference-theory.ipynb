{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Inference: Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some helper functions (please ignore this!)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** \n",
    "\n",
    "**Challenge:**\n",
    "\n",
    "**Outline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why We Need Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The MLE is Over-Confident.** In safety-critical contexts, like those from the IHH, it's important that our ML models don't just fit the observed data well; they should also communicate with us the limits of their \"knowledge.\" Let's illustrate what we mean. Consider the regression data below:\n",
    "\n",
    "TODO: figure with in-between and OOD uncertainty. \n",
    "\n",
    "As you see in the figure above, we don't have data for some segments of the population of interest. \n",
    "\n",
    "TODO: explain why this is bad in the context of the task. \n",
    "\n",
    "TODO: give example with cats and COVID\n",
    "\n",
    "Ideally, our learning algorithm would give us options; it would give us several models that all fit the data reasonably well, but behave differently away from the data. Given these options, perhaps we could devise some algorithm to select the one we would like to use on our downstream task, or find some way to *combine* them. Unfortunately, the learning algorithm we've use so far---the MLE---doesn't provide us with a way to do this. The MLE gives a *single* model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembling.** One way to solve this issue is by relying on the imperfections of the optimizer. Remember that, especially for more expressive models, optimization tends to get stuck in local optima. What if we were to collect an *ensemble* of models, all fit with the MLE to data, but each optimized from a different random initialization? Because each model would get stuck in a different local optima, each *might* behave differently than the others away from the data. What's nice about this approach is that it's easy to implement: we already have all the tools we need! Let's see what ensembling a neural network regression model looks like:\n",
    "\n",
    "TODO figure of NN ensemble on above data\n",
    "\n",
    "While effective in practice, ensembling also has one big problem when it comes to safety-critical contexts. It makes implicit assumptions that are difficult to understand. Specifically, we relied on the imperfections of our black-box optimizer to find us a diverse set of models. What kind of models will the optimizer give us, however? Do these models have an *inductive bias* that are appropriate for our task? \n",
    "\n",
    "The need for explicit assumptions motivates us to find an alternative way of fitting our models to day, leading us to the *Bayesian approach*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Models via Bayes' Rule\n",
    "\n",
    "**The Bayesian Paradigm.** So let's go back to the drawing board and rethink how we've been fitting models this whole time. So far, our approach has been finding the *single* model that maximizes the probability of our observed data: $\\theta^\\text{MLE} = \\log p(\\mathcal{D}; \\theta)$. But isn't what we're actually interested is the *distribution* of models given the data, $p(\\theta | \\mathcal{D})$? In other words, conditioned on the data we've observed so far, we want to know which models (represented by their parameters, $\\theta$) are likely to fit the data well. In this new paradigm, we hope that:\n",
    "1. $p(\\theta | \\mathcal{D})$ will capture a diversity of models with different inductive biases.\n",
    "2. We can make our assumptions clear, and we can specify what type of inductive biases are appropriate for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes' Rule.** But what is $p(\\theta | \\mathcal{D})$, exactly? How can we possibly write down a distribution of models that fit the data well by hand? Isn't the whole point that the machine will do the learning for us? To get around this problem, we will use *Baye's rule* to write down $p(\\theta | \\mathcal{D})$ in terms of what we already know how to specify: $p(\\mathcal{D} | \\theta)$.\n",
    "\n",
    "Recall from the chapter on joint probability that a joint distribution over two random variables, $A$ and $B$, can be factorized as follows:\n",
    "\\begin{align}\n",
    "p_{A, B}(a, b) &= p_{B | A}(b | a) \\cdot p_A(a) \\quad (\\text{Option 1}) \\\\\n",
    "&= p_{A | B}(a | b) \\cdot p_B(b) \\quad (\\text{Option 2})\n",
    "\\end{align}\n",
    "This means we can also equate the two factorizations:\n",
    "\\begin{align}\n",
    "p_{B | A}(b | a) \\cdot p_A(a) &= p_{A | B}(a | b) \\cdot p_B(b)\n",
    "\\end{align}\n",
    "Diving both sides by $p_A(a)$, we get:\n",
    "\\begin{align}\n",
    "p_{B | A}(b | a) &= \\frac{p_{A | B}(a | b) \\cdot p_B(b)}{p_A(a)} \\quad \\text{(Bayes' Rule)}\n",
    "\\end{align}\n",
    "This is Bayes' rule. What's cool about it is that relates $p_{B | A}(b | a)$ to $p_{A | B}(a | b)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian Inference.** Using Bayes' rule in the context of our problem, let's treat $\\mathcal{D}$ *and* $\\theta$ as random variables. We can now relate $p(\\theta | \\mathcal{D})$, which we don't know how to specify, to $p(\\mathcal{D} | \\theta)$, which we do know how to specify:\n",
    "\\begin{align}\n",
    "\\underbrace{p(\\theta | \\mathcal{D})}_{\\text{posterior}} &= \\frac{\\overbrace{p(\\mathcal{D} | \\theta)}^{\\text{likelihood}} \\cdot \\overbrace{p(\\theta)}^{\\text{prior}}}{\\underbrace{Z}_{\\text{normalizing const.}}}\n",
    "\\end{align}\n",
    "When used as a model-fitting paradigm, each term in Bayes' rule has a special name. We'll now define each:\n",
    "* **Likelihood:** This is the data joint likelihood, which we've previously maximized as part of the MLE.\n",
    "* **Prior:** This is the distribution of models we're willing to consider *before having observed any data*. As we will show visually in a bit, this distribution determines our *inductive bias*.\n",
    "* **Posterior:** This is the distribution of interest. It's called a posterior because it determines the distribution of likely models, $\\theta$, *after having observed data*.\n",
    "* **Normalizing Constant**: This is a constant that turns the whole fraction into a valid probability density function (i.e. a function that integrates to 1). To compute $Z$, we integrate the numerator of Bayes' rule over the support of $\\theta$:\n",
    "    \\begin{align}\n",
    "    Z &= \\int\\limits \\underbrace{p(\\mathcal{D} | \\theta) \\cdot p(\\theta)}_{\\text{numerator of Bayes' rule}} d\\theta\n",
    "    \\end{align}\n",
    "    In this way, when we divide by it, the whole fraction integrates to $1$.\n",
    "\n",
    "Now that we have a formula for the posterior, $p(\\theta | \\mathcal{D})$, what do we do with it? Since it's a distribution, we can *sample* from it. That is, we can draw $\\theta \\sim p(\\theta | \\mathcal{D})$ and visualize the models corresponding to each draw to plot our modeling uncertainty. The process of sampling from $p(\\theta | \\mathcal{D})$ is called *Bayesian inference.* In a bit, we'll also talk about how to quantitatively evaluate the fit of Bayesian models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choice of Prior.** In comparison to our first model-fitting paradigm, the MLE, the Bayesian approach requires us to specify one more thing: the prior, $p(\\theta)$. How do we pick a good prior? Generally, we have to think about the model's inductive bias. What types of models do we consider *realistic* for our setting? We will next illustrate the whole Bayesian paradigm visually to gain a better sense of what the prior means, and how it interacts with the likelihood to get the posterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Modeling: Intuition\n",
    "\n",
    "**Example: Bayesian Regression**. We'll now instantiate the Bayesian modeling paradigm for regression in 1-dimension to gain some intuition. \n",
    "\n",
    "TODO picture of prior\n",
    "\n",
    "TODO picture of posterior after 1 data, after 2 data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relating Modeling Uncertainty to Everyday Life.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the Predictive Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview.** TODO\n",
    "* Our goal isn't just to learn the parameters; our goal is to determine, given observed data points, what's the probability of new data? State goal mathematically.\n",
    "* Bayesian inference (sampling from the posterior) will actually help us accomplish that.\n",
    "* To derive the distribution of new data given old data, we'll have to introduce a few more facts about directed graphical models.\n",
    "* We'll instantiate everything here for regression, but the general principles apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representing Unobserved Variables in DGMs.**\n",
    "* TODO show for regression, circle instead of dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representing the Joint Distribution of Training and Test Data in a DGM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laws of Conditional Independence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Exercise: Deriving the Posterior Predictive Distribution\n",
    "TODO\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
