# The Ethics of Uncertainty and Interpretability in Human/AI Systems

**Context:** 


## What do we need for effective Human/AI collaborations?


````{admonition} Exercise: Effective Human/AI Collaboration
**Part 1:** Read, *[Clinical AI tools must convey predictive uncertainty for each individual patient](https://www.nature.com/articles/s41591-023-02562-7)*. 
* Why does the author advocate for uncertainty? 
* How does the author envision using uncertainty (epistemic, aleatoric, and conformal) in clinical settings?
* Do you anticipate any challenges incorporating uncertainty as they envision? If so, what are they? If not, why not?

**Part 2:** Read, *[How Should Clinicians Communicate With Patients About the Roles of Artificially Intelligent Team Members?](https://journalofethics.ama-assn.org/article/how-should-clinicians-communicate-patients-about-roles-artificially-intelligent-team-members/2019-02)*.
* What are the challenges highlighted by the authors in using AI in their context? 
* Are there any additional challenges you foresee?
````


## Challenges with current Human/AI collaboration practices


````{admonition} Exercise: Challenges in Human/AI Collaboration
**Part 1:** Read, *[Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment](https://arxiv.org/pdf/1901.07694)*. 
* Why do the authors argue interpretability is important for ensuring fairness?
* What are the different notions of fairness discussed in the paper?
* What did the authors find?
* If the type of explanation influences a user's perception of fairness, are there aspects of the user's decision-making process that might be influenced? 
* Given that explanations have such influence over the users, how can we ensure our explanations are *informative* and not *manipulative*? 

Note: This paper focuses on recidivism prediction. There are many criticisms of the use for AI in this domain that are important to consider. If you're interested in learning more, read the following two papers:
* [Layers of Bias: A Unified Approach for Understanding Problems With Risk Assessment](https://journals.sagepub.com/doi/abs/10.1177/0093854818811379)
* [Beyond Bias: Re-Imagining the Terms of ‘Ethical AI’ in Criminal Law](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3377921)

**Part 2:** From *[Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757236)*, read "Introduction," "Robots on the road," and "Conclusion."
* What were the sources of failure in the self-driving Uber car accident? 
* In the accident, who ended up being held responsible? Do you agree with this? Why or why not? 
* Do you think that if the self-driving AI had some notion of uncertainty, what this have helped avoid the accident? If so, in what ways? If not, why not?
* What's a "moral crumple zone"? Instantiate this using the Google incidents from 2015-2017. 
* What are the author's criticisms of human-in-the-loop paradigm?

**Part 3:** In your opinion, are explainablility, uncertainty, and more broadly, human-in-the-loop AI-assisted decision making just shifting the blame from the AI developers to the AI users? Why or why not?

````


