{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some helper functions (please ignore this!)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** At this point, we have a general framework for developing probabilistic models, as well as one way of fitting them to data, MLE. We've then instantiated this framework to develop two types of predictive models---regression and classification. We further learned how to build in expressivity using tools from deep learning---namely, neural networks---into these models. Are we finally ready to apply these models to real-life tasks? \n",
    "\n",
    "Unfortunately, there's one key piece we're still missing: so far, we've only used 1- and 2-dimensional input data and 1-dimensional output data. While in principle, we already have the tools to implement predictive models for higher dimensional data, we don't yet have the tools to *evaluate* them. We've purposefully worked with lower dimensional data because it is easy to visualize, and therefore easy to qualitatively evaluate. But as data becomes higher dimensional, it's much more difficult to get intuition using visualizations. As a result, we will have to rely on *metrics*. \n",
    "\n",
    "**Challenge:** There are many ways of measuring model performance. Which metrics should we use? What are the pros and cons of each metric? We will answer these questions here.\n",
    "\n",
    "**Outline:**\n",
    "* What's underfitting/overfitting? How do we prevent it?\n",
    "* Introduce log-likelihood\n",
    "* Introduce metrics specific to regression\n",
    "* Introduce metrics specific to classification\n",
    "* Discuss broader challenges with model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data:**\n",
    "* Introduce motivating high-dimensional regression and classification data and tasks.\n",
    "* Provide them students with several choices of fitted models for each task to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing Over- and Under-fitting\n",
    "\n",
    "TODO:\n",
    "* Explain both concepts, show visuals for regression and classification\n",
    "* Introduce the idea of validation and test set\n",
    "* Raise question: but which metric to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Likelihood\n",
    "\n",
    "TODO:\n",
    "* Pros: can be used on all probabilistic models\n",
    "* Pros: has intuitive properties (exercise with regression with biased mean, and with biased variance)\n",
    "* Cons: units are difficult to interpret\n",
    "* Note: which points you include in your test set matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Regression\n",
    "\n",
    "TODO:\n",
    "* Introduce MSE, R^2\n",
    "* Pros: easy to interpret\n",
    "* Cons: hard to know if scale of MSE is reasonable\n",
    "* Exercise: ask students to compute MSE and log-likelihood and decide which model they prefer\n",
    "* Exercise: add additional sensitive feature to data---ask them to break down metrics on sensitive variable. Which model do they pick now? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Classification\n",
    "\n",
    "TODO:\n",
    "* Introduce accuracy. Con: doesn't communicate false positives of negatives (use class imbalance example)\n",
    "* Introduce confusion matrix. Cons: doesn't communicate predictive uncertainty\n",
    "* Exercise: ask students to compute confusion matrices and decide which model they prefer\n",
    "* Exercise: add an additional sensitive feature to data---ask them to break down metrics on sensitive variable. Which model do they pick now? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with Evaluation\n",
    "\n",
    "TODO:\n",
    "* Model performance is multi-dimensional, but we can only use one \"scoring system\" to select\n",
    "* This is becomes even more tricky when fairness is involved\n",
    "* How to be ethical? Participatory design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
