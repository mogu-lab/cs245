{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Inference: Prior and Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some helper functions (please ignore this!)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** If there's one thing we learned from the chapter on model selection and evaluation is that we should not blindly trust our models. Models are complicated and require a robust and diverse toolkit for responsible evaluation in their intended context. For safety-critical applications of ML, like the ones from the IHH, we must take additional precautions to ensure responsible use. We therefore adopt the following philosophy:\n",
    "1. **Finite information $\\rightarrow$ uncertainty.** We're often asked to make decisions without all the information necessary for certainty. We ask the same of our models: given a finite data set and an incomplete understanding of the phenomenon we're modeling, we ask models to make predictions for data they have never encountered. Therefore, for responsible use in safety-critical contexts, our models must have some way of quantifying the limits of their \"knowledge.\"\n",
    "2. **Not making choices $\\rightarrow$ a choice will be made for you.** If we avoid making explicit choices in the design of our model, a choice will still be made for us---and it might not be the choice we want. For example, without explicitly choosing what's important to us, we might get a model with the highest accuracy for a task for which minimizing false negatives is most important. *It's therefore better to make your choices explicitly.* Making assumptions explicit is especially important for uncertainty quantification. \n",
    "\n",
    "**Challenge:** To satisfy our new modeling philosophy, we need (1) a way to quantify uncertainty, and (2) a way to understand how uncertainty depends on our modeling choices. How can we do that with the tools we have? As we show here, we can't. We will then introduce a new way of fitting ML models called Bayesian inference.\n",
    "\n",
    "**Outline:** \n",
    "* Motivate the need for uncertainty\n",
    "* Introduce a new modeling paradigm based on Bayes' rule\n",
    "* Provide intuition for this modeling paradigm\n",
    "* Implement this modeling paradigm in `NumPyro` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data:** To help make the concepts concrete, we'll return to our regression data, in which we wanted to predict telekinetic ability from age. Let's load the data in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a bunch of libraries we'll be using below\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpyro\n",
    "import numpyro.distributions as D\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "csv_fname = 'data/IHH-CTR-CGLF-regression-augmented.csv'\n",
    "data = pd.read_csv(csv_fname, index_col='Patient ID')\n",
    "\n",
    "# Print a random sample of patients, just to see what's in the data\n",
    "data.sample(15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why We Need Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The MLE is Over-Confident.** In safety-critical contexts, like those from the IHH, it's important that our ML models don't just fit the observed data well; they should also communicate with us the limits of their \"knowledge.\" Let's illustrate what we mean. Consider the regression data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.5, 3.5))\n",
    "\n",
    "plt.scatter(data['Age'], data['Telekinetic-Ability'], color='black', alpha=0.5, marker='x', label='Data')\n",
    "plt.axvspan(78, 93, alpha=0.5, color='red', label='No Data')\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Telekinetic Ability')\n",
    "plt.title('What should the model do where there\\'s no data?')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to make a prediction for a patient of age 85, what should the model do? The model wasn't trained on any such patients. Does the trend keep going down with age? Doing so would ignore the point at the very right of the plot, treating it as an *outlier*. Or maybe the trend should go up after age 80? It's impossible for us to know because we haven't observed data about such patients. In cases such as these, it's important that our model alert us about its uncertainty. \n",
    "\n",
    "Especially in recent years, there has been more and more research dedicated to developing models that can reliably quantify uncertainty. As an example, a recent paper evaluated how confident different deep learning models are on a medical imaging task. In the paper, the authors evaluated models for predicting whether patients had COVID or not from X-ray scans. Here's what they found:\n",
    "\n",
    "\n",
    "```{figure} _static/figs/cats-vs-covid.png\n",
    "---\n",
    "width: 100%\n",
    "name: cats-vs-covid\n",
    "align: center\n",
    "---\n",
    "\n",
    "ML models can be over-confident about wrong predictions. Figure adapted from [this paper](https://www.semanticscholar.org/paper/Can-Your-AI-Differentiate-Cats-from-Covid-19-Sample-Mallick-Dwivedi/c100c33afbb2efa5197f7d6042022e1227c5e298).\n",
    "```\n",
    "As you can see from the figure, the model makes predictions that aren't only incorrect, *but also overconfident*. And it does this for inputs (like the cat) for which it should really just communicate \"I don't know.\" \n",
    "\n",
    "How can a model know when it \"doesn't know\"? One way to do this is to alert us when many possible models fit the data reasonably well, but behave differently away from the data (i.e. on previously unseen inputs, like the cat). Unfortunately, the learning algorithm we've used so far---the MLE---doesn't provide us with a way to do this. The MLE gives a *single* model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembling.** If the MLE gives us a single model, why not use it to fit a whole *ensemble* of models? In doing this we would rely on the imperfections of the optimizer to give us a diversity of models. Remember that, especially for more expressive models, optimization tends to get stuck in local optima. What if we were to collect an ensemble of models, all fit with the MLE to data, but each optimized from a different random initialization? Because each model would get stuck in a different local optima, each *might* behave differently than the others away from the data. What's nice about this approach is that it's easy to implement: we already have all the tools we need! Let's see what ensembling a neural network regression model looks like:\n",
    "\n",
    "```{figure} _static/figs/example_nn_ensemble_regression.png\n",
    "---\n",
    "width: 100%\n",
    "name: nn_ensemble_regression\n",
    "align: center\n",
    "---\n",
    "\n",
    "Ensembling 10 neural networks of different sizes.\n",
    "```\n",
    "\n",
    "As you can see, with ensembling, we managed to get a more diverse set of functions (though in this case, not all that diverse---they all have roughly the same trend). Ensembling can be quite effective in practice, but it suffers from one main shortcoming when it comes to safety-critical contexts: it makes implicit assumptions that are difficult to understand. Specifically, we relied on the imperfections of our black-box optimizer to find us a diverse set of models. What kind of models will the optimizer give us, however? Do these models have an *inductive bias* that's appropriate for our task? \n",
    "\n",
    "The need for explicit assumptions motivates us to find an alternative way of fitting our models, leading us to the *Bayesian approach*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Uncertainty?\n",
    "\n",
    "To design a system that captures uncertainty, we first need to know what it means. To do this, let's think about the uncertainty we encounter in everyday scenarios. Maybe these will help us formalize uncertainty mathematically.\n",
    "\n",
    "**Total Certainty.** When you have lots of data, and you have a *mechanistic* understanding of the system, you have certainty. By mechanistic understand, we mean that you can characterize the system mathematically (e.g. you can predict how quickly an object will fall because you have an equation for gravity).\n",
    "> Example: You're asked to predict whether the sun will rise tomorrow. Of course, you know the sun will rise tomorrow, and you're absolutely certain about it (if you have reason to believe the sun will not rise tomorrow, please do let the teaching staff know so they can head to the course bunker). So what makes you sure the sun will rise tomorrow? There are two reasons you will likely think about. (1) You have an abundance of observational data---the sun has risen every day of your life. (2) You have a model (or inductive bias)---you know that day and night are created by the earth's rotation around its axis. Because you understand the mechanical properties of the system, you know that certain predictions just don't make sense---like the sun can't rise twice in the span of 24 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aleatoric Uncertainty.** When you have lots of data from a *noisy* system, you can be certain about the probability of an outcome, but not about the actual outcome. As an example, having observed 100 flips of a fair coin, you can say with fair certainty that the coin will land heads 50\\% of the time. But you will never be able to predict whether the next flip will be heads with any greater accuracy. We call this type of uncertainty, *aleatoric uncertainty*. This is uncertainty that's due to the inherent stochasticity in the system. \n",
    "> You're asked to predict whether it will rain at Wellesley next week. Having lived in New England for a little while, you scoff at the possibility of getting this prediction correct. New England weather is notoriously unpredictable. So what makes you uncertain about our prediction? You have an abundance of experiential data suggesting that weather is difficult to predict (how many times have you stood outside in the rain, while your phone's weather app says it's sunny?). As a result, you're certain we can't make a good prediction.\n",
    "\n",
    "All models we've worked with so far quantify aleatoric uncertainty. For example, in our regression and classification models, our observation error captures aleatoric uncertainty.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epistemic Uncertainty.** When you don't have enough data, and you also don't have a mechanistic understanding of the system, you have epistemic uncertainty. In this case, we'd ideally like to have a diversity of possible models that fit the data. \n",
    "> You're colleague for the IHH is getting married on Venus next week. Your friend, who is also attending the wedding, asked you to predict the weather on Venus next week (so you can choose your outfit). Having forgotten all of your astrophysics knowledge (or having never learned it), you actually don't know what the weather on Venus is like in general. As a result, what makes you uncertain is: (1) a lack of observations or experiential knowledge, and (2) a model (or domain knowledge) about Venus's climate.\n",
    "\n",
    "None of our models so far have been able to capture epistemic uncertainty. In the regression case, epistemic uncertainty would be uncertainty over the *parameters* of the model. Epistemic uncertainty indicates that many potential models could explain the observed data, but we don't know which one is the \"right\" one. We can reduce our uncertainty by observing more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing Regions of Uncertainty.** Returning to our original data of Telekinetic Ability vs. Age, we see that:\n",
    "* Where we've observed data, there's *aleatoric uncertainty*: there's \"noise\" around the trend. No matter how good your model is, it will only be able to predict the trend, not the noise around it.\n",
    "* Where we haven't observed data, there's *epistemic uncertainty*: we don't know what's the appropriate model behavior. In addition to epistemic uncertainty, there's still aleatoric uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.5, 3.5))\n",
    "\n",
    "plt.scatter(data['Age'], data['Telekinetic-Ability'], color='black', alpha=0.5, marker='x', label='Data')\n",
    "plt.axvspan(78, 93, alpha=0.5, color='red', label='Epistemic Uncertainty', zorder=-10)\n",
    "plt.axvspan(0, 78, alpha=0.3, color='blue', label='Aleatoric Uncertainty', zorder=-10)\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Telekinetic Ability')\n",
    "plt.title('Epistemic vs. Aleatoric Uncertainty')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion.** We need some way of capturing epistemic uncertainty. To do this, we'll next introduce a different way of fitting models, called *Bayesian inference*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian Modeling Paradigm\n",
    "\n",
    "**Capturing Epistemic Uncertainty.** So let's go back to the drawing board and rethink how we've been fitting models this whole time. So far, our approach has been finding the *single* model that maximizes the probability of our observed data: $\\theta^\\text{MLE} = \\log p(\\mathcal{D}; \\theta)$. But isn't what we're actually interested is the *distribution* of models given the data, $p(\\theta | \\mathcal{D})$? In other words, conditioned on the data we've observed so far, we want to know which models (represented by their parameters, $\\theta$) are likely to fit the data well. In this new paradigm, we hope that:\n",
    "1. $p(\\theta | \\mathcal{D})$ will capture a diversity of models with different inductive biases.\n",
    "2. We can make our assumptions clear, and we can specify what type of inductive biases are appropriate for our task.\n",
    "\n",
    "And assuming we could compute this distribution, $p(\\theta | \\mathcal{D})$, how would we actually use it for fitting a model? The process goes something like this:\n",
    "1. **Prior:** Prior to having observed data, we express our beliefs about possible sets of parameters, $\\theta$. Our beliefs don't have to be correct, just reasonable. We then encode our beliefs into a distribution, $p_\\theta(\\cdot)$, called the \"prior.\"  For example, we scientifically believe that as age increases, glow decreases. As such, we force the slope to be negative by setting $p_\\theta(\\cdot)$ to be a Normal distribution centered at some negative number. \n",
    "2. **Likelihood:** Having observed data, we can score how well any set of parameters, $\\theta$, from the prior fits the data by evaluating $p(\\mathcal{D} | \\theta)$, the joint data likelihood. This distribution is the *very same* distribution we've worked with in all previous chapters.\n",
    "3. **Posterior Update:** Post observing data, we *update* our beliefs about $\\theta$. We do this by computing $p(\\theta | \\mathcal{D})$. Observing data will help us reduce the initial uncertainty from the prior, honing in on a set of parameters that could explain the data well. As we show in a bit, this posterior update will depend on both the prior and the likelihood distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian Models.** By having a prior distribution $\\theta$ to encode our beliefs, we now treat $\\theta$ as a *random variable*. This means that our generative process will now include an additional line, in which we sample $\\theta$ from the prior. For example, for Bayesian regression, our generative process is:\n",
    "\\begin{align}\n",
    "\\theta &\\sim p_\\theta(\\cdot) \\quad (\\text{prior}) \\\\\n",
    "y_n | x_n, \\theta &\\sim p_{Y | X}(\\cdot | x_n, \\theta) = \\mathcal{N}(\\mu(x_n; \\theta), \\sigma^2) \\quad (\\text{likelihood}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "We can similarly depict our Bayesian model using a directed graphical model as follows:\n",
    "<div class=\"canva-centered-embedding\">\n",
    "  <div class=\"canva-iframe-container\">\n",
    "    <iframe loading=\"lazy\" class=\"canva-iframe\" src=\"https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGLCabxKck&#x2F;QEOGc0tkynDEzLYyuE3z9w&#x2F;view?embed\">\n",
    "    </iframe>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "As you can see, the difference between the depiction of the non-Bayesian and the Bayesian regression is that $\\theta$ is in a circle, indicating its a random variable. Next, the circle is *white* (not filled in), indicating that $\\theta$ is not observed. Our goal will be to infer it given the data. \n",
    "\n",
    "Before defining $p(\\theta | \\mathcal{D})$, let's walk through an example to show you what this process looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Illustration: Bayesian Regression.** Let's see what the Bayesian modeling paradigm looks like for regression, visually. We'll use the generative process from above, setting $\\sigma$ as a constant (so we can ignore it). We've picked an expressive function, $\\mu(x_n; \\theta)$, that will be fun to visualize---its details aren't important. \n",
    "\n",
    "Given our generative process, our goal is to sample the posterior,\n",
    "\\begin{align}\n",
    "p(\\theta | \\mathcal{D}) &= p(\\theta | x_1, \\dots, x_N, y_1, \\dots, y_n).\n",
    "\\end{align}\n",
    "For intuition, we can visualize posterior samples $\\theta \\sim p(\\theta | \\mathcal{D})$ by plotting the *functions* they represent, $\\mu(x_n; \\theta)$. The plot below shows samples from the posterior as the number of points, $N$, increases. \n",
    "\n",
    "```{figure} _static/figs/example_online_bayesian_regression.png\n",
    "---\n",
    "width: 100%\n",
    "name: bayesian-update-example\n",
    "align: center\n",
    "---\n",
    "\n",
    "Samples from the posterior of a Bayesian regression model, capturing epistemic uncertainty.\n",
    "```\n",
    "\n",
    "In the above plot, $N = 0$ represents our *prior*. The functions drawn from our prior illustrate our beliefs about which functions are appropriate for the data. In this specific case, our prior functions don't exhibit any strong trends; overall, the functions don't increase/decrease as age increases---they just wiggle about. However, the functions are incredibly smooth---another prior may have drawn more jagged functions. Whether this prior is appropriate for our task is up to you to decide. \n",
    "\n",
    "Next, we see what happens as we start observing data. As $N$ increases, you can see our prior distribution getting \"filtered out\" by the likelihood. By this, we mean that our posterior will sample functions that are both likely under the prior *and* likelihood. It therefore keeps samples from the prior that also go *through the data* to ensure the likelihood is high. As you can see, in regions of the input space near our observed data, the posterior is quite certain about the trend; it knows the function must pass close to the observed data. But as we move away from the observed data, the posterior maintains a diversity of possible functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Fitting via Bayes' Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes' Rule.** But what is $p(\\theta | \\mathcal{D})$, exactly? How can we possibly write down a distribution of models that fit the data well by hand? To avoid specifying this distribution by hand, we will use *Baye's rule* to write down $p(\\theta | \\mathcal{D})$ in terms of what we already know how to specify: the joint data likelihood, $p(\\mathcal{D} | \\theta)$. and the prior. \n",
    "\n",
    "Let's derive Bayes' rule in general before applying it to our problem. Recall from the chapter on joint probability that a joint distribution over two random variables, $A$ and $B$, can be factorized as follows:\n",
    "\\begin{align}\n",
    "p_{A, B}(a, b) &= p_{B | A}(b | a) \\cdot p_A(a) \\quad (\\text{Option 1}) \\\\\n",
    "&= p_{A | B}(a | b) \\cdot p_B(b) \\quad (\\text{Option 2})\n",
    "\\end{align}\n",
    "This means we can also equate the two factorizations:\n",
    "\\begin{align}\n",
    "p_{B | A}(b | a) \\cdot p_A(a) &= p_{A | B}(a | b) \\cdot p_B(b)\n",
    "\\end{align}\n",
    "Diving both sides by $p_A(a)$, we get:\n",
    "\\begin{align}\n",
    "p_{B | A}(b | a) &= \\frac{p_{A | B}(a | b) \\cdot p_B(b)}{p_A(a)} \\quad \\text{(Bayes' Rule)}\n",
    "\\end{align}\n",
    "This is Bayes' rule. What's cool about it is that relates $p_{B | A}(b | a)$ to $p_{A | B}(a | b)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian Inference.** Using Bayes' rule in the context of our problem, let's treat $\\mathcal{D}$ *and* $\\theta$ as random variables. We can now relate $p(\\theta | \\mathcal{D})$, which we don't know how to specify, to $p(\\mathcal{D} | \\theta)$, which we do know how to specify:\n",
    "\\begin{align}\n",
    "\\underbrace{p(\\theta | \\mathcal{D})}_{\\text{posterior}} &= \\frac{\\overbrace{p(\\mathcal{D} | \\theta)}^{\\text{likelihood}} \\cdot \\overbrace{p(\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal{D})}_{\\text{normalizing const.}}}\n",
    "\\end{align}\n",
    "When used as a model-fitting paradigm, each term in Bayes' rule has a special name. We'll now define each:\n",
    "* **Likelihood:** This is the data joint likelihood, which we've previously maximized as part of the MLE.\n",
    "    > For example, suppose we're fitting a linear regression model to predict an intergalactic being's glow given age. Our model is then:\n",
    "    > \\begin{align}\n",
    "    p(\\mathcal{D} | \\theta) &= \\prod\\limits_{n=1}^N p(\\mathcal{D}_n | \\theta) \\\\\n",
    "    &= \\prod\\limits_{n=1}^N p_{Y | X}(y_n | x_n, \\theta) \\\\\n",
    "    &= \\prod\\limits_{n=1}^N \\mathcal{N}(y_n | \\underbrace{\\theta_0 + \\theta_1 \\cdot x_n}_{\\mu(x_n; \\theta)}, \\sigma^2)\n",
    "    \\end{align}\n",
    "    > where $\\theta = \\{ \\theta_0, \\theta_1 \\}$ is the slope and intercept, and $\\sigma$ is observation noise variance (which we fix as a constant for now). \n",
    "* **Prior:** This is the distribution of models we're willing to consider *before having observed any data*. The prior allows us to specify our model's *inductive bias*.\n",
    "    > Continuing with the above example, we know that in general, glow decreases with age. We can encode this belief into the inductive bias of the model by selecting an appropriate prior distribution---one for which the slope, $\\theta_1$, is likely negative. As an example, we could select, $\\mathcal{N}(-1, 0.1)$. In this way, $\\theta_1$ is most likely to be near $-1$. We can similarly encode our belief into the intercept, $\\theta_0$, saying we believe it should be positive: $\\mathcal{N}(1, 0.1)$. \n",
    "    > Putting these together, we get the following prior distribution over our model parameters:\n",
    "    > \\begin{align}\n",
    "    p_\\theta(\\cdot) = p_{\\theta_1}(\\cdot) \\cdot p_{\\theta_0}(\\cdot) = \\mathcal{N}(-1, 0.1) \\cdot \\mathcal{N}(1, 0.1)\n",
    "    \\end{align}\n",
    "    In contrast to the ensembling approach, prior specification makes our assumptions about uncertainty explicit and easier to interrogate. \n",
    "* **Posterior:** This is the distribution of interest. It's called a posterior because it determines the distribution of likely models, $\\theta$, *after having observed data*. The posterior balances information from both the prior and the likelihood. \n",
    "* **Normalizing Constant:** This is a constant that turns the whole fraction into a valid probability density function (i.e. a function that integrates to 1). To compute $p(\\mathcal{D})$, we integrate the numerator of Bayes' rule over the support of $\\theta$:\n",
    "    \\begin{align}\n",
    "    p(\\mathcal{D}) &= \\int\\limits \\underbrace{p(\\mathcal{D} | \\theta) \\cdot p(\\theta)}_{\\text{numerator of Bayes' rule}} d\\theta\n",
    "    \\end{align}\n",
    "    In this way, when we divide by it, the whole fraction integrates to $1$. The formula for this normalizing constant is derived from the *[law of total probability](https://en.wikipedia.org/wiki/Law_of_total_probability)*---more on this at a later chapter. And for now, we won't worry about how to actually compute this integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational Efficiency.** Unfortunately for us, for most models, Bayesian inference is *intractable*, meaning there exists no efficient algorithm for posterior sampling. As a result, we will have to resort to approximations. This is the main drawback of Bayesian inference. Approximate Bayesian inference is fascinating, but unfortunately, we will not get to study it here. We will, however, learn how to use some approximate inference algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference in `NumPyro`\n",
    "\n",
    "**The Model.** As you may have expected, writing out explicitly all parts of the model in math will now allow us to translate into `NumPyro`. And using some wizardry, `NumPyro` will do the heavy lifting for us, sampling from the model's posterior.\n",
    "\n",
    "The process of describing a Bayesian in `NumPyro` will actually not differ much from the process of writing its coding its non-Bayesian counterpart. We'll therefore start with the non-Bayesian version using the syntax you're already familiar with, and then we'll show you how to make it Bayesian. \n",
    "\n",
    "**Review: Non-Bayesian Linear Regression.** For the specific model we'll implement, we'll use a univariate linear regression model. Here's the non-Bayesian version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_linear_regression(N, x, y=None):\n",
    "    slope = numpyro.param(\n",
    "        'slope',\n",
    "        jnp.array([-1.0]),\n",
    "        constraint=C.real,\n",
    "    )\n",
    "\n",
    "    intercept = numpyro.param(\n",
    "        'intercept',\n",
    "        jnp.array([1.0]),\n",
    "        constraint=C.real,\n",
    "    )\n",
    "\n",
    "    std_dev = numpyro.param(\n",
    "        'std_dev',\n",
    "        jnp.array(1.0),\n",
    "        constraint=C.positive,\n",
    "    )\n",
    "\n",
    "    with numpyro.plate('data', N):\n",
    "        mu = numpyro.deterministic('mu', slope * x + intercept)\n",
    "        p_y_given_x = D.Normal(mu, std_dev)\n",
    "        numpyro.sample('y', p_y_given_x, obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model, there's only one primitive we haven't covered yet: `numpyro.deterministic`. Notice how `cs349_sample_generative_process` returns variables created with `numpyro.param` and `numpyro.sample`? This new primitive, `numpyro.deterministic`, allows you to save all other variables. In this case, since we're interested in visualizing *epistemic uncertainty*, we want to visualize $\\mu(\\cdot; \\theta)$. This new primitive allows us to save it. When calling `cs349_sample_generative_process`, we'll now be able to see a new variable called `mu`.\n",
    "\n",
    "For completeness, let's also fit the non-Bayesian model using the MLE to our IHH data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 10000\n",
    "\n",
    "# Define an optimizer; here we chose the \"Adam\" algorithm\n",
    "optimizer = numpyro.optim.Adam(step_size=0.01)\n",
    "\n",
    "# Pick a random generator seed for the optimizer\n",
    "key_optimizer = jrandom.PRNGKey(seed=0)\n",
    "\n",
    "# Fit the model via the MLE\n",
    "result = cs349_mle(\n",
    "    univariate_linear_regression, \n",
    "    optimizer, \n",
    "    key_optimizer, \n",
    "    NUM_ITERATIONS,\n",
    "    len(data), \n",
    "    jnp.array(data['Age']), \n",
    "    y=jnp.array(data['Telekinetic-Ability']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fitted model, we can make predictions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for a set of test inputs\n",
    "x_test = jnp.linspace(0.0, 100.0, 100)\n",
    "\n",
    "# Make predictions\n",
    "samples = cs349_sample_generative_process(\n",
    "    result.model_mle, \n",
    "    jrandom.PRNGKey(seed=0), \n",
    "    len(x_test), \n",
    "    x_test,\n",
    ")\n",
    "\n",
    "y_pred = samples['mu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `samples` now contains `mu`, saved from `numpyro.deterministic`.\n",
    "\n",
    "Finally, let's plot its loss and trend against the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# Plot the loss\n",
    "axes[0].scatter(jnp.arange(NUM_ITERATIONS), result.losses)\n",
    "axes[0].set_xlabel('Optimization Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Convergence of MLE')\n",
    "\n",
    "# Plot the trend of the regression\n",
    "axes[1].plot(\n",
    "    x_test, \n",
    "    y_pred, \n",
    "    color='blue', \n",
    "    alpha=0.5,\n",
    "    label=r'$\\mu(\\cdot; \\theta)$',\n",
    ")\n",
    "\n",
    "# Plot the data\n",
    "axes[1].scatter(\n",
    "    data['Age'], data['Telekinetic-Ability'], \n",
    "    color='black', marker='x', alpha=0.5, label='Data',\n",
    ")\n",
    "\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Telekinetic Ability')\n",
    "axes[1].set_title('Non-Bayesian Linear Regression')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our gradient optimizer converged, and our model fits the data as well as any linear model can fit a non-linear trend.\n",
    "\n",
    "**Bayesian Linear Regression.** The main difference between the non-Bayesian and the Bayesian versions of the model is that we no longer have fixed parameters. Our parameters are now random variables with distributions (or priors). As such, instead of using the primitive `numpyro.param`, we will use `numpyro.sample`. For example, instead of describing `slope` as follows:\n",
    "```\n",
    "slope = numpyro.param(\n",
    "    'slope',\n",
    "    jnp.array([-1.0]),\n",
    "    constraint=C.real,\n",
    ")\n",
    "```\n",
    "we instead write:\n",
    "```\n",
    "p_slope = D.Normal(-1.0, 0.1)\n",
    "slope = numpyro.sample('slope', p_slope)\n",
    "```\n",
    "In this example, we specified that the prior distribution for `slope` is a Gaussian, centered at $-1.0$. Lastly, notice that, unlike in `numpyro.sample('y', p_y_given_x, obs=y)`,  when sampling the slope we *do not* pass in `obs=`. This is because we have not *observed* slope in the data---this tells `NumPyro` we'd like to *inter* it.\n",
    "\n",
    "Replacing all fixed parameters with random variables yields the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_bayesian_linear_regression(N, x, y=None):\n",
    "    # Prior for the slope\n",
    "    p_slope = D.Normal(-1.0, 0.1)\n",
    "    slope = numpyro.sample('slope', p_slope)\n",
    "\n",
    "    # Prior for the intercept\n",
    "    p_intercept = D.Normal(-1.0, 0.1)\n",
    "    intercept = numpyro.sample('intercept', p_intercept)\n",
    "\n",
    "    # Inverse gamma is a standard prior for the observation noise variance\n",
    "    # To get the standard deviation, we just take the square-root\n",
    "    p_var = D.InverseGamma(3.0, rate=0.5)\n",
    "    var = numpyro.sample('var', p_var)\n",
    "    std_dev = jnp.sqrt(var)\n",
    "\n",
    "    # Same as in the non-Bayesian version\n",
    "    with numpyro.plate('data', N):\n",
    "        mu = numpyro.deterministic('mu', slope * x + intercept)\n",
    "        p_y_given_x = D.Normal(mu, std_dev)\n",
    "        numpyro.sample('y', p_y_given_x, obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Bayesian model, let's fit it by sampling from the posterior. We will do this using a helper function we've created, `cs349_bayesian_inference`. This function will use an algorithm called *Markov Chain Monte Carlo* (MCMC). We will not get into how such algorithms work, but there are a few things you should know about them:\n",
    "1. They are iterative.\n",
    "2. Given an infinite number of iterations, they will eventually draw samples from the model's posterior.\n",
    "3. Since we cannot run them for an infinite number of iterations, we have to take a few precautions. First, it takes MCMC a while to start drawing samples from the posterior. As a result, we toss out the first batch of samples drawn in what's called a \"warmup\" phase. Second, there are some diagnostics you can run to assess the quality of the samples. We'll get into these in a bit. \n",
    "\n",
    "Our helper function takes in the following arguments:\n",
    "* A Bayesian `NumPyro` model.\n",
    "* A random generator key, used by the inference algorithm.\n",
    "* The number of warmup steps we'd like the algorithm to run for.\n",
    "* The number of samples to draw (after the warmup phase). \n",
    "* Finally, we pass in the arguments of the model. \n",
    "\n",
    "Putting all of this together, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = cs349_bayesian_inference(\n",
    "    univariate_bayesian_linear_regression, \n",
    "    jrandom.PRNGKey(seed=0), \n",
    "    10000, \n",
    "    30, \n",
    "    len(data), \n",
    "    jnp.array(data['Age']), \n",
    "    y=jnp.array(data['Telekinetic-Ability']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our helper function returns a dictionary, `posterior_samples`, the posterior samples. Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in posterior_samples.items():\n",
    "    print('\"{}\" has shape = {}'.format(k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the $0$th dimension of all of the shapes is the number of samples drawn. \n",
    "\n",
    "Notice also that the shape of `mu` is the number of samples by number of training points. We, however, would like to make predictions for non-training points. How can we do that? Using another helper function we've created: `cs349_sample_predictive`. This function takes in:\n",
    "* The `NumPyro` model.\n",
    "* A random generator key.\n",
    "* The posterior samples, returned from `cs349_bayesian_inference`.\n",
    "* Arguments needed for the model. In this case, the arguments are the test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_samples = cs349_sample_predictive(\n",
    "    univariate_bayesian_linear_regression, \n",
    "    jrandom.PRNGKey(seed=0),\n",
    "    posterior_samples,\n",
    "    len(x_test),\n",
    "    x_test,\n",
    ")\n",
    "\n",
    "for k, v in predictive_samples.items():\n",
    "    print('\"{}\" has shape = {}'.format(k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can go ahead and visualize our samples of `mu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "\n",
    "x_test = jnp.linspace(0.0, 100.0, 100)\n",
    "\n",
    "# Iterate over posterior samples and plot them\n",
    "for mu in predictive_samples['mu']:\n",
    "    plt.plot(\n",
    "        x_test, \n",
    "        mu, \n",
    "        color='blue', \n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "# Plot the training data\n",
    "plt.scatter(\n",
    "    data['Age'], data['Telekinetic-Ability'], \n",
    "    color='black', marker='x', alpha=0.5, label='Data',\n",
    ")\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Telekinetic Ability')\n",
    "plt.title('Bayesian Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the figure, we have a full distribution over different linear functions that fit our data best. Of course, none of them are particularly good, since the model is non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convergence of Posterior as $N \\rightarrow \\infty$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependence of Uncertainty on Prior Assumptions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
