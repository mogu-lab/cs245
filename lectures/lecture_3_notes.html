

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>14. Lecture #3: Bayesian Modeling &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/lecture_3_notes';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/lectures/lecture_3_notes.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="21. Lecture #4: Bayesian versus Frequentist Inference" href="lecture_4_notes.html" />
    <link rel="prev" title="7. Lecture #2: Maximimum Likelihood Estimation" href="lecture_2_notes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Probabilistic Foundations of Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Exact Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_1_notes.html">1. Lecture #1: Course Overview</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_2_notes.html">7. Lecture #2: Maximimum Likelihood Estimation</a></li>






<li class="toctree-l1 current active"><a class="current reference internal" href="#">14. Lecture #3: Bayesian Modeling</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_4_notes.html">21. Lecture #4: Bayesian versus Frequentist Inference</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Sampling-Based Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_5_notes.html">1. Lecture #5: Sampling for Posterior Simulation</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_6_notes.html">7. Lecture #6: Monte Carlo Integration</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_7_notes.html">14. Lecture #7: Markov Chain Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_8_notes.html">18. Lecture #8: Metropolis-Hastings and Gibbs</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_9_notes.html">23. Lecture #9: Latent Variable Models and MLE</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Gradient-Based Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_10_notes.html">1. Lecture #10: Bayesian Latent Variable Models and Variational Inference</a></li>

<li class="toctree-l1"><a class="reference internal" href="lecture_11_notes.html">3. Lecture #11: Hierarchical Models</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_12_notes.html">7. Lecture #12: Logistic Regression and Gradient Descent</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_13_notes.html">11. Lecture #13: Stochastic Gradient Descent and Simulated Annealing</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_14_notes.html">17. Lecture #14: Hamiltonian Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_15_notes.html">21. Lecture #15: Parallel Tempering and Stochastic HMC</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_16_notes.html">25. Lecture #16: Neural Network Models for Regression</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_17_notes.html">31. Lecture #17: Black-box Variational Inference</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_18_notes.html">35. Lecture #18: Automatic Differentiation</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_19_notes.html">38. Lecture #19: Variational Inference in Context</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_20_notes.html">43. Lecture #20: Variational Autoencoders</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_21_notes.html">46. Lecture #21: Implementation of Variational Autoencoders</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/mogu-lab/cs349-fall-2024/blob/master/lectures/lecture_3_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li><a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fmogu-lab%2Fcs349-fall-2024%2Fblob%2Fmaster%2Flectures/lecture_3_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onDeepnote"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_deepnote.svg">
  </span>
<span class="btn__text-container">Deepnote</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/lecture_3_notes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture #3: Bayesian Modeling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">14. Lecture #3: Bayesian Modeling</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">15. AM 207: Advanced Scientific Computing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">15.1. Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">15.2. Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">16. Outline</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-the-method-of-maximum-likelihood">17. Review of the Method of Maximum Likelihood</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-method-of-maximum-likelihood">17.1. The Method of Maximum Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-mle">17.2. Evaluating the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-the-maximum-likelihood-estimator">17.3. Properties of The Maximum Likelihood Estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-bias-always-bad">17.4. Is Bias Always Bad?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decomposition-of-mse">Decomposition of MSE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-the-bias-variance-trade-off">Example of the Bias Variance Trade-off</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-mle-overfitting-under-scarcity-of-data">17.5. Limitations of MLE: Overfitting Under Scarcity of Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">17.6. Limitations of MLE: Overfitting Under Scarcity of Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">17.7. Regularization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#models-for-real-data">18. Models for Real Data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#die-roll">18.1. Die Roll</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#video-ranking">18.2. Video Ranking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kidney-cancer-rates">18.3. Kidney Cancer Rates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">18.4. Kidney Cancer Rates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#birth-weights">18.5. Birth Weights</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-beta-binomial-model">19. The Beta-Binomial Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coin-toss-model-revisited">19.1. The Coin Toss Model: Revisited</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#incoporating-prior-beliefs">19.2. Incoporating Prior Beliefs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">19.3. The Beta-Binomial Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-for-the-beta-binomial-model">19.4. Posterior for the Beta-Binomial Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-posterior-bayesian-update">19.5. Interpreting the Posterior: Bayesian Update</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-bayesian-update-for-the-coin-flip">19.6. Simulation: Bayesian Update for the Coin Flip</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-iterated-bayesian-update-for-the-coin-flip">19.7. Simulation: Iterated Bayesian Update for the Coin Flip</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions">19.8. Making Predictions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-posterior-predictive-for-the-coin-flip">19.9. Simulation: Posterior Predictive for the Coin Flip</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-modeling-a-summary">20. Bayesian Modeling - A Summary</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayesian-modeling-process">20.1. The Bayesian Modeling Process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-bayesian-models">20.2. Evaluating Bayesian Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-do-priors-come-from">20.3. Where do Priors Come From?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uninformative-priors-how-to-say-i-don-t-know">20.4. Uninformative Priors: How to Say I Don’t Know</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="lecture-3-bayesian-modeling">
<h1><span class="section-number">14. </span>Lecture #3: Bayesian Modeling<a class="headerlink" href="#lecture-3-bayesian-modeling" title="Permalink to this heading">#</a></h1>
</section>
<section id="am-207-advanced-scientific-computing">
<h1><span class="section-number">15. </span>AM 207: Advanced Scientific Computing<a class="headerlink" href="#am-207-advanced-scientific-computing" title="Permalink to this heading">#</a></h1>
<section id="stochastic-methods-for-data-analysis-inference-and-optimization">
<h2><span class="section-number">15.1. </span>Stochastic Methods for Data Analysis, Inference and Optimization<a class="headerlink" href="#stochastic-methods-for-data-analysis-inference-and-optimization" title="Permalink to this heading">#</a></h2>
</section>
<section id="fall-2021">
<h2><span class="section-number">15.2. </span>Fall, 2021<a class="headerlink" href="#fall-2021" title="Permalink to this heading">#</a></h2>
<img src="fig/logos.jpg" style="height:150px;"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Import basic libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="outline">
<h1><span class="section-number">16. </span>Outline<a class="headerlink" href="#outline" title="Permalink to this heading">#</a></h1>
<ol class="arabic simple">
<li><p>Review of the Method of Maximum Likelihood</p></li>
<li><p>Models for Real Data</p></li>
<li><p>The Beta-Binomial Model</p></li>
<li><p>Bayesian Modeling</p></li>
<li><p>Connections and Comparisons with Frequentist Inference</p></li>
</ol>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="review-of-the-method-of-maximum-likelihood">
<h1><span class="section-number">17. </span>Review of the Method of Maximum Likelihood<a class="headerlink" href="#review-of-the-method-of-maximum-likelihood" title="Permalink to this heading">#</a></h1>
<section id="the-method-of-maximum-likelihood">
<h2><span class="section-number">17.1. </span>The Method of Maximum Likelihood<a class="headerlink" href="#the-method-of-maximum-likelihood" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>(Model)</strong> Assume observations from <span class="math notranslate nohighlight">\(N\)</span> number of <em><strong>independently and identically distributed</strong></em> outcomes, <span class="math notranslate nohighlight">\(Y_1, \ldots, Y_N\)</span>, with <span class="math notranslate nohighlight">\(Y_n \sim p(Y|\theta)\)</span> and where <span class="math notranslate nohighlight">\(\theta\)</span> is the set of parameters of the distribution <span class="math notranslate nohighlight">\(p(Y|\theta)\)</span>. The likelihood function is
\begin{aligned}
\mathcal{L}(\theta) = \prod_{n=1}^N p(y_n | \theta)
\end{aligned}</p></li>
<li><p><strong>(Inference)</strong>
We estimate <span class="math notranslate nohighlight">\(\theta\)</span> using the <em><strong>maximium likelihood estimate</strong></em>, defined as
\begin{aligned}
\theta_{\text{MLE}} = \underset{\theta}{\mathrm{argmax}}; \mathcal{L}(\theta) = \underset{\theta}{\mathrm{argmax}}; \prod_{n=1}^N p(y_n | \theta)
\end{aligned}</p></li>
<li><p><strong>(Inference Method)</strong></p></li>
</ol>
<ul class="simple">
<li><p><em>Unconstrained Optimization:</em> if <span class="math notranslate nohighlight">\(\theta\)</span> doesn’t need to satisfy any special property, then it’s as simple as setting the <em><strong>gradient</strong></em> of the likelihood equal to zero and solving!
<strong>(Except it’s not that simple!)</strong><br><br></p></li>
<li><p><em>Constrained Optimization:</em> if <span class="math notranslate nohighlight">\(\theta\)</span> needs to satisfy special properties, then it’s as simple as setting the gradient of the <em><strong>Lagrangian</strong></em> of the likelihood and the constraint equal to zero and solving!
<strong>(Except it’s not that simple!)</strong></p></li>
</ul>
</section>
<section id="evaluating-the-mle">
<h2><span class="section-number">17.2. </span>Evaluating the MLE<a class="headerlink" href="#evaluating-the-mle" title="Permalink to this heading">#</a></h2>
<p>If we have the true parameters <span class="math notranslate nohighlight">\(\theta\)</span>, we can compute the <em><strong>Mean Squared Error</strong></em>:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{MSE}_{\theta} = \mathbb{E}_{Y^{\theta}}\left[(\theta_{\text{MLE}}\left(Y^\theta\right) - \theta)^2\right]
\]</div>
<p>If we don’t have the true parameters <span class="math notranslate nohighlight">\(\theta\)</span>, we can use <span class="math notranslate nohighlight">\(\theta_{\text{MLE}}\)</span> <em><strong>predict</strong></em> or <em><strong>simulate</strong></em> data and compare it with the observed data, i.e. sample</p>
<div class="math notranslate nohighlight">
\[
Y^{\theta_{\text{MLE}}} \sim p(Y| \theta_{\text{MLE}}),
\]</div>
<p>compare <span class="math notranslate nohighlight">\(Y^{\theta_{\text{MLE}}}\)</span> and <span class="math notranslate nohighlight">\(Y^{\theta}\)</span>.</p>
</section>
<section id="properties-of-the-maximum-likelihood-estimator">
<h2><span class="section-number">17.3. </span>Properties of The Maximum Likelihood Estimator<a class="headerlink" href="#properties-of-the-maximum-likelihood-estimator" title="Permalink to this heading">#</a></h2>
<p><strong>Why Choose MLE?</strong>
Asymptotically, i.e. given an infinite number of samples, the MLE is</p>
<ol class="arabic simple">
<li><p><em>Consistent:</em> <span class="math notranslate nohighlight">\(\theta_{\text{MLE}}\)</span> approaches the true parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p><em>Unbiased:</em> The average <span class="math notranslate nohighlight">\(\theta_{\text{MLE}}\)</span>, taken over many different samples of the data, is <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p><em>Minimum Variance:</em> The MLE has the lowest variance of all unbiased estimators.</p></li>
</ol>
<p><strong>Why Not Choose MLE?</strong>
When the sample size is “small”, the MLE can be</p>
<ol class="arabic simple">
<li><p><em>Overfitted:</em> The MLE can is sensitive to outliers in the data.</p></li>
<li><p><em>Biased:</em> The average <span class="math notranslate nohighlight">\(\theta_{\text{MLE}}\)</span>, taken over many different data samples, is not <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p><em>Imprecise:</em> The MLE can have high variance.</p></li>
</ol>
<p><strong>What Other Estimators are There?</strong></p>
<ol class="arabic simple">
<li><p>Method of Moments</p></li>
<li><p>Minimum Variance Unbiased Estimator</p></li>
<li><p>Regularized MLE</p></li>
</ol>
<p><strong>Note:</strong> the computation of (1) and (2) can be much more complex than MLE.</p>
</section>
<section id="is-bias-always-bad">
<h2><span class="section-number">17.4. </span>Is Bias Always Bad?<a class="headerlink" href="#is-bias-always-bad" title="Permalink to this heading">#</a></h2>
<p>Recall that we can use the properties of expectations to decompose the mean squared error of the maximum likelihood estimator:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{MSE}_{\theta_{\text{MLE}}} = \underbrace{\mathbb{E}\left[(\theta_{\text{MLE}} - \mathbb{E}[\theta_{\text{MLE}}])^2\right]}_{\text{variance}} + \underbrace{\left( \mathbb{E}[\theta_{\text{MLE}}] - \theta\right)^2}_{\text{bias}}
\]</div>
<p>Although we want an unbiased estimator, the above decomposition says that if the variance of the estimator is high our espected error will nonetheless be high.</p>
<p>The <em><strong>variance-bias trade-off</strong></em> refers to the phenomenon that, in many cases, when estimators have low bias they have high corresponding variance (and vice versa), and hence high MSE.</p>
<section id="decomposition-of-mse">
<h3>Decomposition of MSE<a class="headerlink" href="#decomposition-of-mse" title="Permalink to this heading">#</a></h3>
<p>\begin{aligned}
\mathrm{MSE}<em>{\theta</em>{\text{MLE}}} &amp;= \mathbb{E}<em>{Y^\theta} \left[\left(\theta</em>{\text{MLE}} - \theta \right)^2 \right]\
&amp;= \mathbb{E}<em>{Y^\theta}\left[\left( \theta</em>{\text{MLE}} - \mathbb{E}[\theta_{\text{MLE}}] + \mathbb{E}[\theta_{\text{MLE}}] - \theta \right)^2 \right]\
&amp;= \mathbb{E}\left[\left(\theta_{\text{MLE}} - \mathbb{E}[\theta_{\text{MLE}}]\right)^2\right] + \mathbb{E}\left[\left(\mathbb{E}[\theta_{\text{MLE}}] - \theta \right)^2 \right] + 2\mathbb{E}\left[ (\theta_{\text{MLE}} - \mathbb{E}[\theta_{\text{MLE}}])(\mathbb{E}[\theta_{\text{MLE}}] - \theta)\right]\
&amp;= \underbrace{\mathbb{E}\left[(\theta_{\text{MLE}} - \mathbb{E}[\theta_{\text{MLE}}])^2\right]}<em>{\text{variance}} + \underbrace{\left( \mathbb{E}[\theta</em>{\text{MLE}}] - \theta\right)^2}_{\text{bias}}
\end{aligned}</p>
</section>
<section id="example-of-the-bias-variance-trade-off">
<h3>Example of the Bias Variance Trade-off<a class="headerlink" href="#example-of-the-bias-variance-trade-off" title="Permalink to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(Y \sim\mathcal{N}(0, \sigma^2)\)</span>. Consider two estimators of the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\widehat{\theta}_1(Y) = Y^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\widehat{\theta}_2(Y) = \frac{1}{3}Y^2\)</span></p></li>
</ol>
<p>The expected values of the estimators are:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\widehat{\theta}_1] = \mathbb{E}[Y^2]=\sigma^2\)</span>, using the fact <span class="math notranslate nohighlight">\(Y^2\)</span> is a Gamma distribution with mean <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\widehat{\theta}_1] = \mathbb{E}[Y^2]=\frac{1}{3}\sigma^2\)</span>
So we see that <span class="math notranslate nohighlight">\(\widehat{\theta}_1\)</span> is unbiased, whereas <span class="math notranslate nohighlight">\(\widehat{\theta}_2\)</span> is biased.</p></li>
</ol>
<p>Note that the two estimators are both of the form <span class="math notranslate nohighlight">\(cY^2\)</span> where <span class="math notranslate nohighlight">\(c\)</span> is a constant. We can compute the MSE, in a general way, for this class of estimators:</p>
<p>\begin{aligned}
\mathrm{MSE}_{\widehat{\theta}} &amp;= \mathbb{E}\left[\left(\widehat{\theta}_2 - \sigma^2\right)^2\right]\
&amp;= \mathbb{E}\left[c^2Y^4 - 2\sigma^2 cY^2 + \sigma^4 \right]\
&amp;= c^2\mathbb{E} \left[Y^4 \right] - 2\sigma^2c\mathbb{E} \left[Y^2 \right] + \sigma^4\
&amp;= 3c^2\sigma^4 - 2c\sigma^4 + \sigma^4\
&amp;= \sigma^4(3c^2 - 2c + 1)
\end{aligned}</p>
<p>From the expression of the MSE, we can find the <span class="math notranslate nohighlight">\(c\)</span> that will minimize <span class="math notranslate nohighlight">\(\mathrm{MSE}_{\widehat{\theta}}\)</span> - this involves finding the stationary points of the derivative of <span class="math notranslate nohighlight">\(\mathrm{MSE}_{\widehat{\theta}}\)</span> (and then checking concavity/convexity!):</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dc}\mathrm{MSE}_{\widehat{\theta}} = \sigma^4(6a - 2) = 0
\]</div>
<p>We see that <span class="math notranslate nohighlight">\(\mathrm{MSE}_{\widehat{\theta}}\)</span> has a unique global minimum at <span class="math notranslate nohighlight">\(c = \frac{1}{3}\)</span>. That is, the biased estimator <span class="math notranslate nohighlight">\(\widehat{\theta}_2\)</span> has smaller MSE than the unbiased estimator <span class="math notranslate nohighlight">\(\widehat{\theta}_1\)</span>.</p>
</section>
</section>
<section id="limitations-of-mle-overfitting-under-scarcity-of-data">
<h2><span class="section-number">17.5. </span>Limitations of MLE: Overfitting Under Scarcity of Data<a class="headerlink" href="#limitations-of-mle-overfitting-under-scarcity-of-data" title="Permalink to this heading">#</a></h2>
<p>Suppose that we have three observations from a Bernoulli distribution, <span class="math notranslate nohighlight">\(Ber(\theta)\)</span>: <span class="math notranslate nohighlight">\(\{ H, H, H\}\)</span>. From what we’ve see before, the MLE of <span class="math notranslate nohighlight">\(\theta\)</span> is
$<span class="math notranslate nohighlight">\(
\theta_{\text{MLE}} = \frac{3}{3} = 1.
\)</span>$
Is this a good estimate of the bias of the coin? What can we do to make this estimate better?</p>
</section>
<section id="id1">
<h2><span class="section-number">17.6. </span>Limitations of MLE: Overfitting Under Scarcity of Data<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>Suppose that we have two YouTube videos with 4/5 likes and 3,500/5,000 likes respectively. We can model the probability that a viewer will like each video as two Bernoulli distribtutions, <span class="math notranslate nohighlight">\(Ber(\theta_1)\)</span>, <span class="math notranslate nohighlight">\(Ber(\theta_2)\)</span>, where <span class="math notranslate nohighlight">\(\theta_i\)</span> is the “inherent” likeability of each video.</p>
<p>Again, we can compute the MLE of the Bernoulli parameters:</p>
<div class="math notranslate nohighlight">
\[
\theta_1 = 4/5 = 0.8,\; \theta_2 = 3,500/5,000 = 0.75.
\]</div>
<p>It is fair to say that the second video is more likeable base on our estimates?</p>
</section>
<section id="regularization">
<h2><span class="section-number">17.7. </span>Regularization<a class="headerlink" href="#regularization" title="Permalink to this heading">#</a></h2>
<p>We can prevent MLE from overfitting to the observations when training data is scarce by constraining it from unreasonable values.</p>
<p>Recall that in order to prevent the MLE solution for linear regression from learn unrealistically large slopes and intercepts, we add <span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization on the model parameters during training, essentially forcing the parameters to stay as small (close to zero) as they can be while still capturing the data.</p>
<p>Similarly, if want the MLE of the parameter <span class="math notranslate nohighlight">\(\theta\)</span> of a Bernoulli distribution to avoid extreme values (1 and 0), we need to ‘anchor’ our estimation of <span class="math notranslate nohighlight">\(\theta\)</span> to some ‘reasonable’ value:</p>
<p>\begin{align}
\theta_{\text{MLE Reg.}} = \frac{\text{# positive outcome} + \alpha}{\text{# total trials} + \beta}
\end{align}</p>
<p>The fraction <span class="math notranslate nohighlight">\(\alpha/\beta\)</span> expresses your notion of what is a reasonable looking probability.</p>
<p>But is regularization a principled way to perform inference (i.e. will it ruin the nice properties of MLE)? How do you choose the hyperparameters <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span> in a principled manner?</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="models-for-real-data">
<h1><span class="section-number">18. </span>Models for Real Data<a class="headerlink" href="#models-for-real-data" title="Permalink to this heading">#</a></h1>
<section id="die-roll">
<h2><span class="section-number">18.1. </span>Die Roll<a class="headerlink" href="#die-roll" title="Permalink to this heading">#</a></h2>
<p>We can model the outcome, <span class="math notranslate nohighlight">\(Y\)</span>, of a single dice roll with a <em><strong>categorical</strong></em> distribution,</p>
<div class="math notranslate nohighlight">
\[
Y \sim Cat(\theta)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta = [\theta_1 \ldots \theta_6]\)</span> is a vector satisfying <span class="math notranslate nohighlight">\(\sum_{i=1}^6 \theta_i = 1\)</span>, and where <span class="math notranslate nohighlight">\(\theta_i\)</span> is the probability of rolling a number <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Given <span class="math notranslate nohighlight">\(Y_1,\ldots, Y_N\)</span> identically and independently distributed outcomes with <span class="math notranslate nohighlight">\(Y_n \sim Cat(\theta)\)</span>. Denote the total number observed for each face as a vector <span class="math notranslate nohighlight">\(F = [f_1 \ldots f_6]\)</span>, where <span class="math notranslate nohighlight">\(f_i\)</span> is the number of times the number <span class="math notranslate nohighlight">\(i\)</span> was rolled. Then <span class="math notranslate nohighlight">\(F\)</span> can be modeled with a <em><strong>multinomial</strong></em> distribution,</p>
<div class="math notranslate nohighlight">
\[
F \sim Mul(N, \theta).
\]</div>
<p><strong>Model Critique:</strong> What are the assumptions made in this model? Are they realistic?</p>
</section>
<section id="video-ranking">
<h2><span class="section-number">18.2. </span>Video Ranking<a class="headerlink" href="#video-ranking" title="Permalink to this heading">#</a></h2>
<p>We can model the outcome, <span class="math notranslate nohighlight">\(Y\)</span>, of a user rating for a specific YouTube video as a Bernoulli distribution,</p>
<div class="math notranslate nohighlight">
\[
Y \sim Ber(\theta)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is the probability that a user will like the video.</p>
<p>Given <span class="math notranslate nohighlight">\(Y_1,\ldots, Y_N\)</span> identically and independently distributed outcomes with <span class="math notranslate nohighlight">\(Y_n \sim Ber(\theta)\)</span>. Denote the total number of likes by <span class="math notranslate nohighlight">\(L\)</span>. Then <span class="math notranslate nohighlight">\(L\)</span> can be modeled with a <em><strong>binomial</strong></em> distribution,</p>
<div class="math notranslate nohighlight">
\[
L \sim Bin(N, \theta).
\]</div>
<p><strong>Model Critique:</strong> What are the assumptions made in this model? Are they realistic?</p>
</section>
<section id="kidney-cancer-rates">
<h2><span class="section-number">18.3. </span>Kidney Cancer Rates<a class="headerlink" href="#kidney-cancer-rates" title="Permalink to this heading">#</a></h2>
<p>Given a dataset with <span class="math notranslate nohighlight">\(N\)</span> number of US counties and the incidents of kidney cancer in each county, we can model the observed incidents of cancer, <span class="math notranslate nohighlight">\(C_n\)</span>, of the <span class="math notranslate nohighlight">\(n\)</span>-th county with a Poisson distribution,</p>
<div class="math notranslate nohighlight">
\[
C_n \sim Poi(T_n\theta_n)
\]</div>
<p>where <span class="math notranslate nohighlight">\(T_n\)</span> is the total population of the county and <span class="math notranslate nohighlight">\(\theta_n\)</span> is the “true” cancer rate of the county.</p>
<p><strong>Model Critique:</strong> What are the assumptions made in this model? Are they realistic?</p>
</section>
<section id="id2">
<h2><span class="section-number">18.4. </span>Kidney Cancer Rates<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>The following is a visualization of the counties with the highest rates of kidney cancer (Gelman 1998). Is there any noticeable spatial pattern in these maps? Recall that the MLE of the rate of the Poisson distribution for each county is <span class="math notranslate nohighlight">\(\frac{C_n}{T_n}\)</span>.</p>
<img src="fig/kidney.jpg" style="height:350px;"></section>
<section id="birth-weights">
<h2><span class="section-number">18.5. </span>Birth Weights<a class="headerlink" href="#birth-weights" title="Permalink to this heading">#</a></h2>
<p>In our lab, we work with IVF clinics to build models for prediction and data analysis. One of our current tasks is to model the birth weights of the infants born in the clinic.</p>
<p>Naively, given observed birth weights <span class="math notranslate nohighlight">\(Y_1, \ldots, Y_N\)</span>, we can model each outcome <span class="math notranslate nohighlight">\(Y_n\)</span> with a normal distribution,</p>
<div class="math notranslate nohighlight">
\[
Y_n \sim \mathcal{N}(\mu, \sigma^2)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the average birth weight for this population and <span class="math notranslate nohighlight">\(\sigma^2\)</span> is the population variance.</p>
<p><strong>Model Critique:</strong> What are the assumptions made in this model? Are they realistic?</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="the-beta-binomial-model">
<h1><span class="section-number">19. </span>The Beta-Binomial Model<a class="headerlink" href="#the-beta-binomial-model" title="Permalink to this heading">#</a></h1>
<section id="the-coin-toss-model-revisited">
<h2><span class="section-number">19.1. </span>The Coin Toss Model: Revisited<a class="headerlink" href="#the-coin-toss-model-revisited" title="Permalink to this heading">#</a></h2>
<p>Suppose that we have three observations from a Bernoulli distribution, <span class="math notranslate nohighlight">\(Ber(\theta)\)</span>: <span class="math notranslate nohighlight">\(\{ H, H, H\}\)</span>. The MLE of <span class="math notranslate nohighlight">\(\theta\)</span> is
$<span class="math notranslate nohighlight">\(
\theta_{\text{MLE}} = \frac{3}{3} = 1.
\)</span>$
This is a clear case of the MLE overfitting to the observed data.</p>
<p>Last time, you’d suggested tying the estimate to some fixed reasonable number, for example,
$<span class="math notranslate nohighlight">\(
\theta_{\text{MLE Reg}} = \frac{H + 1}{N + 2},
\)</span><span class="math notranslate nohighlight">\(
and in general,
\)</span><span class="math notranslate nohighlight">\(
\theta_{\text{MLE Reg}} = \frac{H + \alpha}{N + \beta}.
\)</span><span class="math notranslate nohighlight">\(
The terms \)</span>\alpha<span class="math notranslate nohighlight">\( and \)</span>\beta$ are called <em><strong>regularization terms</strong></em> .</p>
<p><strong>Question:</strong> What is the effect of <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span> on our estimate? What values should we choose for <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>?</p>
</section>
<section id="incoporating-prior-beliefs">
<h2><span class="section-number">19.2. </span>Incoporating Prior Beliefs<a class="headerlink" href="#incoporating-prior-beliefs" title="Permalink to this heading">#</a></h2>
<p>Our choise of the regularization terms, <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>, depends on our prior beliefs about the coin. The way we chose to incorporate these beliefs doesn’t indicate any uncertainty.</p>
<p>Alternatively, we can incorporate our prior belief about <span class="math notranslate nohighlight">\(\theta\)</span> as a distribution, this is called the <em><strong>prior distribution</strong></em>. Since <span class="math notranslate nohighlight">\(\theta\)</span> is a number between 0 and 1, a beta distribution is an appropriate choice.</p>
<img src="fig/beta.jpg" style="height:350px;"></section>
<section id="id3">
<h2><span class="section-number">19.3. </span>The Beta-Binomial Model<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h2>
<p>A model that involves both a likelihood for the data and prior on the parameters in the likelihood is called a <em><strong>Bayesian model</strong></em>.</p>
<p><strong>Bayesian Model for Coin Flip</strong></p>
<p>\begin{aligned}
Y &amp;\sim Bin(N, \theta),\quad &amp;\text{(Likelihood)}\
\theta &amp;\sim Beta(\alpha, \beta),\quad &amp;\text{(Prior)}
\end{aligned}</p>
<p>where <span class="math notranslate nohighlight">\(\alpha, \beta\)</span> are called <em><strong>hyperparameters</strong></em> of the model.</p>
<p>Now, computing the MLE no longer makes sense (since the MLE only considers the likelihood). Luckily, Bayes’ Rule allows us to derive a distribution that considers both the prior and the likelihood:</p>
<p>\begin{aligned}
p(\theta | Y) = \frac{\overbrace{p(Y| \theta)}^{\text{likelihood}} \overbrace{p(\theta)}^{\text{prior}}}{\underbrace{p(Y)}_{\text{marginal data likelihood}}} = \frac{p(Y, \theta)}{\int p(Y, \theta) d\theta}
\end{aligned}</p>
<p>The distribution <span class="math notranslate nohighlight">\(p(\theta | Y)\)</span> is called the <em><strong>posterior</strong></em>.</p>
</section>
<section id="posterior-for-the-beta-binomial-model">
<h2><span class="section-number">19.4. </span>Posterior for the Beta-Binomial Model<a class="headerlink" href="#posterior-for-the-beta-binomial-model" title="Permalink to this heading">#</a></h2>
<p>In our case, the posterior is given by
\begin{aligned}
p(\theta | Y) = \frac{p(Y| \theta)p(\theta)}{p(Y)} = \frac{\overbrace^{\text{binomial pdf}} \overbrace{\frac{1}{B(\alpha, \beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}}^{\text{beta pdf}}}{p(Y)}
\end{aligned}
We can rewrite the posterior as</p>
<p>\begin{aligned}
p(\theta | Y) = const * \theta^{(Y+\alpha) - 1}(1 - \theta)^{(N-Y + \beta) - 1}
\end{aligned}</p>
<p>where <span class="math notranslate nohighlight">\(const = \frac{{N \choose Y}}{B(\alpha, \beta)p(Y)}\)</span> must be the normalizing constant for <span class="math notranslate nohighlight">\(p(\theta | Y)\)</span>.</p>
<p>We recognize the posterior as a beta distribution, <span class="math notranslate nohighlight">\(Beta(Y+\alpha, N-Y + \beta)\)</span>! Can you see why?</p>
</section>
<section id="interpreting-the-posterior-bayesian-update">
<h2><span class="section-number">19.5. </span>Interpreting the Posterior: Bayesian Update<a class="headerlink" href="#interpreting-the-posterior-bayesian-update" title="Permalink to this heading">#</a></h2>
<p>Rather than a point estimate for <span class="math notranslate nohighlight">\(\theta\)</span>, we now have a posterior distribution, <span class="math notranslate nohighlight">\(p(\theta|Y)\)</span>, over <span class="math notranslate nohighlight">\(\theta\)</span>. What does the posterior tell us about <span class="math notranslate nohighlight">\(\theta\)</span>?</p>
<p>Since the prior distribution <span class="math notranslate nohighlight">\(p(\theta)\)</span> encoded our beliefs about <span class="math notranslate nohighlight">\(\theta\)</span> along with our uncertainty, it is natural to interpret the posterior as yet another <strong>belief</strong> about <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Since the posterior includes the likelihood, this belief has been <strong>updated by the data</strong>.</p>
</section>
<section id="simulation-bayesian-update-for-the-coin-flip">
<h2><span class="section-number">19.6. </span>Simulation: Bayesian Update for the Coin Flip<a class="headerlink" href="#simulation-bayesian-update-for-the-coin-flip" title="Permalink to this heading">#</a></h2>
<p>What is the effect of the choice of the prior on the posterior? What is the effect of the number of observations, <span class="math notranslate nohighlight">\(N\)</span>, on the posterior?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#a function for computing the prior and the posterior (after simulating data)</span>
<span class="k">def</span> <span class="nf">make_models</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="c1">#prior definition: beta distribution</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span>
    
    <span class="c1">#sample data</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
    
    <span class="c1">#update posterior: beta distribution</span>
    <span class="n">posterior_alpha</span> <span class="o">=</span> <span class="n">H</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="n">posterior_beta</span> <span class="o">=</span> <span class="n">N</span> <span class="o">-</span> <span class="n">H</span> <span class="o">+</span> <span class="n">beta</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">posterior_alpha</span><span class="p">,</span> <span class="n">posterior_beta</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span>
    
    <span class="k">return</span> <span class="n">prior</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">posterior_alpha</span><span class="p">,</span> <span class="n">posterior_beta</span>

<span class="c1"># a function for ploting the prior and posterior distribution</span>
<span class="k">def</span> <span class="nf">plot_prior_posterior</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Prior distribution&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Posterior distribution after seeing </span><span class="si">{}</span><span class="s1"> Heads/</span><span class="si">{}</span><span class="s1"> Flips&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ax</span>

<span class="c1">#prior definition: beta distribution</span>
<span class="c1">#try: alpha = beta = 0.5, 1, 10; try: alpha=10, beta=1; try: alpha=1, beta=10</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="c1">#data: binomial distribution</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">0.3</span>

<span class="n">prior</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">posterior_alpha</span><span class="p">,</span> <span class="n">posterior_beta</span> <span class="o">=</span> <span class="n">make_models</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_prior_posterior</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0871200352d82071cc190e9eae9b42516198241da96b7351632c870f3d01b61e.png" src="../_images/0871200352d82071cc190e9eae9b42516198241da96b7351632c870f3d01b61e.png" />
</div>
</div>
</section>
<section id="simulation-iterated-bayesian-update-for-the-coin-flip">
<h2><span class="section-number">19.7. </span>Simulation: Iterated Bayesian Update for the Coin Flip<a class="headerlink" href="#simulation-iterated-bayesian-update-for-the-coin-flip" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#run this multiple times to see the effect of updates</span>
<span class="n">posterior_alpha</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">posterior_beta</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">prior</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">posterior_alpha</span><span class="p">,</span> <span class="n">posterior_beta</span> <span class="o">=</span> <span class="n">make_models</span><span class="p">(</span><span class="n">posterior_alpha</span><span class="p">,</span> <span class="n">posterior_beta</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_prior_posterior</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/340fda816e1e11aa79559b4756b6b8af1f82741e1cd7b34819568733a0543213.png" src="../_images/340fda816e1e11aa79559b4756b6b8af1f82741e1cd7b34819568733a0543213.png" />
</div>
</div>
</section>
<section id="making-predictions">
<h2><span class="section-number">19.8. </span>Making Predictions<a class="headerlink" href="#making-predictions" title="Permalink to this heading">#</a></h2>
<p>If the posteriors we infer represent beliefs, how do we evaluate these beliefs?</p>
<ol class="arabic simple">
<li><p>In the case that we the true parameter <span class="math notranslate nohighlight">\(\theta^{\text{true}}\)</span>, we can check to see if the posterior assigns high likelihood to <span class="math notranslate nohighlight">\(\theta^{\text{true}}\)</span>, and the certainty the posterior has around <span class="math notranslate nohighlight">\(\theta^{\text{true}}\)</span>.<br><br></p></li>
<li><p>When we do not know <span class="math notranslate nohighlight">\(\theta^{\text{true}}\)</span>, we can simulate data <span class="math notranslate nohighlight">\(Y^{\theta}\)</span> using samples of <span class="math notranslate nohighlight">\(\theta\)</span> from the posterior. We compare the distribution of simulated data, or <em><strong>posterior predictive</strong></em>, to the observed data.</p></li>
</ol>
</section>
<section id="simulation-posterior-predictive-for-the-coin-flip">
<h2><span class="section-number">19.9. </span>Simulation: Posterior Predictive for the Coin Flip<a class="headerlink" href="#simulation-posterior-predictive-for-the-coin-flip" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># function to plot the posterior predictive of the beta-binomial model</span>
<span class="k">def</span> <span class="nf">plot_posterior_predictive</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">posterior_pred</span><span class="p">,</span> <span class="n">H</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">H</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;observed heads&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">posterior_pred</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;histogram of predicted head&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">posterior_pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;posterior predictive over heads&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">posterior_alpha</span><span class="p">,</span> <span class="n">posterior_beta</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">posterior_pred</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">theta_sample</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">theta_sample</span> <span class="ow">in</span> <span class="n">posterior_samples</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_posterior_predictive</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">posterior_pred</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e3e4f7a5d10dc2cb9e7692139b65d9d59439f4ffe9b19a43ac26389ce0291b78.png" src="../_images/e3e4f7a5d10dc2cb9e7692139b65d9d59439f4ffe9b19a43ac26389ce0291b78.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="bayesian-modeling-a-summary">
<h1><span class="section-number">20. </span>Bayesian Modeling - A Summary<a class="headerlink" href="#bayesian-modeling-a-summary" title="Permalink to this heading">#</a></h1>
<section id="the-bayesian-modeling-process">
<h2><span class="section-number">20.1. </span>The Bayesian Modeling Process<a class="headerlink" href="#the-bayesian-modeling-process" title="Permalink to this heading">#</a></h2>
<p>In order to make statements about <span class="math notranslate nohighlight">\(Y\)</span>, the outcome, and <span class="math notranslate nohighlight">\(\theta\)</span>, parameters of the distribution generating the data, we form the joint distribution over both variables and use the various marginals/conditional distributions to reason about <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<ol class="arabic simple">
<li><p>we form the <em><strong>joint distribution</strong></em> over both variables <span class="math notranslate nohighlight">\(p(Y, \theta) = p(Y | \theta) p(\theta)\)</span>.</p></li>
<li><p>we can condition on the observed outcome to make inferences about <span class="math notranslate nohighlight">\(\theta\)</span>,
$<span class="math notranslate nohighlight">\(
p(\theta | Y) = \frac{p(Y, \theta)}{p(Y)}
\)</span><span class="math notranslate nohighlight">\(
where \)</span>p(\theta | Y)<span class="math notranslate nohighlight">\( is called the ***posterior distribution*** and \)</span>p(Y)$ is called the <em><strong>evidence</strong></em>.</p></li>
<li><p>before any data is observed, we can simulate data by using our prior
$<span class="math notranslate nohighlight">\(
p(Y^*) = \int_\Theta p(Y^*, \theta) d\theta = \int_\Theta p(Y^* | \theta) p(\theta) d\theta
\)</span><span class="math notranslate nohighlight">\(
where \)</span>Y^<em><span class="math notranslate nohighlight">\( represents new data and \)</span>p(Y^</em>)$ is called the <em><strong>prior predictive</strong></em>.</p></li>
<li><p>after observing data, we can simulate new data simliar to the observed data by using our posterior
$<span class="math notranslate nohighlight">\(
p(Y^*|Y) = \int_\Theta p(Y^*, \theta|Y) d\theta = \int_\Theta p(Y^* | \theta) p(\theta | Y) d\theta
\)</span><span class="math notranslate nohighlight">\(
where \)</span>Y^<em><span class="math notranslate nohighlight">\( represents new data and \)</span>p(Y^</em>|Y)$ is called the <em><strong>posterior predictive</strong></em>.</p></li>
</ol>
</section>
<section id="evaluating-bayesian-models">
<h2><span class="section-number">20.2. </span>Evaluating Bayesian Models<a class="headerlink" href="#evaluating-bayesian-models" title="Permalink to this heading">#</a></h2>
<p>As we have seen in the Beta-Binomial model, we can simulate the posterior (and prior) predictive rather than compute them analytically. That is, you don’t need to know the pdf of <span class="math notranslate nohighlight">\(p(Y^*|Y)\)</span>.</p>
<p>The posterior predictive can be represented by <strong>samples</strong> of predictions:</p>
<ol class="arabic simple">
<li><p>we sample values of <span class="math notranslate nohighlight">\(\theta_n\)</span> from the posterior, <span class="math notranslate nohighlight">\(p(\theta|Y)\)</span>.</p></li>
<li><p>we sample an outcome <span class="math notranslate nohighlight">\(Y_n\)</span> from <span class="math notranslate nohighlight">\(p(Y|\theta_n)\)</span> for each posterior sample <span class="math notranslate nohighlight">\(\theta_n\)</span>.</p></li>
</ol>
<p>The set <span class="math notranslate nohighlight">\(Y_n\)</span> we obtain emprirically represents the posterior predictve distribution <span class="math notranslate nohighlight">\(p(Y^*|Y)\)</span>.</p>
<p>How do we evaluate the quality of our Bayesian model: how appropriate was our choice of prior? How appropriate was our choice of noise structure for the data (i.e. how appropriate is our choice of likelihood)?</p>
<p>Evaluating Bayesian models is challenging for two reasons:</p>
<ol class="arabic simple">
<li><p>Instead of learning a single best estimate of the model parameters <span class="math notranslate nohighlight">\(\theta\)</span>, like in the case of maximum likelihood estimation, we learn a distribution over possible values of <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(p(\theta|Y)\)</span></p></li>
<li><p>In real life we do not have access to the ground truth values of model parameters</p></li>
</ol>
<p>Thus, we evaluate our model by seeing how well it fits the observed data. That is, we check to see how well our predicted distribution of <span class="math notranslate nohighlight">\(Y^*\)</span>, <span class="math notranslate nohighlight">\(p(Y^* | Y)\)</span>, matches up with the observed data <span class="math notranslate nohighlight">\(Y^*\)</span>.</p>
<p>When we are able to visualize the data (i.e. when <span class="math notranslate nohighlight">\(Y^*\)</span> is low-dimensional), we can visualize both data <span class="math notranslate nohighlight">\(Y^*\)</span> and samples from  <span class="math notranslate nohighlight">\(p(Y^* | Y)\)</span>. If the model is a good fit for the data, these two plots should be similar.</p>
<p>But what if it’s not possible to visualize the data?</p>
</section>
<section id="where-do-priors-come-from">
<h2><span class="section-number">20.3. </span>Where do Priors Come From?<a class="headerlink" href="#where-do-priors-come-from" title="Permalink to this heading">#</a></h2>
<p>Hopefully you’ve noticed a key property of the priors we chose:</p>
<blockquote>
<div><p>All the priors combined with the likelihood to form a distribution we recognize! Specifically, the posterior distribution is of the same type as the prior!</p>
</div></blockquote>
<p>These priors are called <em><strong>conjugate priors</strong></em> for the corresponding likelihoods. This is a purely mathematical property.</p>
<p><strong>Question:</strong> is it right to choose priors that are mathematically convenient? What is a good way to choose a prior? What if we “choose wrong”?</p>
</section>
<section id="uninformative-priors-how-to-say-i-don-t-know">
<h2><span class="section-number">20.4. </span>Uninformative Priors: How to Say I Don’t Know<a class="headerlink" href="#uninformative-priors-how-to-say-i-don-t-know" title="Permalink to this heading">#</a></h2>
<p>If you don’t have a strong prior believe about the parameters in the likelihood, you should choose a prior that has no effect when combined with the likelihood – i.e. <strong>let the data speak for itself</strong>.</p>
<p><strong>Example:</strong>
In the Beta-Binomial model, is there a choice of the hyperparameters of the prior that has no effect on the posterior?</p>
<p>We might try <span class="math notranslate nohighlight">\(Beta(1, 1)\)</span>, which is the uniform distribution over <span class="math notranslate nohighlight">\([0, 1]\)</span>. The posterior <span class="math notranslate nohighlight">\(p(\theta|Y)\)</span> is then <span class="math notranslate nohighlight">\(Beta(Y + 1, N - Y + 1)\)</span>. In the following example, visualizing the posterior, we see that the posterior assigns the highest mass to the MLE of <span class="math notranslate nohighlight">\(\theta\)</span>. But the posterior assigns more mass to values of <span class="math notranslate nohighlight">\(\theta\)</span> smaller than the MLE. In this sense, the prior <strong>does</strong> inform the posterior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">MLE</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">*</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">N</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">Y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span> <span class="o">-</span> <span class="n">Y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span>
<span class="n">posterior_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mf">2.</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior pdf&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">MLE</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Maximum Likelihood Estimate&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">posterior_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior Mean&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Comparison of the posterior with the MLE&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c574cf2e196563f46941620d9bdd7e3753d03641e38168debdf0d488af718d51.png" src="../_images/c574cf2e196563f46941620d9bdd7e3753d03641e38168debdf0d488af718d51.png" />
</div>
</div>
<p>There is a general purpose technique for creating non-informative priors, called <em><strong>Jeffreys priors</strong></em>, which places less prior weight on parameter values where the likelihood function is flat. This prevents the prior from having undue influency on the posterior.</p>
<p>There are many resources that cover the construction of Jeffreys priors for common likelihoods.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture_2_notes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Lecture #2: Maximimum Likelihood Estimation</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture_4_notes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">21. </span>Lecture #4: Bayesian versus Frequentist Inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">14. Lecture #3: Bayesian Modeling</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">15. AM 207: Advanced Scientific Computing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">15.1. Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">15.2. Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">16. Outline</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-the-method-of-maximum-likelihood">17. Review of the Method of Maximum Likelihood</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-method-of-maximum-likelihood">17.1. The Method of Maximum Likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-mle">17.2. Evaluating the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-the-maximum-likelihood-estimator">17.3. Properties of The Maximum Likelihood Estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-bias-always-bad">17.4. Is Bias Always Bad?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decomposition-of-mse">Decomposition of MSE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-the-bias-variance-trade-off">Example of the Bias Variance Trade-off</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-mle-overfitting-under-scarcity-of-data">17.5. Limitations of MLE: Overfitting Under Scarcity of Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">17.6. Limitations of MLE: Overfitting Under Scarcity of Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">17.7. Regularization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#models-for-real-data">18. Models for Real Data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#die-roll">18.1. Die Roll</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#video-ranking">18.2. Video Ranking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kidney-cancer-rates">18.3. Kidney Cancer Rates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">18.4. Kidney Cancer Rates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#birth-weights">18.5. Birth Weights</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-beta-binomial-model">19. The Beta-Binomial Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coin-toss-model-revisited">19.1. The Coin Toss Model: Revisited</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#incoporating-prior-beliefs">19.2. Incoporating Prior Beliefs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">19.3. The Beta-Binomial Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-for-the-beta-binomial-model">19.4. Posterior for the Beta-Binomial Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-posterior-bayesian-update">19.5. Interpreting the Posterior: Bayesian Update</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-bayesian-update-for-the-coin-flip">19.6. Simulation: Bayesian Update for the Coin Flip</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-iterated-bayesian-update-for-the-coin-flip">19.7. Simulation: Iterated Bayesian Update for the Coin Flip</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions">19.8. Making Predictions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-posterior-predictive-for-the-coin-flip">19.9. Simulation: Posterior Predictive for the Coin Flip</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-modeling-a-summary">20. Bayesian Modeling - A Summary</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayesian-modeling-process">20.1. The Bayesian Modeling Process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-bayesian-models">20.2. Evaluating Bayesian Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-do-priors-come-from">20.3. Where do Priors Come From?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uninformative-priors-how-to-say-i-don-t-know">20.4. Uninformative Priors: How to Say I Don’t Know</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yaniv Yacoby
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>