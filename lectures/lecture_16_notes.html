

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>25. Lecture #16: Neural Network Models for Regression &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/lecture_16_notes';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/lectures/lecture_16_notes.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="31. Lecture #17: Black-box Variational Inference" href="lecture_17_notes.html" />
    <link rel="prev" title="21. Lecture #15: Parallel Tempering and Stochastic HMC" href="lecture_15_notes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Probabilistic Foundations of Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Exact Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_1_notes.html">1. Lecture #1: Course Overview</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_2_notes.html">7. Lecture #2: Maximimum Likelihood Estimation</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_3_notes.html">14. Lecture #3: Bayesian Modeling</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_4_notes.html">21. Lecture #4: Bayesian versus Frequentist Inference</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Sampling-Based Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_5_notes.html">1. Lecture #5: Sampling for Posterior Simulation</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_6_notes.html">7. Lecture #6: Monte Carlo Integration</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_7_notes.html">14. Lecture #7: Markov Chain Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_8_notes.html">18. Lecture #8: Metropolis-Hastings and Gibbs</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_9_notes.html">23. Lecture #9: Latent Variable Models and MLE</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Gradient-Based Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_10_notes.html">1. Lecture #10: Bayesian Latent Variable Models and Variational Inference</a></li>

<li class="toctree-l1"><a class="reference internal" href="lecture_11_notes.html">3. Lecture #11: Hierarchical Models</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_12_notes.html">7. Lecture #12: Logistic Regression and Gradient Descent</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_13_notes.html">11. Lecture #13: Stochastic Gradient Descent and Simulated Annealing</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_14_notes.html">17. Lecture #14: Hamiltonian Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_15_notes.html">21. Lecture #15: Parallel Tempering and Stochastic HMC</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">25. Lecture #16: Neural Network Models for Regression</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_17_notes.html">31. Lecture #17: Black-box Variational Inference</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_18_notes.html">35. Lecture #18: Automatic Differentiation</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_19_notes.html">38. Lecture #19: Variational Inference in Context</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_20_notes.html">43. Lecture #20: Variational Autoencoders</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_21_notes.html">46. Lecture #21: Implementation of Variational Autoencoders</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/mogu-lab/cs349-fall-2024/blob/master/lectures/lecture_16_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li><a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fmogu-lab%2Fcs349-fall-2024%2Fblob%2Fmaster%2Flectures/lecture_16_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onDeepnote"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_deepnote.svg">
  </span>
<span class="btn__text-container">Deepnote</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/lecture_16_notes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture #16: Neural Network Models for Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">25. Lecture #16: Neural Network Models for Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-as-generalized-linear-models">26. Regression as Generalized Linear Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-models">Linear Regression Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-would-you-parameterize-a-non-linear-trend">How Would You Parameterize a Non-linear Trend?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-the-geometry-of-logistic-regression">Review of the Geometry of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-would-you-parametrize-a-ellipitical-decision-boundary">How would you parametrize a ellipitical decision boundary?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-would-you-parametrize-an-arbitrary-complex-decision-boundary">How would you parametrize an arbitrary complex decision boundary?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">27. Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-arbitrarily-complex-decision-boundaries">Approximating Arbitrarily Complex Decision Boundaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-neural-network">What is a Neural Network?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-as-function-approximators">Neural Networks as Function Approximators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-flexible-framework-for-function-approximation">A Flexible Framework for Function Approximation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-choices-for-the-activation-function">Common Choices for the Activation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-are-universal-function-approximators">Neural Networks are Universal Function Approximators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-regression">Neural Networks Regression</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-differentiation-and-backpropagation">28. Automatic Differentiation and Backpropagation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-computation-for-neural-networks">Gradient Computation for Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-computing-neural-network-gradients">Example: Computing Neural Network Gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-gradient-descent-for-neural-networks">Backpropagation: Gradient Descent for Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-computation-with-automatic-differentiation">Gradient Computation with Automatic Differentiation</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-a-neural-network-learn">29. What Does a Neural Network Learn?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-a-neural-network-classifier-so-effective">Why is a Neural Network Classifier So Effective?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Why is a Neural Network Classifier So Effective?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-interpretations-of-a-neural-network-classifier">Two Interpretations of a Neural Network Classifier:</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#with-great-flexibility-comes-with-great-problems">30. With Great Flexibility Comes with Great Problems</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-regression-vs-linear-regression">Neural Network Regression vs Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-deep-learning">Interpretable Deep Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-machine-learning-models-make-use-of-human-concepts">Can Machine Learning Models Make Use of Human Concepts?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-machine-learning-models-learn-to-explore-hypothetical-scenarios">Can Machine Learning Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#right-for-the-right-reasons">Right for the Right Reasons?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perils-of-explanations">The Perils of Explanations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-error-and-bias-variance">Generalization Error and Bias/Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-of-deep-models">Generalization of Deep Models</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-16-neural-network-models-for-regression">
<h1><span class="section-number">25. </span>Lecture #16: Neural Network Models for Regression<a class="headerlink" href="#lecture-16-neural-network-models-for-regression" title="Permalink to this heading">#</a></h1>
<section id="am-207-advanced-scientific-computing">
<h2>AM 207: Advanced Scientific Computing<a class="headerlink" href="#am-207-advanced-scientific-computing" title="Permalink to this heading">#</a></h2>
<section id="stochastic-methods-for-data-analysis-inference-and-optimization">
<h3>Stochastic Methods for Data Analysis, Inference and Optimization<a class="headerlink" href="#stochastic-methods-for-data-analysis-inference-and-optimization" title="Permalink to this heading">#</a></h3>
</section>
<section id="fall-2021">
<h3>Fall, 2021<a class="headerlink" href="#fall-2021" title="Permalink to this heading">#</a></h3>
<img src="fig/logos.jpg" style="height:150px;"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Import basic libraries</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.animation</span> <span class="k">as</span> <span class="nn">animation</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">rc</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1">### Import basic libraries</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">numpy</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;autograd&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="outline">
<h2>Outline<a class="headerlink" href="#outline" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Regression as Generalized Linear Models</p></li>
<li><p>Neural Networks</p></li>
<li><p>Automatic Differentiation and BackPropagation</p></li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="regression-as-generalized-linear-models">
<h1><span class="section-number">26. </span>Regression as Generalized Linear Models<a class="headerlink" href="#regression-as-generalized-linear-models" title="Permalink to this heading">#</a></h1>
<section id="linear-regression-models">
<h2>Linear Regression Models<a class="headerlink" href="#linear-regression-models" title="Permalink to this heading">#</a></h2>
<p>Here is a generalized linear model (GLM) you’ve known since day one!
\begin{align}
\mu &amp;= \mathbf{w}^\top \mathbf{X}^{(n)}\
Y^{(n)}&amp;\sim \mathcal{N}(\mu, \sigma^2)
\end{align}
Alternatively, we can write this model as</p>
<p>\begin{align}
Y^{(n)} = \mathbf{w}^\top \mathbf{X}^{(n)} + \epsilon; \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
\end{align}</p>
<p>That is, injecting covariates into a normal likelihood is precisely linear regression! Just like in the case of logistic regression, we can form scientific hypotheses by examining the parameters of a linear regression model:</p>
<p>\begin{align}
\widehat{\text{income}} = 2 * \text{education (yr)} + 3.1 * \text{married} - 1.5 * \text{gaps in work history}
\end{align}</p>
</section>
<section id="how-would-you-parameterize-a-non-linear-trend">
<h2>How Would You Parameterize a Non-linear Trend?<a class="headerlink" href="#how-would-you-parameterize-a-non-linear-trend" title="Permalink to this heading">#</a></h2>
<img src="./fig/fig12.png" style='height:400px;'>
It's not easy to think of a function $g(x)$ can capture the trend in the data, e.g. what degree of polynomial should we use?</section>
<section id="review-of-the-geometry-of-logistic-regression">
<h2>Review of the Geometry of Logistic Regression<a class="headerlink" href="#review-of-the-geometry-of-logistic-regression" title="Permalink to this heading">#</a></h2>
<p>In <strong>logistic regression</strong>, we model the probability of an input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> being labeled ‘1’ as a function of its distance from the hyperplane parametrized by <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>
<img src="./fig/fig0.png" style='height:300px;'>
That is, we model <span class="math notranslate nohighlight">\(p(y=1 | \mathbf{w}, \mathbf{x}) = \sigma(\mathbf{w}^\top \mathbf{x})\)</span>. Where <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}=0\)</span> is the equation of the decision boundary.</p>
</section>
<section id="how-would-you-parametrize-a-ellipitical-decision-boundary">
<h2>How would you parametrize a ellipitical decision boundary?<a class="headerlink" href="#how-would-you-parametrize-a-ellipitical-decision-boundary" title="Permalink to this heading">#</a></h2>
<img src="./fig/fig1.png" style='height:300px;'>
<p>We can say that the decision boundary is given by a <em><strong>quadratic function</strong></em> of the input:
$<span class="math notranslate nohighlight">\(
w_1x^2_1 + w_2x^2_2 + w_3 = 0
\)</span>$
We say that we can fit such a decision boundary using logistic regression with degree 2 polynomial features</p>
</section>
<section id="how-would-you-parametrize-an-arbitrary-complex-decision-boundary">
<h2>How would you parametrize an arbitrary complex decision boundary?<a class="headerlink" href="#how-would-you-parametrize-an-arbitrary-complex-decision-boundary" title="Permalink to this heading">#</a></h2>
<img src="./fig/fig2.png" style='height:300px;'>
<p>It’s not easy to think of a function <span class="math notranslate nohighlight">\(g(x)\)</span> can capture this decision boundary.</p>
<p><strong>GOAL:</strong> Find models that can capture <em>arbitrarily complex</em> functions.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="neural-networks">
<h1><span class="section-number">27. </span>Neural Networks<a class="headerlink" href="#neural-networks" title="Permalink to this heading">#</a></h1>
<section id="approximating-arbitrarily-complex-decision-boundaries">
<h2>Approximating Arbitrarily Complex Decision Boundaries<a class="headerlink" href="#approximating-arbitrarily-complex-decision-boundaries" title="Permalink to this heading">#</a></h2>
<p>Given an exact parametrization, we could learn the functional form, <span class="math notranslate nohighlight">\(g\)</span>, of the decision boundary directly.</p>
<p>However, assuming an exact form for <span class="math notranslate nohighlight">\(g\)</span> is restrictive.</p>
<p>Rather, we can build increasingly good approximations, <span class="math notranslate nohighlight">\(\widehat{g}\)</span>, of <span class="math notranslate nohighlight">\(g\)</span> by composing simple functions.</p>
</section>
<section id="what-is-a-neural-network">
<h2>What is a Neural Network?<a class="headerlink" href="#what-is-a-neural-network" title="Permalink to this heading">#</a></h2>
<p><strong>Goal:</strong> build a good approximation <span class="math notranslate nohighlight">\(\widehat{g}\)</span> of a complex function <span class="math notranslate nohighlight">\(g\)</span> by composing simple functions.</p>
<p>For example, let the following picture represents <span class="math notranslate nohighlight">\(a = f\left(\sum_{i}w_ix_i\right)\)</span>, where <span class="math notranslate nohighlight">\(f\)</span> is a non-linear transform, and we denote the intermediate value <span class="math notranslate nohighlight">\(\sum_{i}w_ix_i\)</span> by <span class="math notranslate nohighlight">\(s\)</span>.</p>
<img src="./fig/fig4.png" style='height:300px;'>
<p><strong>Note:</strong> we always assume that <span class="math notranslate nohighlight">\(x_0=1\)</span> and hence <span class="math notranslate nohighlight">\(w_0\)</span> is the intercept or <em><strong>bias</strong></em> of the linear expression <span class="math notranslate nohighlight">\(\sum_i w_i x_i\)</span>.</p>
</section>
<section id="neural-networks-as-function-approximators">
<h2>Neural Networks as Function Approximators<a class="headerlink" href="#neural-networks-as-function-approximators" title="Permalink to this heading">#</a></h2>
<p>Then we can define the approximation <span class="math notranslate nohighlight">\(\widehat{g}\)</span> with a graphical schema representing a complex series of compositions and sums of the form, <span class="math notranslate nohighlight">\(f\left(\sum_{i}w_ix_i\right)\)</span></p>
<img src="./fig/fig5.png" style='height:300px;'>
<p>This is a <em><strong>neural network</strong></em>. We denote the weights of the neural network collectively by <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.
The non-linear function <span class="math notranslate nohighlight">\(f\)</span> is called the <em><strong>activation function</strong></em>.</p>
<p><strong>Note:</strong>
Typically, at each node, we want to take a weighted sum of the values of the previous nodes with a additional <em><strong>bias term</strong></em>. That is, we want as input <span class="math notranslate nohighlight">\(\sum_i w^l_{ij} \text{node}^l_i + \text{bias}_j\)</span> for the <span class="math notranslate nohighlight">\(j\)</span>-th node in the <span class="math notranslate nohighlight">\(l\)</span>-th hidden layer. This is often done by adding an extra node per layer with the value of 1:
<img src="./fig/bias.png" style='height:250px;'>
The bias terms are considered to be part of the network parameters and when we jointly denote the network parameters by <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>, bias terms are included.</p>
</section>
<section id="a-flexible-framework-for-function-approximation">
<h2>A Flexible Framework for Function Approximation<a class="headerlink" href="#a-flexible-framework-for-function-approximation" title="Permalink to this heading">#</a></h2>
<img src="./fig/fig6.png" style='height:500px;'></section>
<section id="common-choices-for-the-activation-function">
<h2>Common Choices for the Activation Function<a class="headerlink" href="#common-choices-for-the-activation-function" title="Permalink to this heading">#</a></h2>
<img src="./fig/fig8.png" style='height:500px;'></section>
<section id="neural-networks-are-universal-function-approximators">
<h2>Neural Networks are Universal Function Approximators<a class="headerlink" href="#neural-networks-are-universal-function-approximators" title="Permalink to this heading">#</a></h2>
<p>So what kind of functions can be approximated by neural networks?</p>
<p><strong>Theorem: (Hornik, Stinchombe, White, 1989)</strong> Fix a “nice” activation function <span class="math notranslate nohighlight">\(f\)</span>. For any continuous function <span class="math notranslate nohighlight">\(g\)</span> on a compact set <span class="math notranslate nohighlight">\(K\)</span>, there exists a feedforward neural network with activation <span class="math notranslate nohighlight">\(f\)</span>, having only a single hidden layer, which approximates <span class="math notranslate nohighlight">\(g\)</span> to within an arbitrary degree of precision on <span class="math notranslate nohighlight">\(K\)</span>.</p>
<p>For this reason, we call neural networks <em><strong>universal function approximators</strong></em>.</p>
</section>
<section id="neural-networks-regression">
<h2>Neural Networks Regression<a class="headerlink" href="#neural-networks-regression" title="Permalink to this heading">#</a></h2>
<p><strong>Model for Regression:</strong> <span class="math notranslate nohighlight">\(Y^{(n)}\sim \mathcal{N}(\mu, \sigma^2)\)</span>, <span class="math notranslate nohighlight">\(\mu = g_\mathbf{W}(\mathbf{X}^{(n)})\)</span>, where <span class="math notranslate nohighlight">\(g_\mathbf{W}\)</span> is a neural network with parameters <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.</p>
<p><strong>Training Objective:</strong> find <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> to maximize the likelihood of our data. This is equivalent to minimizing the Mean Square Error,
$<span class="math notranslate nohighlight">\(
\max_{\mathbf{W}}\, \mathrm{MSE}(\mathbf{W}) = \frac{1}{N}\sum^N_{n=1} \left(y_n - g_\mathbf{W}(x_n)\right)^2
\)</span>$</p>
<p><strong>Optimizing the Training Objective:</strong> For linear regression (when <span class="math notranslate nohighlight">\(g_\mathbf{W}\)</span> is a linear function), we computed the gradient of the MSE with respective to the model parameters <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>, set it equal to zero and solved for the optimal <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> analytically (see Homework #0). For logistic regression, we computed the gradient and used (stochastic) gradient descent to “solve for where the gradient is zero”.</p>
<p>Can we do the same when <span class="math notranslate nohighlight">\(g_\mathbf{W}\)</span> is a neural network?</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="automatic-differentiation-and-backpropagation">
<h1><span class="section-number">28. </span>Automatic Differentiation and Backpropagation<a class="headerlink" href="#automatic-differentiation-and-backpropagation" title="Permalink to this heading">#</a></h1>
<section id="gradient-computation-for-neural-networks">
<h2>Gradient Computation for Neural Networks<a class="headerlink" href="#gradient-computation-for-neural-networks" title="Permalink to this heading">#</a></h2>
<p>Computing the gradient for any parameter <span class="math notranslate nohighlight">\(w^l_{ij}\)</span> in the following network requires us to use the <em><strong>chain rule</strong></em>:</p>
<p>\begin{align}
\frac{\partial}{\partial t} g(h(t)) = g’(h(t))h’(t),\quad&amp; \text{or}\quad\frac{\partial g}{\partial t} = \frac{\partial g}{\partial h} \frac{\partial h}{\partial t}
\end{align}</p>
<p>This is because a neural network is just a big composition of functions.</p>
<img src="./fig/fig7.png" style='height:150px;'></section>
<section id="example-computing-neural-network-gradients">
<h2>Example: Computing Neural Network Gradients<a class="headerlink" href="#example-computing-neural-network-gradients" title="Permalink to this heading">#</a></h2>
<img src="./fig/backprop.jpg" style='height:600px;'></section>
<section id="backpropagation-gradient-descent-for-neural-networks">
<h2>Backpropagation: Gradient Descent for Neural Networks<a class="headerlink" href="#backpropagation-gradient-descent-for-neural-networks" title="Permalink to this heading">#</a></h2>
<p>The <em><strong>backpropagation</strong></em> algorithm consists of three phases:
0. (<strong>Initialize</strong>) intialize the network parameters <span class="math notranslate nohighlight">\(\mathbf{W}\)</span></p>
<ol class="arabic simple">
<li><p>Repeat:</p></li>
<li><p>(<strong>Forward Pass</strong>) compute all intermediate values <span class="math notranslate nohighlight">\(s_{ij}^l\)</span> and <span class="math notranslate nohighlight">\(a_{ij}^l\)</span> for the given covariates <span class="math notranslate nohighlight">\(\mathbf{X}\)</span></p></li>
<li><p>(<strong>Backward Pass</strong>) compute all the gradients <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial w^l_{ij}}\)</span></p></li>
<li><p>(<strong>Update Parameters</strong>) update each parameter by <span class="math notranslate nohighlight">\(-\eta \frac{\partial \mathcal{L}}{\partial w^l_{ij}}\)</span></p></li>
</ol>
<img src="./fig/graph_structure.png" style='height:200px;'></section>
<section id="gradient-computation-with-automatic-differentiation">
<h2>Gradient Computation with Automatic Differentiation<a class="headerlink" href="#gradient-computation-with-automatic-differentiation" title="Permalink to this heading">#</a></h2>
<p>The forwards-backwards way of computing the gradient lends itself to an algorithm that automates gradient computation for any neural network.</p>
<p>This is a special instance of <em><strong>reverse mode automatic differentiation</strong></em> – a method of algorithmically computing exact gradients for functions defined by combinations of simple functions, by drawing graphical models of the composition of functions and then taking gradients by going forwards-backwards.
<img src="./fig/function.png" style='height:50px;'></p>
<img src="./fig/computation_graph.png" style='height:150px;'>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="what-does-a-neural-network-learn">
<h1><span class="section-number">29. </span>What Does a Neural Network Learn?<a class="headerlink" href="#what-does-a-neural-network-learn" title="Permalink to this heading">#</a></h1>
<section id="why-is-a-neural-network-classifier-so-effective">
<h2>Why is a Neural Network Classifier So Effective?<a class="headerlink" href="#why-is-a-neural-network-classifier-so-effective" title="Permalink to this heading">#</a></h2>
<p>Visualizing the decision boundary:</p>
<img src="./fig/boundary.png" style='height:350px;'></section>
<section id="id1">
<h2>Why is a Neural Network Classifier So Effective?<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>Visualizing the output of the last hidden layer.</p>
<p>Before neural network models became wildly popular in machine learning, a common method for building non-linear classifiers is to first map the data, in the input space <span class="math notranslate nohighlight">\(\mathbb{R}^{\text{input}}\)</span>, into a ‘feature’ space <span class="math notranslate nohighlight">\(\mathbb{R}^{\text{feature}}\)</span>, such that the classes are well-separated in the feature space. Then, a linear classifier can be fitted to the transformed data.</p>
<p>If we ignore the output node of our neural network classifier, we are left with a function, <span class="math notranslate nohighlight">\(\mathbb{R}^{2} \to \mathbb{R}^2\)</span>, mapping the data from the input space to a 2-dimensional feature space. The transformed data (and in general, the output from a hidden layer in a neural network) is called a <em><strong>representation</strong></em> of the data.</p>
<img src="fig/architecture.jpeg" style="height:350px;">
<p>Visualizing these representations can often shed light on how and what neural network models learns from the data.</p>
<img src="./fig/latent.png" style='height:350px;'></section>
<section id="two-interpretations-of-a-neural-network-classifier">
<h2>Two Interpretations of a Neural Network Classifier:<a class="headerlink" href="#two-interpretations-of-a-neural-network-classifier" title="Permalink to this heading">#</a></h2>
<table>
    <tr><td><font size="3">A Complex Decision Boundary $g\quad\quad\quad\quad\quad$</font></td>
        <td><font size="3">A Transformation $g_0$ and a linear model $g_1\quad\quad\quad\quad$</font></td>
    <tr><td><img src="fig/decision.png" style="height:350px;"></td>
        <td><img src="fig/architecture2.png" style="height:400px;"></td></tr>
</table></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="with-great-flexibility-comes-with-great-problems">
<h1><span class="section-number">30. </span>With Great Flexibility Comes with Great Problems<a class="headerlink" href="#with-great-flexibility-comes-with-great-problems" title="Permalink to this heading">#</a></h1>
<section id="neural-network-regression-vs-linear-regression">
<h2>Neural Network Regression vs Linear Regression<a class="headerlink" href="#neural-network-regression-vs-linear-regression" title="Permalink to this heading">#</a></h2>
<p>Linear models are easy to interpret. Once we’ve found the MLE of the model parameters, we can formulate scientific hypotheses about the relationship between the outcome <span class="math notranslate nohighlight">\(Y\)</span> and the covariates <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>:</p>
<p>\begin{align}
\widehat{\text{income}} = 2 * \text{education (yr)} + 3.1 * \text{married} - 1.5 * \text{gaps in work history}
\end{align}</p>
<p>What do the weights of a neural network tell you about the relationship between the covariates and the outcome?
<img src="./fig/fig5.png" style='height:250px;'></p>
</section>
<section id="interpretable-deep-learning">
<h2>Interpretable Deep Learning<a class="headerlink" href="#interpretable-deep-learning" title="Permalink to this heading">#</a></h2>
<p>We might be tempted to conclude that neural networks are uninterpretable due to their complexity. But just because we can’t understand neural networks by inspecting the value of the individual weights, it does not mean that we can’t understand them.</p>
<p>In <a class="reference external" href="https://arxiv.org/abs/1606.03490">The Mythos of Model Interpretability</a>, the authors survey a large number of methods for interpreting deep models.</p>
<img src="fig/cnnviz.jpg" style="height:400px;" align="center"/></section>
<section id="can-machine-learning-models-make-use-of-human-concepts">
<h2>Can Machine Learning Models Make Use of Human Concepts?<a class="headerlink" href="#can-machine-learning-models-make-use-of-human-concepts" title="Permalink to this heading">#</a></h2>
<p><em><strong>(with Anita Mahinpei, Justin Clark, Ike Lage, Finale Doshi-Velez)</strong></em></p>
<p>What if instead building complex non-linear models based on raw inputs, we instead build simple linear models based on human interpretable <strong>concepts</strong>? We use a neural network to predict concepts from inputs and then use a linear model to predict the outcome from the concepts. We interpret the relationship between the outcome and the concepts via the linear model. These models are called <strong>concept bottleneck models</strong>.</p>
<p>In <a class="reference external" href="https://arxiv.org/abs/2106.13314">The Promises and Pitfalls of Black-box Concept Learning Models</a>, we examine the advantages and drawbacks of these models.</p>
<img src="fig/slide15.png" style="height:300px;" align="center"/></section>
<section id="can-machine-learning-models-learn-to-explore-hypothetical-scenarios">
<h2>Can Machine Learning Models Learn to Explore Hypothetical Scenarios?<a class="headerlink" href="#can-machine-learning-models-learn-to-explore-hypothetical-scenarios" title="Permalink to this heading">#</a></h2>
<p><em><strong>(with Michael Downs, Jonathan Chu, Wisoo Song, Yaniv Yacoby, Finale Doshi-Velez)</strong></em></p>
<p>Rather than explaining why the model made a decision, it’s often more helpful to explain how to change the data in order to change the model’s decision. This modified input is a <strong>counter-factual</strong>. In <a class="reference external" href="https://finale.seas.harvard.edu/files/finale/files/cruds-_counterfactual_recourse_using_disentangled_subspaces.pdf">CRUDS: Counterfactual Recourse Using Disentangled Subspaces</a>, we study how to automatically generate counter-factual explanations that can help users achieve a favorable outcome from a decision system.</p>
<img src="fig/slide16.png" style="height:350px;" align="center"/></section>
<section id="right-for-the-right-reasons">
<h2>Right for the Right Reasons?<a class="headerlink" href="#right-for-the-right-reasons" title="Permalink to this heading">#</a></h2>
<p>In <span class="xref myst"><em>An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets</em></span>, the authors build a neural network model to detect acute intracranial haemorrhage (ICH) and classifies five ICH subtypes.</p>
<p>Model classifications are explained by highlighting the pixels that contributed the most to the decision. The highligthed regions tends to overlapped with ‘bleeding points’ annotated by neuroradiologists on the images.</p>
<img src="./fig/shap.png" style="height: 350px;" align="center"/></section>
<section id="the-perils-of-explanations">
<h2>The Perils of Explanations<a class="headerlink" href="#the-perils-of-explanations" title="Permalink to this heading">#</a></h2>
<p>In <span class="xref myst"><em>How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection</em></span>, the authors found that clinicians interacting with incorrect recommendations paired with simple explanations experienced significant reduction in treatment selection accuracy.</p>
<img src="./fig/reliance.png" style="height: 350px;" align="center"/>
<p><strong>Take-away:</strong> Incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms.</p>
</section>
<section id="generalization-error-and-bias-variance">
<h2>Generalization Error and Bias/Variance<a class="headerlink" href="#generalization-error-and-bias-variance" title="Permalink to this heading">#</a></h2>
<p>Complex models have <em><strong>low bias</strong></em> – they can model a wide range of functions, given enough samples.</p>
<p>But complex models like neural networks can use their ‘extra’ capacity to explain non-meaningful features of the training data that are unlikely to appear in the test data (i.e. noise). These models have <em><strong>high variance</strong></em> – they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings.</p>
<table>
    <tr>
        <td>
            <img src="./fig/fig11.png" style="width: 380px;" align="center"/>
        </td>
        <td>
            <img src="./fig/fig12.png" style="width: 380px;" align="center"/>
        </td>
    </tr>
</table></section>
<section id="generalization-of-deep-models">
<h2>Generalization of Deep Models<a class="headerlink" href="#generalization-of-deep-models" title="Permalink to this heading">#</a></h2>
<p>Just as in the case of linear and polynomial models, we can prevent nerual networks from overfitting (i.e. poor generalization due to high variance) by regularization or by ensembling a large number of models.</p>
<p>However, a new body of work like <a class="reference external" href="https://mltheory.org/deep.pdf">Deep Double Descent: Where Bigger Models and More Data Hurt</a> show that very wide neural networks (with far more parameters than there are data observations) actually ceases to overfit as the width surpasses a certain threshold. In fact, as the width of a neural network approaches infinity, training the neural network becomes kernel regression (this kernel is called the <em><strong>neural tangent kernel</strong></em>)!</p>
<img src="./fig/dd.jpg" style="width: 500px;" align="center"/></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture_15_notes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">21. </span>Lecture #15: Parallel Tempering and Stochastic HMC</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture_17_notes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">31. </span>Lecture #17: Black-box Variational Inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">25. Lecture #16: Neural Network Models for Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-as-generalized-linear-models">26. Regression as Generalized Linear Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-models">Linear Regression Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-would-you-parameterize-a-non-linear-trend">How Would You Parameterize a Non-linear Trend?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-the-geometry-of-logistic-regression">Review of the Geometry of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-would-you-parametrize-a-ellipitical-decision-boundary">How would you parametrize a ellipitical decision boundary?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-would-you-parametrize-an-arbitrary-complex-decision-boundary">How would you parametrize an arbitrary complex decision boundary?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">27. Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-arbitrarily-complex-decision-boundaries">Approximating Arbitrarily Complex Decision Boundaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-neural-network">What is a Neural Network?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-as-function-approximators">Neural Networks as Function Approximators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-flexible-framework-for-function-approximation">A Flexible Framework for Function Approximation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-choices-for-the-activation-function">Common Choices for the Activation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-are-universal-function-approximators">Neural Networks are Universal Function Approximators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-regression">Neural Networks Regression</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-differentiation-and-backpropagation">28. Automatic Differentiation and Backpropagation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-computation-for-neural-networks">Gradient Computation for Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-computing-neural-network-gradients">Example: Computing Neural Network Gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-gradient-descent-for-neural-networks">Backpropagation: Gradient Descent for Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-computation-with-automatic-differentiation">Gradient Computation with Automatic Differentiation</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-a-neural-network-learn">29. What Does a Neural Network Learn?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-a-neural-network-classifier-so-effective">Why is a Neural Network Classifier So Effective?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Why is a Neural Network Classifier So Effective?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-interpretations-of-a-neural-network-classifier">Two Interpretations of a Neural Network Classifier:</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#with-great-flexibility-comes-with-great-problems">30. With Great Flexibility Comes with Great Problems</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-regression-vs-linear-regression">Neural Network Regression vs Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-deep-learning">Interpretable Deep Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-machine-learning-models-make-use-of-human-concepts">Can Machine Learning Models Make Use of Human Concepts?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-machine-learning-models-learn-to-explore-hypothetical-scenarios">Can Machine Learning Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#right-for-the-right-reasons">Right for the Right Reasons?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perils-of-explanations">The Perils of Explanations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-error-and-bias-variance">Generalization Error and Bias/Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-of-deep-models">Generalization of Deep Models</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yaniv Yacoby
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>