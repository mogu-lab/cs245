

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>1. Lecture #10: Bayesian Latent Variable Models and Variational Inference &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/lecture_10_notes';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/lectures/lecture_10_notes.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Lecture #11: Hierarchical Models" href="lecture_11_notes.html" />
    <link rel="prev" title="23. Lecture #9: Latent Variable Models and MLE" href="lecture_9_notes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Probabilistic Foundations of Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Exact Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_1_notes.html">1. Lecture #1: Course Overview</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_2_notes.html">7. Lecture #2: Maximimum Likelihood Estimation</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_3_notes.html">14. Lecture #3: Bayesian Modeling</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_4_notes.html">21. Lecture #4: Bayesian versus Frequentist Inference</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Sampling-Based Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_5_notes.html">1. Lecture #5: Sampling for Posterior Simulation</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_6_notes.html">7. Lecture #6: Monte Carlo Integration</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_7_notes.html">14. Lecture #7: Markov Chain Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_8_notes.html">18. Lecture #8: Metropolis-Hastings and Gibbs</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_9_notes.html">23. Lecture #9: Latent Variable Models and MLE</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Gradient-Based Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">1. Lecture #10: Bayesian Latent Variable Models and Variational Inference</a></li>

<li class="toctree-l1"><a class="reference internal" href="lecture_11_notes.html">3. Lecture #11: Hierarchical Models</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_12_notes.html">7. Lecture #12: Logistic Regression and Gradient Descent</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_13_notes.html">11. Lecture #13: Stochastic Gradient Descent and Simulated Annealing</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_14_notes.html">17. Lecture #14: Hamiltonian Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_15_notes.html">21. Lecture #15: Parallel Tempering and Stochastic HMC</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_16_notes.html">25. Lecture #16: Neural Network Models for Regression</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_17_notes.html">31. Lecture #17: Black-box Variational Inference</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_18_notes.html">35. Lecture #18: Automatic Differentiation</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_19_notes.html">38. Lecture #19: Variational Inference in Context</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_20_notes.html">43. Lecture #20: Variational Autoencoders</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_21_notes.html">46. Lecture #21: Implementation of Variational Autoencoders</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/mogu-lab/cs349-fall-2024/blob/master/lectures/lecture_10_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li><a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fmogu-lab%2Fcs349-fall-2024%2Fblob%2Fmaster%2Flectures/lecture_10_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onDeepnote"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_deepnote.svg">
  </span>
<span class="btn__text-container">Deepnote</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/lecture_10_notes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture #10: Bayesian Latent Variable Models and Variational Inference</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Lecture #10: Bayesian Latent Variable Models and Variational Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">1.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">1.2. Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-latent-variable-models">2. Bayesian Latent Variable Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.1. Bayesian Latent Variable Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-bayesian-inference">2.2. Challenges in Bayesian Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-idea-of-variational-inference">2.3. The Idea of Variational Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-design-of-the-variational-objective">2.4. The Design of the Variational Objective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference-as-optimization">2.5. Variational Inference as Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-of-the-elbo">2.6. Gradients of the ELBO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coordinate-ascent-variational-inference">2.7. Coordinate Ascent Variational Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximizing-the-elbo-via-coordinate-ascent">2.8. Maximizing the ELBO via Coordinate Ascent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-the-update-rule-for-q-psi-i-lambda-text-new-i">2.9. Proof of the Update Rule for <span class="math notranslate nohighlight">\(q(\psi_i | \lambda^{\text{new}}_i)\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-show-that-mathbb-e-psi-sim-q-psi-lambda-ldots-mathbb-e-psi-i-sim-q-psi-i-lambda-i-left-mathbb-e-psi-i-sim-q-psi-i-lambda-i-ldots-right">Step 1: Show that <span class="math notranslate nohighlight">\(\mathbb{E}_{\psi \sim q(\psi|\lambda)}[\ldots] = \mathbb{E}_{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}_{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})}[\ldots] \right]\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-show-that-underset-lambda-i-max-mathbb-e-psi-i-sim-q-psi-i-lambda-i-ldots-equiv-underset-lambda-i-min-d-text-kl-ldots">Step 2: Show that <span class="math notranslate nohighlight">\(\underset{\lambda_i}{\max}\mathbb{E}_{\psi_i \sim q(\psi_i|\lambda_i)}[\ldots] \equiv \underset{\lambda_i}{\min} D_{\text{KL}}[\ldots]\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-minimize-the-kl-divergence">Step 3: Minimize the KL-divergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-gaussian-mixture-models">2.10. Bayesian Gaussian Mixture Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference-for-bayesian-gaussian-mixture-models">2.11. Variational Inference for Bayesian Gaussian Mixture Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coordinate-ascent-variational-inference-updates-for-bayesian-gmm">2.12. Coordinate Ascent Variational Inference Updates for Bayesian GMM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-rule-for-q-z-n-phi-n">Update rule for <span class="math notranslate nohighlight">\(q(Z_n | \phi_n)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-rule-for-q-mu-k-m-k-s-k-2">Update rule for <span class="math notranslate nohighlight">\(q(\mu_k | m_k, s_k^2)\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implemenation-of-cavi-for-bayesian-gmm">2.13. Implemenation of CAVI for Bayesian GMM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sanity-check-elbo-during-training">2.14. Sanity Check: ELBO During Training</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-10-bayesian-latent-variable-models-and-variational-inference">
<h1><span class="section-number">1. </span>Lecture #10: Bayesian Latent Variable Models and Variational Inference<a class="headerlink" href="#lecture-10-bayesian-latent-variable-models-and-variational-inference" title="Permalink to this heading">#</a></h1>
<section id="am-207-advanced-scientific-computing">
<h2><span class="section-number">1.1. </span>AM 207: Advanced Scientific Computing<a class="headerlink" href="#am-207-advanced-scientific-computing" title="Permalink to this heading">#</a></h2>
<section id="stochastic-methods-for-data-analysis-inference-and-optimization">
<h3>Stochastic Methods for Data Analysis, Inference and Optimization<a class="headerlink" href="#stochastic-methods-for-data-analysis-inference-and-optimization" title="Permalink to this heading">#</a></h3>
</section>
<section id="fall-2021">
<h3>Fall, 2021<a class="headerlink" href="#fall-2021" title="Permalink to this heading">#</a></h3>
<img src="fig/logos.jpg" style="height:150px;"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Import basic libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="outline">
<h2><span class="section-number">1.2. </span>Outline<a class="headerlink" href="#outline" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Bayesian Latent Variable Models</p></li>
<li><p>Coordinate Ascent Variational Inference</p></li>
<li><p>Bayesian Gaussian Mixture Models</p></li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="bayesian-latent-variable-models">
<h1><span class="section-number">2. </span>Bayesian Latent Variable Models<a class="headerlink" href="#bayesian-latent-variable-models" title="Permalink to this heading">#</a></h1>
<section id="id1">
<h2><span class="section-number">2.1. </span>Bayesian Latent Variable Models<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>Overfitting is an always a concern when using MLE model parameters. We can mitigate the effect of outliers in the data on the model we learn by treating the parameters as random variables and placing priors on them.</p>
<p>In a latent variable model, maximum liklihood inference treats parameters <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(\phi\)</span> as unknown constants and produces point-estimates for them. In a Bayesian latent variable model, <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(\phi\)</span> are random variables and we derive the posterior distribution over them.</p>
<img src="fig/bayesian_model.jpg" style="height:220px;">
<p>That is, we want to infer
$<span class="math notranslate nohighlight">\(p(\theta, \phi, Z_1, \ldots, Z_N|Y_1, \ldots, Y_N, a, b) = \frac{p(\theta | a)p(\phi|b)\prod_{n}p(Y_n|Z_n, \phi)p(Z_n|\theta)}{\prod_{n} p(Y_n)}.\)</span>$</p>
</section>
<section id="challenges-in-bayesian-inference">
<h2><span class="section-number">2.2. </span>Challenges in Bayesian Inference<a class="headerlink" href="#challenges-in-bayesian-inference" title="Permalink to this heading">#</a></h2>
<p>Unfortunately, most Bayesian models with multiple types of random variables (like Bayesian latent variable models) have complex posteriors that do not match known distributions. <em><strong>Exact inference</strong></em> is not possible.</p>
<p>Sampling from the posterior may not always be the best option because:</p>
<ol class="arabic simple">
<li><p>Convergence of samplers may be slow (due to high dimensionality of the distribution or multimodality)<br><br></p></li>
<li><p>Samplers like Metropolis-Hastings requires evaluating the liklihood <span class="math notranslate nohighlight">\(\prod_n p(Y_n | Z_n, \phi)\)</span> in each iteration, if the observed data is large (<span class="math notranslate nohighlight">\(N\)</span> is in the millions), this computation is expensive.</p></li>
</ol>
</section>
<section id="the-idea-of-variational-inference">
<h2><span class="section-number">2.3. </span>The Idea of Variational Inference<a class="headerlink" href="#the-idea-of-variational-inference" title="Permalink to this heading">#</a></h2>
<p><strong>Idea: (Approximate Inferenec)</strong> Approximate the hard posterior <span class="math notranslate nohighlight">\(p(\theta, \phi, Z_1, \ldots, Z_N|Y_1, \ldots, Y_N)\)</span> with a distribution <span class="math notranslate nohighlight">\(q\)</span> that is easy to sample from (like a Gaussian). Any computation involving the posterior can now be done with <span class="math notranslate nohighlight">\(q\)</span>.</p>
<p>This approximation of <span class="math notranslate nohighlight">\(p(\theta, \phi, Z_1, \ldots, Z_N|Y_1, \ldots, Y_N)\)</span> with a distribution <span class="math notranslate nohighlight">\(q\)</span> is called <em><strong>variational inference</strong></em>.
<img src="fig/variational.jpg" style="height:220px;"></p>
</section>
<section id="the-design-of-the-variational-objective">
<h2><span class="section-number">2.4. </span>The Design of the Variational Objective<a class="headerlink" href="#the-design-of-the-variational-objective" title="Permalink to this heading">#</a></h2>
<p><strong>Goal:</strong> given a target posterior distribution <span class="math notranslate nohighlight">\(p(\psi | Y_1, \ldots, Y_N)\)</span>, <span class="math notranslate nohighlight">\(\psi \in \mathbb{R}^I\)</span> we want to find a distribution <span class="math notranslate nohighlight">\(q(\psi |\lambda^*)\)</span> in a family of distributions <span class="math notranslate nohighlight">\(Q = \{q(\psi |\lambda) | \lambda \in \Lambda \}\)</span> such that <span class="math notranslate nohighlight">\(q(\psi |\lambda^*)\)</span> best approximates <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p><strong>Design Choices:</strong> we need to choose:</p>
<p>A. <em><strong>(Variational family)</strong></em> a family <span class="math notranslate nohighlight">\(Q\)</span> of candidate distributions for approximating <span class="math notranslate nohighlight">\(p\)</span>. The members of <span class="math notranslate nohighlight">\(Q\)</span> are called the <em><strong>variational distributions</strong></em>.</p>
<p><strong>Our Choice:</strong>  we assume that the joint <span class="math notranslate nohighlight">\(q(\psi)\)</span> factorizes completely over each dimension of <span class="math notranslate nohighlight">\(\psi\)</span>, i.e. <span class="math notranslate nohighlight">\(q(\psi)= \prod_{i=1}^I q(\psi_i | \lambda_i)\)</span>. This is called the <em><strong>mean field assumption</strong></em>. What can go wrong with this design choice?</p>
<p>B. <em><strong>(Divergence measure)</strong></em> a divergence measure to quantify the difference between <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>.</p>
<p><strong>Our Choice:</strong>
$<span class="math notranslate nohighlight">\(D_{\text{KL}}(q(\psi | \lambda) \| p(\psi | Y_1, \ldots, Y_N)) = \mathbb{E}_{\psi \sim q(\psi|\lambda)}\left[\log\left( \frac{q(\psi | \lambda)}{p(\psi | Y_1, \ldots, Y_N)} \right) \right]\)</span>$
What can go wrong with this design choice?</p>
</section>
<section id="variational-inference-as-optimization">
<h2><span class="section-number">2.5. </span>Variational Inference as Optimization<a class="headerlink" href="#variational-inference-as-optimization" title="Permalink to this heading">#</a></h2>
<p>We now formalize variational inference for a target <span class="math notranslate nohighlight">\(p(\psi)\)</span>: find <span class="math notranslate nohighlight">\(q(\psi|\lambda^*)\)</span> where</p>
<p>\begin{aligned}
\lambda^* &amp;= \underset{\lambda}{\text{argmin}}; D_{\text{KL}}(q(\psi|\lambda) | p(\psi|Y_1, \ldots, Y_N))) \
&amp;= \underset{\lambda}{\text{argmin}}; \mathbb{E}_{\psi \sim q(\psi|\lambda)}\left[\log\left(\frac{q(\psi | \lambda)}{p(\psi|Y_1, \ldots, Y_N))}\right) \right]
\end{aligned}</p>
<p>Recall that for EM, we had proved that minimizing the KL is equivalent to maximizing the ELBO (for which it is easier to compute the gradient). We will do the same here:</p>
<p>\begin{aligned}
\underset{\lambda}{\min}D_{\text{KL}}(q(\psi|\lambda) | p(\psi|Y_1, \ldots, Y_N))) \overset{\text{equiv}}{\equiv}&amp; \underset{\lambda}{\max} -D_{\text{KL}}(q(\psi|\lambda) | p(\psi|Y_1, \ldots, Y_N))) \
=&amp; \underset{\lambda}{\max} -\mathbb{E}<em>{\psi \sim q(\psi|\lambda)}\left[\log\left(\frac{q(\psi | \lambda)}{p(\psi|Y_1, \ldots, Y_N))} \right)\right] \
=&amp; \underset{\lambda}{\max}\underbrace{\mathbb{E}</em>{\psi \sim q(\psi|\lambda)}\left[\log\left(\frac{p(\psi, Y_1, \ldots, Y_N))}{q(\psi | \lambda)} \right)\right]}_{ELBO(\lambda)} \
&amp;- \log p(Y_1, \ldots, Y_N).
\end{aligned}</p>
<p>Thus, the variational objective can be rephrased as maximizing the <span class="math notranslate nohighlight">\(ELBO\)</span>.</p>
</section>
<section id="gradients-of-the-elbo">
<h2><span class="section-number">2.6. </span>Gradients of the ELBO<a class="headerlink" href="#gradients-of-the-elbo" title="Permalink to this heading">#</a></h2>
<p>Unfortunately, the ELBO for variational inference of the posterior does not have easy gradients,</p>
<p>\begin{aligned}
\nabla_{\lambda},\underbrace{\mathbb{E}<em>{\psi \sim q(\psi|\lambda)}\left[\log\left(\frac{p(\psi, Y_1, \ldots, Y_N)}{q(\psi | \lambda)} \right)\right]}</em>{ELBO(\lambda)}.
\end{aligned}</p>
<p>In particular, the issue is that the gradient taken is with respect to the parameter <span class="math notranslate nohighlight">\(\psi\)</span> of the distribution over which we are taking the expectation - i.e. we cannot push the gradient into the expectation.</p>
<p>Today we will maximize the <span class="math notranslate nohighlight">\(ELBO\)</span> using coordinate ascent (just as in the case of EM). But you’ll see that <em><strong>coordinate ascent variational inference</strong></em> requires that we perform model specific computations (often in closed form). This restrict the class of Bayesian models for which we can perform variational inference.</p>
<p>Two of the major development we will cover later in the semester address how to estimate this gradient <strong>efficiently and without bias</strong>.</p>
</section>
<section id="coordinate-ascent-variational-inference">
<h2><span class="section-number">2.7. </span>Coordinate Ascent Variational Inference<a class="headerlink" href="#coordinate-ascent-variational-inference" title="Permalink to this heading">#</a></h2>
<img src="fig/Lecture_10_derivation.pdf" style="height:900px;"></section>
<section id="maximizing-the-elbo-via-coordinate-ascent">
<h2><span class="section-number">2.8. </span>Maximizing the ELBO via Coordinate Ascent<a class="headerlink" href="#maximizing-the-elbo-via-coordinate-ascent" title="Permalink to this heading">#</a></h2>
<p>The coordinate ascent algorithm maximizes an objective function <span class="math notranslate nohighlight">\(ELBO(\lambda)\)</span> by iteratively maximizing over <span class="math notranslate nohighlight">\(\lambda_i\)</span>, holding constant <span class="math notranslate nohighlight">\(\lambda_{-i} = [\lambda_1\; \ldots\; \lambda_{i-1}\; \lambda_{i+1}\; \ldots\; \lambda_{I}]\)</span>.</p>
<p>The <em><strong>coordinate ascent variational inference algorithm</strong></em>:
0. <strong>Initialization:</strong> pick an intial value <span class="math notranslate nohighlight">\(\lambda^{(0)}\)</span></p>
<ol class="arabic">
<li><p><strong>Coordinate-wise maximization:</strong></p>
<p>Repeat for <span class="math notranslate nohighlight">\(j=1, \ldots, J\)</span> iterations:</p>
<p><span class="math notranslate nohighlight">\(\quad\quad\)</span> Cycle thru <span class="math notranslate nohighlight">\(i=1, \ldots, I\)</span> coordinates:</p>
</li>
</ol>
<div class="math notranslate nohighlight">
\[q(\psi_i | \lambda^{\text{new}}_i) \propto \exp\left\{ \mathbb{E}_{\psi_{-i} \sim q(\psi_{-i} | \lambda^{\text{new}}_{1}, \ldots, \lambda^{\text{new}}_{i-1}, \lambda^{\text{old}}_{i+1}, \ldots, \lambda^{\text{old}}_{I})}\left[\log p(Y_1, \ldots, Y_N, \psi)\right]\right\}.\]</div>
<p>where <span class="math notranslate nohighlight">\(\psi_{-i} = [\psi_1\; \ldots\; \psi_{i-1}\; \psi_{i+1}\; \ldots\; \psi_{I}]\)</span>.</p>
</section>
<section id="proof-of-the-update-rule-for-q-psi-i-lambda-text-new-i">
<h2><span class="section-number">2.9. </span>Proof of the Update Rule for <span class="math notranslate nohighlight">\(q(\psi_i | \lambda^{\text{new}}_i)\)</span><a class="headerlink" href="#proof-of-the-update-rule-for-q-psi-i-lambda-text-new-i" title="Permalink to this heading">#</a></h2>
<p>We want to show that <span class="math notranslate nohighlight">\(q(\psi_i | \lambda^{*}_i) \propto \exp\left\{ \mathbb{E}_{\psi_{-i} \sim q(\phi_{-i} | \lambda_{-i})}\left[\log p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i})\right]\right\}\)</span>, where</p>
<div class="math notranslate nohighlight">
\[
\lambda^{*}_i = \underset{\lambda_i}{\max}\underbrace{\mathbb{E}_{\psi \sim q(\psi|\lambda)}\left[\log\left(\frac{p(\psi, Y_1, \ldots, Y_N))}{q(\psi | \lambda)} \right)\right]}_{ELBO(\lambda)}.
\]</div>
<p>To maximize the <span class="math notranslate nohighlight">\(ELBO\)</span>, we will</p>
<ol class="arabic simple">
<li><p>use the mean-field assumption to break up the expectation <span class="math notranslate nohighlight">\(\mathbb{E}_{\psi \sim q(\psi|\lambda)}\)</span> into an interated expectation <span class="math notranslate nohighlight">\(\mathbb{E}_{\psi_i \sim q(\psi_i|\lambda_i)}\mathbb{E}_{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})}\)</span></p></li>
<li><p>we will rewrite the outer expectation <span class="math notranslate nohighlight">\(\mathbb{E}_{\psi_i \sim q(\psi_i|\lambda_i)}\)</span> as a negative KL-divergence</p></li>
<li><p>we will maximize the negative KL-divergence by setting the two arguments of the divergence equal to each other</p></li>
</ol>
<p>For the following, we will use the following notation:</p>
<p>\begin{aligned}
\lambda_{-i} &amp;= [\lambda_1; \ldots; \lambda_{i-1}; \lambda_{i+1}; \ldots; \lambda_{I}]\
\psi_{-i} &amp;= [\psi_1; \ldots; \psi_{i-1}; \psi_{i+1}; \ldots; \psi_{I}]\
q(\psi_{-i}|\lambda_{-i}) &amp;= \prod_{j\neq i}q(\psi_{j}|\lambda_{j})\
\Psi_{-i} &amp;= \bigcup_{j\neq i} \Psi_j\
d\psi_{-1} &amp;= d(\psi_1, \ldots, \psi_{i-1}, \psi_{i+1}, \ldots, \psi_{I})
\end{aligned}</p>
<section id="step-1-show-that-mathbb-e-psi-sim-q-psi-lambda-ldots-mathbb-e-psi-i-sim-q-psi-i-lambda-i-left-mathbb-e-psi-i-sim-q-psi-i-lambda-i-ldots-right">
<h3>Step 1: Show that <span class="math notranslate nohighlight">\(\mathbb{E}_{\psi \sim q(\psi|\lambda)}[\ldots] = \mathbb{E}_{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}_{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})}[\ldots] \right]\)</span><a class="headerlink" href="#step-1-show-that-mathbb-e-psi-sim-q-psi-lambda-ldots-mathbb-e-psi-i-sim-q-psi-i-lambda-i-left-mathbb-e-psi-i-sim-q-psi-i-lambda-i-ldots-right" title="Permalink to this heading">#</a></h3>
<p>For step 1, we rewrite the ELBO:</p>
<p>\begin{aligned}
\underbrace{\mathbb{E}<em>{\psi \sim q(\psi|\lambda)}\left[\log\left(\frac{p(\psi, Y_1, \ldots, Y_N))}{q(\psi | \lambda)} \right)\right]}</em>{ELBO(\lambda)} &amp;= \int_{\Psi} \left[ \log \left(\frac{p(Y_1, \ldots, Y_N, \psi)}{q(\psi|\lambda)} \right)\right]q(\psi|\lambda) d\psi\
&amp;= \int_{\Psi_i} \int_{\Psi_{-i}} \left[ \log \left(\frac{p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i})p(\psi_{-i})}{q(\psi_{i}|\lambda_{i})q(\psi_{-i}|\lambda_{-i})} \right)\right] q(\psi_{i}|\lambda_{i})q(\psi_{-i}|\lambda_{-i}) d\psi_{-i}d\psi_i\quad (\text{Fubini’s Theorem})\
&amp;= \int_{\Psi_i} \left[\int_{\Psi_{-i}} \left[ \log \left(\frac{p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i})p(\psi_{-i})}{q(\psi_{i}|\lambda_{i})q(\psi_{-i}|\lambda_{-i})} \right)\right] q(\psi_{-i}|\lambda_{-i}) d\psi_{-i} \right] q(\psi_{i}|\lambda_{i})d\psi_i\
&amp;= \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(\frac{p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i})p(\psi_{-i})}{q(\psi_{i}|\lambda_{i})q(\psi_{-i}|\lambda_{-i})} \right)\right]\right]
\end{aligned}</p>
<p>Now we decompose the ELBO into terms containing <span class="math notranslate nohighlight">\(\lambda_i\)</span>, over which we are going to optimize, and terms not containing <span class="math notranslate nohighlight">\(\lambda_i\)</span>:</p>
<p>\begin{aligned}
ELBO(\lambda)&amp;=\mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(\frac{p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i})p(\psi_{-i})}{q(\psi_{i}|\lambda_{i})q(\psi_{-i}|\lambda_{-i})} \right)\right]\right]\
&amp;= \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(\frac{p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i})}{q(\psi_{i}|\lambda_{i})} \right) + \log\left(\frac{p(\psi_{-i})}{q(\psi_{-i}|\lambda_{-i})} \right)\right]\right]\
&amp;= \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(\frac{p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i})}{q(\psi_{i}|\lambda_{i})} \right)\right]\right] + \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log\left(\frac{p(\psi_{-i})}{q(\psi_{-i}|\lambda_{-i})} \right)\right]\right]\
&amp;= \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(\frac{p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i})}{q(\psi_{i}|\lambda_{i})} \right)\right]\right] + \underbrace{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log\left(\frac{p(\psi_{-i})}{q(\psi_{-i}|\lambda_{-i})} \right)\right]}_{\text{constant with respect to }\lambda_i}\
\end{aligned}</p>
<p>Note that we don’t need to consider the constant term when optimizing with respect to <span class="math notranslate nohighlight">\(\lambda_i\)</span>.</p>
</section>
<section id="step-2-show-that-underset-lambda-i-max-mathbb-e-psi-i-sim-q-psi-i-lambda-i-ldots-equiv-underset-lambda-i-min-d-text-kl-ldots">
<h3>Step 2: Show that <span class="math notranslate nohighlight">\(\underset{\lambda_i}{\max}\mathbb{E}_{\psi_i \sim q(\psi_i|\lambda_i)}[\ldots] \equiv \underset{\lambda_i}{\min} D_{\text{KL}}[\ldots]\)</span><a class="headerlink" href="#step-2-show-that-underset-lambda-i-max-mathbb-e-psi-i-sim-q-psi-i-lambda-i-ldots-equiv-underset-lambda-i-min-d-text-kl-ldots" title="Permalink to this heading">#</a></h3>
<p>We show that maximizing the <span class="math notranslate nohighlight">\(ELBO\)</span> with respect to <span class="math notranslate nohighlight">\(\lambda_{i}\)</span> is equivalent to minimizing a KL-divergence:</p>
<p>\begin{aligned}
\underset{\lambda_i}{\max} ELBO(\lambda) &amp;= \underset{\lambda_i}{\max} \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(\frac{p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i})}{q(\psi_{i}|\lambda_{i})} \right)\right]\right]\
&amp;= \underset{\lambda_i}{\max} \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right] - \mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log\left(q(\psi_{i}|\lambda_{i})\right)\right]\right]\
&amp;= \underset{\lambda_i}{\max} \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right] - \log\left(q(\psi_{i}|\lambda_{i})\right)\right]\
&amp;= \underset{\lambda_i}{\max} \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log \left(\exp\left{\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right] - \log\left(q(\psi_{i}|\lambda_{i})\right)\right}\right)\right]\quad (\text{adding both a log and an exp})\
&amp;= \underset{\lambda_i}{\max} \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log \left(\frac{\exp\left{\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right} } {\exp \left{\log\left(q(\psi_{i}|\lambda_{i})\right)\right}} \right)\right]\
&amp;= \underset{\lambda_i}{\max} \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log \left(\frac{\exp\left{\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right} } {q(\psi_{i}|\lambda_{i})} \right)\right]\
&amp;\equiv \underset{\lambda_i}{\min} -\mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log \left(\frac{\exp\left{\mathbb{E}</em>{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right} } {q(\psi_{i}|\lambda_{i})} \right)\right]\
&amp;= \underset{\lambda_i}{\min} \underbrace{\mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log \left(\frac{ q(\psi</em>{i}|\lambda_{i}) } {\exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right} } \right)\right]}<em>{\text{looks like a KL except the denominator is not a distribution}}\quad (\text{using the properties of log})\
&amp;= \underset{\lambda_i}{\min} \mathbb{E}</em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log \left(\frac{ \mathcal{Z} q(\psi_{i}|\lambda_{i}) } {\mathcal{Z}\exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right} } \right)\right],\quad (\mathcal{Z} \text{ normalizes the denominator})\
&amp;= \underset{\lambda_i}{\min} \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log \left(\frac{  q(\psi</em>{i}|\lambda_{i}) } {\mathcal{Z}\exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right} } \right)\right] +\mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log(\mathcal{Z})\right] \
&amp;= \underset{\lambda_i}{\min} \mathbb{E}</em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log \left(\frac{  q(\psi_{i}|\lambda_{i}) } {\mathcal{Z}\exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right} } \right)\right] +\log(\mathcal{Z})\
&amp;= \underset{\lambda_i}{\min} \mathbb{E}<em>{\psi_i \sim q(\psi_i|\lambda_i)}\left[ \log \left(\frac{  q(\psi</em>{i}|\lambda_{i}) } {\mathcal{Z}\exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right} } \right)\right]\
&amp;= \underset{\lambda_i}{\min}D_{\text{KL}} \left[ q(\psi_{i}|\lambda_{i})| \mathcal{Z}\exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right}\right].
\end{aligned}</p>
</section>
<section id="step-3-minimize-the-kl-divergence">
<h3>Step 3: Minimize the KL-divergence<a class="headerlink" href="#step-3-minimize-the-kl-divergence" title="Permalink to this heading">#</a></h3>
<p>We see that</p>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}} \left[ q(\psi_{i}|\lambda_{i})\| \mathcal{Z}\exp\left\{\mathbb{E}_{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right\}\right]
\]</div>
<p>is minimized when</p>
<div class="math notranslate nohighlight">
\[
 q(\psi_{i}|\lambda_{i})\propto \exp\left\{\mathbb{E}_{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right\}.
\]</div>
<p>This is also exactly where the ELBO is maximized.</p>
<p>We can further rewrite the update rule for <span class="math notranslate nohighlight">\( q(\psi_{i}|\lambda_{i})\)</span> as:</p>
<p>\begin{aligned}
q(\psi_{i}|\lambda_{i})&amp;\propto \exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)\right]\right}\
&amp;= \exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)p(\psi_{-i})\right] - \mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log p(\psi_{-i})\right]\right}\
&amp;= \frac{\exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)p(\psi_{-i})\right]\right}}{\exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})}  \left[\log p(\psi_{-i})\right]\right}}\
&amp;\propto \exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi_i | \psi_{-i}) \right)p(\psi_{-i})\right]\right}\
&amp;= \exp\left{\mathbb{E}<em>{\psi</em>{-i} \sim q(\psi_{-i}|\lambda_{-i})} \left[\log \left(p(Y_1, \ldots, Y_N, \psi) \right)\right]\right}
\end{aligned}</p>
<p>The rational for rewriting the update for <span class="math notranslate nohighlight">\(q(\psi_{i}|\lambda_{i})\)</span> in terms of the joint <span class="math notranslate nohighlight">\(p(Y_1, \ldots, Y_N, \psi)\)</span> is that the joint is easy to compute.</p>
</section>
</section>
<section id="bayesian-gaussian-mixture-models">
<h2><span class="section-number">2.10. </span>Bayesian Gaussian Mixture Models<a class="headerlink" href="#bayesian-gaussian-mixture-models" title="Permalink to this heading">#</a></h2>
</section>
<section id="variational-inference-for-bayesian-gaussian-mixture-models">
<h2><span class="section-number">2.11. </span>Variational Inference for Bayesian Gaussian Mixture Models<a class="headerlink" href="#variational-inference-for-bayesian-gaussian-mixture-models" title="Permalink to this heading">#</a></h2>
<p>We consider a Bayesian model for a mixture of <span class="math notranslate nohighlight">\(K\)</span> number of univariate Gaussians:</p>
<img src="fig/bayesian_gmm.jpg" style="height:250px;">
<p>The <em><strong>hyperparameters</strong></em> of the models are <span class="math notranslate nohighlight">\(\pi, \sigma^2, m_0, s^2_0\)</span>, which are constants that must be selected prior to inference. For example, to simplify our computations we selected <span class="math notranslate nohighlight">\(\pi = [1/K, \ldots, 1/K]\)</span>, <span class="math notranslate nohighlight">\(m_0 = 0\)</span>, <span class="math notranslate nohighlight">\(\sigma=1\)</span>.</p>
<p>We make the mean field assumption – that our variational posterior factorizes completely:</p>
<div class="math notranslate nohighlight">
\[q(Z, \mu| m, s^2, \phi) = \prod_{k=1}^K q(\mu_k|m_k, s_k^2) \prod_{n=1}^N q(Z_n|\phi_n).\]</div>
</section>
<section id="coordinate-ascent-variational-inference-updates-for-bayesian-gmm">
<h2><span class="section-number">2.12. </span>Coordinate Ascent Variational Inference Updates for Bayesian GMM<a class="headerlink" href="#coordinate-ascent-variational-inference-updates-for-bayesian-gmm" title="Permalink to this heading">#</a></h2>
<p>In coordinate ascent variational inference, we update the variational distribution for each variable in turn. For the Bayesian GMM this means:</p>
<p>\begin{aligned}
q(Z_n | \phi_n) &amp;\propto \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2), Z</em>{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\log p(Y_1, \ldots, Y_N, Z_1, \ldots, Z_N, \mu)\right]\right}\
q(\mu_k | m_k, s_k^2) &amp;\propto \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log p(Y_1, \ldots, Y_N, Z_1, \ldots, Z_N, \mu)\right]\right}
\end{aligned}</p>
<p>where</p>
<p>\begin{aligned}
q(\mu | m, s^2) &amp;= \prod_{k=1}^K q(\mu_k|m_k, s_k^2),\
q(\mu_{-k} | m_{-k}, s_{-k}^2) &amp;= \prod_{j\neq k} q(\mu_j|m_j, s_j^2),\
Z &amp;= [Z_1;;\ldots;; Z_N],\
q(Z |\phi) &amp;= \prod_{n=1}^N q(Z_n|\phi_n),\
Z_{-n} &amp;= [Z_1;;\ldots;;Z_{n-1};; Z_{n+1};;\ldots;; Z_N],\
q(Z_{-n} |\phi_{-n}) &amp;= \prod_{m\neq n} q(Z_m|\phi_m).
\end{aligned}</p>
<section id="update-rule-for-q-z-n-phi-n">
<h3>Update rule for <span class="math notranslate nohighlight">\(q(Z_n | \phi_n)\)</span><a class="headerlink" href="#update-rule-for-q-z-n-phi-n" title="Permalink to this heading">#</a></h3>
<p>To update <span class="math notranslate nohighlight">\(q(Z_n | \phi_n)\)</span>, we rewrite <span class="math notranslate nohighlight">\(exp\left\{ \mathbb{E}_{\mu \sim q(\mu | m, s^2), Z_{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\log p(Y_1, \ldots, Y_N, Z_1, \ldots, Z_N, \mu)\right]\right\}\)</span>, dropping all terms that do not involve <span class="math notranslate nohighlight">\(Z_n\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<p>\begin{aligned}
q(Z_n | \phi_n) &amp;\propto \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2), Z</em>{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\log p(Y_1, \ldots, Y_N, Z_1, \ldots, Z_N, \mu)\right]\right}\
&amp;= \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2), Z</em>{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\log \left(\prod_{m=1}^N p(Y_m|\mu, Z_m) p(Z_m)\right)p(\mu)\right]\right}\
&amp;= \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2), Z</em>{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\log \left(\prod_{m=1}^N p(Y_m|\mu, Z_m) p(Z_m)\right)\right] + \log[p(\mu)]\right}\
&amp;= \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2), Z</em>{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\log \left(\prod_{m=1}^N p(Y_m|\mu, Z_m) p(Z_m)\right)\right]\right} \exp\left{ \log[p(\mu)]\right}\
&amp;\propto \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2), Z</em>{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\log \left(\prod_{m=1}^N p(Y_m|\mu, Z_m) p(Z_m)\right)\right]\right}\quad\quad (\text{dropped priors on <span class="math notranslate nohighlight">\(\mu\)</span>})\
&amp;= \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2), Z</em>{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\log \left(p(Y_n | \mu, Z_n) p(Z_n) \prod_{m\neq n} p(Y_m|\mu, Z_m) p(Z_m)\right)\right]\right}\
&amp;=  \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2), Z</em>{-n} \sim q(Z_{-n} | \phi_{-n})}\left[ \log p(Y_n | \mu, Z_n) + \log p(Z_n) + \sum_{m\neq n} \log \left( p(Y_m|\mu, Z_m) p(Z_m)\right) \right]\right}\
&amp;=  \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[ \log p(Y_n | \mu, Z_n) + \log p(Z_n)\right] +  \mathbb{E}</em>{\mu \sim q(\mu | m, s^2), Z_{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\sum_{m\neq n} \log \left( p(Y_m|\mu, Z_m) p(Z_m)\right) \right]\right}\
&amp;=  \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[ \log p(Y_n | \mu, Z_n) + \log p(Z_n)\right]\right} \underbrace{\exp\left{\mathbb{E}</em>{\mu \sim q(\mu | m, s^2), Z_{-n} \sim q(Z_{-n} | \phi_{-n})}\left[\sum_{m\neq n} \log \left( p(Y_m|\mu, Z_m) p(Z_m)\right) \right]\right}}<em>{\text{constant with respect to <span class="math notranslate nohighlight">\(Z_n\)</span>}}\
&amp;\propto  \exp\left{ \mathbb{E}</em>{\mu \sim q(\mu | m, s^2)}\left[ \log p(Y_n | \mu, Z_n) + \log p(Z_n)\right]\right} \quad\quad (\text{dropped all terms except for the <span class="math notranslate nohighlight">\(n\)</span>-th})\
&amp;= \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[ \log p(Y_n | \mu, Z_n) \right]+ \mathbb{E}</em>{\mu \sim q(\mu | m, s^2)} \left[\log p(Z_n)\right]\right}\
&amp;= \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[ \log p(Y_n | \mu, Z_n) \right]+ \log p(Z_n)\right}\
&amp;= \exp\left{ \mathbb{E}</em>{\mu \sim q(\mu | m, s^2)}\left[ \log p(Y_n | \mu, Z_n) \right]+ \log \frac{1}{K}\right}\
&amp;= \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[ \log p(Y_n | \mu, Z_n) \right]\right} \exp\left{\log \frac{1}{K}\right}\
&amp;\propto \exp\left{ \mathbb{E}</em>{\mu \sim q(\mu | m, s^2)}\left[ \log p(Y_n | \mu, Z_n) \right]\right}\quad\quad (\text{dropped the prior on <span class="math notranslate nohighlight">\(Z_n\)</span>})\
\end{aligned}</p>
<p>The likelihood <span class="math notranslate nohighlight">\(p(Y_n | \mu, Z_n)\)</span> is a normal distribution given the value of <span class="math notranslate nohighlight">\(Z_n\)</span>, we can express this by treating each <span class="math notranslate nohighlight">\(Z_n\)</span> as an indicator variable – a <span class="math notranslate nohighlight">\(K\)</span>-dimensional binary vector with <span class="math notranslate nohighlight">\(Z_{nk}=1\)</span> if <span class="math notranslate nohighlight">\(Y_n\)</span> is in class <span class="math notranslate nohighlight">\(k\)</span> and 0 otherwise. Thus, we can write
\begin{aligned}
\log p(Y_n | \mu, Z_n) &amp;= \log \prod_{k=1}^K \mathcal{N}(Y_n; \mu_k, 1)^{Z_{nk}}\
&amp;= \sum_{k=1}^K Z_{nk} \log \mathcal{N}(Y_n; \mu_k, 1)\
&amp;= -\frac{1}{2}\sum_{k=1}^K Z_{nk} (Y_n - \mu_k)^2 + const\
\end{aligned}</p>
<p>Finally, we can write the update for <span class="math notranslate nohighlight">\(q(Z_n | \phi_n)\)</span> as follows, again dropping all terms not involving <span class="math notranslate nohighlight">\(Z_n\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<p>\begin{aligned}
q(Z_n | \phi_n) &amp;\propto  \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[ -\frac{1}{2}\sum</em>{k=1}^K Z_{nk} (Y_n - \mu_k)^2 + const\right]\right}\
&amp;= \exp\left{ \mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[ -\frac{1}{2}\sum</em>{k=1}^K Z_{nk} (Y_n - \mu_k)^2\right]\right} \exp\left{\mathbb{E}<em>{\mu \sim q(\mu | m, s^2)} \left[const\right]\right}\
&amp;\propto \exp\left{ \mathbb{E}</em>{\mu \sim q(\mu | m, s^2)}\left[ -\frac{1}{2}\sum_{k=1}^K Z_{nk} (Y_n - \mu_k)^2\right]\right}\
&amp;= \exp\left{  -\frac{1}{2}\sum_{k=1}^K Z_{nk} \mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[Y^2_n - 2Y_n\mu_k + \mu_k^2\right]\right}\
&amp;= \exp\left{  -\frac{1}{2}\sum</em>{k=1}^K Z_{nk} \left(\mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[Y^2_n\right] - 2Y_n\mathbb{E}</em>{\mu \sim q(\mu | m, s^2)}\left[\mu_k\right] + \mathbb{E}<em>{\mu \sim q(\mu | m, s^2)}\left[\mu_k^2\right]\right)\right}\
&amp;= \exp\left{  -\frac{1}{2}\sum</em>{k=1}^K Z_{nk} \left(Y^2_n - 2Y_nm_k + (m_k + s_k^2)\right)\right}\
&amp;= \exp\left{  -\sum_{k=1}^K Z_{nk} Y^2_n  +\sum_{k=1}^KZ_{nk}\left( Y_nm_k - \frac{(m_k + s_k^2)}{2}\right) \right}\
&amp;= \exp\left{  -\sum_{k=1}^K Z_{nk} Y^2_n\right} \exp\left{ \sum_{k=1}^KZ_{nk}\left( Y_nm_k - \frac{(m_k + s_k^2)}{2}\right) \right}\
&amp;\propto \exp\left{\sum_{k=1}^KZ_{nk}\left( Y_nm_k - \frac{(m_k + s_k^2)}{2}\right) \right}
\end{aligned}</p>
<p>Since <span class="math notranslate nohighlight">\(q(Z_n | \phi_n)\)</span> is a categorical distribution, we can use the above to compute the <span class="math notranslate nohighlight">\(k\)</span>-th component as:</p>
<p>\begin{aligned}
\phi_{nk}\propto \exp\left{ Y_n m_k - \frac{(m_k + s_k^2)}{2} \right}
\end{aligned}</p>
</section>
<section id="update-rule-for-q-mu-k-m-k-s-k-2">
<h3>Update rule for <span class="math notranslate nohighlight">\(q(\mu_k | m_k, s_k^2)\)</span><a class="headerlink" href="#update-rule-for-q-mu-k-m-k-s-k-2" title="Permalink to this heading">#</a></h3>
<p>To update <span class="math notranslate nohighlight">\(q(\mu_k | m_k, s_k^2)\)</span>, we compute:
\begin{aligned}
q(\mu_k | m_k, s_k^2) &amp;\propto \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log p(Y_1, \ldots, Y_N, \psi)\right]\right}\
&amp;= \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log \left(\prod_{n=1}^N p(Y_n|\mu, Z_n) p(Z_n)\right) p(\mu)\right] \right}\
&amp;= \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log \left(\prod_{n=1}^N p(Y_n|\mu, Z_n) \right) p(\mu)\right]  +  \log \prod_{n=1}^N p(Z_n) \right}\
&amp;= \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log \left(\left(\prod_{n=1}^N p(Y_n|\mu, Z_n) \right)p(\mu)\right) \right] \right}  \exp\left{\log \prod_{m=1}^Mp(Z_m) \right}\
&amp;\propto \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log \left(\prod_{k=1}^K \prod_{n=1}^N p(Y_n|\mu_k, Z_n) \prod_{k=1}^K p(\mu_k)\right) \right] \right}\
&amp;= \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log \left(\prod_{n=1}^N p(Y_n|\mu_k, Z_n) p(\mu_k) \left(\prod_{j\neq k} \prod_{n=1}^N p(Y_n|\mu_j, Z_n) \prod_{j\neq k} p(\mu_j)\right) \right)\right] \right}\
&amp;= \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log \left(\prod_{n=1}^N p(Y_n|\mu_k, Z_n) p(\mu_k)\right) + \log \left(\prod_{j\neq k} \prod_{n=1}^N p(Y_n|\mu_j, Z_n)  \prod_{j\neq k} p(\mu_j)\right)\right] \right}\
&amp;= \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log \left(\prod_{n=1}^N p(Y_n|\mu_k, Z_n) p(\mu_k)\right)\right] + \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log \left(\prod_{j\neq k} \prod_{n=1}^N p(Y_n|\mu_j, Z_n)  \prod_{j\neq k} p(\mu_j)\right)\right] \right}\
&amp;= \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi)}\left[\log \left(\prod</em>{n=1}^N p(Y_n|\mu_k, Z_n) p(\mu_k)\right)\right] \right} \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi), q(\mu</em>{-k}|m_{-k}, s_{-k})}\left[\log \left(\prod_{j\neq k} \prod_{n=1}^N p(Y_n|\mu_j, Z_n)  \prod_{j\neq k} p(\mu_j)\right)\right] \right}\
&amp;\propto \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi)}\left[\log \left(\prod</em>{n=1}^N p(Y_n|\mu_k, Z_n) p(\mu_k)\right)\right] \right}
&amp;=\exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi)}\left[\sum</em>{n=1}^N \log p(Y_n|\mu_k, Z_n) + \log p(\mu_k)\right] \right}\
&amp;= \exp\left{ \mathbb{E}<em>{Z \sim q(Z | \phi)}\left[\sum</em>{n=1}^N \sum_{j=1}^K Z_{nj}\log \mathcal{N}(Y_n; \mu_j, 1) + \log \mathcal{N}(\mu_k; 0, s^2_0)\right] \right}\
&amp;= \exp\left{ \sum_{n=1}^N \sum_{j=1}^K \mathbb{E}<em>{Z \sim q(Z | \phi)}\left[Z</em>{nj}\right]\log \mathcal{N}(Y_n; \mu_j, 1) + \log \mathcal{N}(\mu_k; 0, s^2_0) \right}\
&amp;= \exp\left{ \sum_{n=1}^N \sum_{j=1}^K \phi_{nj}\log \mathcal{N}(Y_n; \mu_j, 1) + \log \mathcal{N}(\mu_k; 0, s^2_0) \right}\
&amp;= \exp\left{ \sum_{n=1}^N  \phi_{nk}\log \mathcal{N}(Y_n; \mu_k, 1) + \log \mathcal{N}(\mu_k; 0, s^2_0)  +  \sum_{n=1}^N \sum_{j\neq k} \phi_{nj}\log \mathcal{N}(Y_n; \mu_j, 1) \right}\
&amp;= \exp\left{ \sum_{n=1}^N  \phi_{nk}\log \mathcal{N}(Y_n; \mu_k, 1) + \log \mathcal{N}(\mu_k; 0, s^2_0)\right}  \exp\left{\sum_{n=1}^N \sum_{j\neq k} \phi_{nj}\log \mathcal{N}(Y_n; \mu_j, 1) \right}\
&amp;\propto \exp\left{ \sum_{n=1}^N  \phi_{nk}\log \mathcal{N}(Y_n; \mu_k, 1) + \log \mathcal{N}(\mu_k; 0, s^2_0)\right}\
&amp;= \exp\left{ \sum_{n=1}^N  \phi_{nk} \frac{(y_n-\mu_k)^2}{2} - \frac{\mu_k^2}{2\sigma_0^2} + const\right}\
&amp;= \exp\left{ \sum_{n=1}^N  \phi_{nk} \frac{(y_n-\mu_k)^2}{2} - \frac{\mu_k^2}{2\sigma_0^2}\right}\exp\left{const\right}\
&amp;\propto \exp\left{ \sum_{n=1}^N  \phi_{nk} \frac{(y_n-\mu_k)^2}{2} - \frac{\mu_k^2}{2\sigma_0^2}\right}\
&amp;= \exp\left{ \left(\sum_{n=1}^N  \phi_{nk} y_n\right)\mu_k - \frac{1}{2}\left(\sum_{n=1}^N  \phi_{nk} + \frac{1}{\sigma_0^2}\right)\mu^2_k + const\right}\
&amp;= \exp\left{ \left(\sum_{n=1}^N  \phi_{nk} y_n\right)\mu_k - \frac{1}{2}\left(\sum_{n=1}^N  \phi_{nk} + \frac{1}{\sigma_0^2}\right)\mu^2_k\right} \exp\left{ const\right}\
&amp;\propto \exp\left{ \underbrace{\left(\sum_{n=1}^N  \phi_{nk} y_n\right)}<em>{A}\mu_k - \underbrace{\frac{1}{2}\left(\sum</em>{n=1}^N  \phi_{nk} + \frac{1}{\sigma_0^2}\right)}_{B}\mu^2_k\right} \
&amp;= \exp\left{ - (B\mu_k^2 - A\mu_k)\right}\
&amp;= \exp\left{ - B\left(\mu_k^2 - \frac{A}{B}\mu_k + \left( \frac{A}{2B}\right)^2 - \left( \frac{A}{2B}\right)^2 \right)\right}\
&amp;= \exp\left{ - B\left(\mu_k^2 - \frac{A}{2B}\right)^2  - B\left( \frac{A}{2B}\right)^2 \right}\
&amp;= \exp\left{ - B\left(\mu_k^2 - \frac{A}{2B}\right)^2\right} \exp\left{- B\left( \frac{A}{2B}\right)^2 \right}\
&amp;\propto \exp\left{ - B\left(\mu_k^2 - \frac{A}{2B}\right)^2\right}
\end{aligned}</p>
<p>Thus, we see that <span class="math notranslate nohighlight">\(q(\mu_k | m_k, s_k^2) \propto \exp\left\{ - B\left(\mu_k^2 - \frac{A}{2B}\right)^2\right\} \)</span> is a Gaussian with mean</p>
<div class="math notranslate nohighlight">
\[m_k = \frac{A}{2B} = \frac{\sum_{n=1}^N \phi_{nk} y_n}{\sum_{n=1}^N \phi_{nk} + \frac{1}{\sigma_0^2}},\]</div>
<p>and variance</p>
<div class="math notranslate nohighlight">
\[s^2_k = \frac{1}{2B} = \frac{1}{\sum_{n=1}^N \phi_{nk} + \frac{1}{\sigma_0^2}}.\]</div>
</section>
</section>
<section id="implemenation-of-cavi-for-bayesian-gmm">
<h2><span class="section-number">2.13. </span>Implemenation of CAVI for Bayesian GMM<a class="headerlink" href="#implemenation-of-cavi-for-bayesian-gmm" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Generate data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">pis</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.</span><span class="o">/</span><span class="mi">3</span><span class="p">]</span>
<span class="n">mus</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">14</span><span class="p">]</span>
<span class="n">sigmas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">zs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">K</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pis</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mus</span><span class="p">[</span><span class="n">z</span><span class="p">],</span> <span class="n">sigmas</span><span class="p">[</span><span class="n">z</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">z</span> <span class="ow">in</span> <span class="n">zs</span><span class="p">])</span>

<span class="c1">#defining the bayesian model</span>
<span class="n">sigma_sq</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">**</span><span class="mi">2</span>
<span class="n">s_sq_0</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="c1">#initialization for CAVI</span>
<span class="n">m_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">s_sq_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>

<span class="c1">#implement coordinate ascent VI</span>
<span class="n">m_current</span> <span class="o">=</span> <span class="n">m_init</span>
<span class="n">s_sq_current</span> <span class="o">=</span> <span class="n">s_sq_init</span>

<span class="n">total_iter</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-10</span>
<span class="n">delta_m</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">delta_s</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1">#implementation of the ELBO</span>
<span class="k">def</span> <span class="nf">elbo</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s_sq</span><span class="p">,</span> <span class="n">phi</span><span class="p">):</span>
    <span class="n">summand_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">s_sq</span><span class="p">)</span> <span class="o">-</span> <span class="n">m</span> <span class="o">/</span> <span class="n">s_sq_0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">summand_2</span> <span class="o">=</span> <span class="p">((</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">s_sq</span> <span class="o">+</span> <span class="n">m</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">phi</span><span class="p">))</span> <span class="o">*</span> <span class="n">phi</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">summand_1</span> <span class="o">+</span> <span class="n">summand_2</span>

<span class="n">ELBOs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">total_iter</span> <span class="ow">and</span> <span class="n">delta_m</span> <span class="o">&gt;</span> <span class="n">threshold</span> <span class="ow">and</span> <span class="n">delta_s</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
    <span class="c1">#update q(Z_n)</span>
    <span class="n">exponent</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m_current</span><span class="p">)</span> <span class="o">+</span> <span class="o">-</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">m_current</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">s_sq_current</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">phi_unnormalized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">exponent</span><span class="p">)</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">phi_unnormalized</span> <span class="o">/</span> <span class="n">phi_unnormalized</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="c1">#update mu</span>
    <span class="n">m_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">phi</span> <span class="o">*</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">s_sq_0</span> <span class="o">+</span> <span class="n">phi</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1">#update s squared </span>
    <span class="n">s_sq_new</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">s_sq_0</span> <span class="o">+</span> <span class="n">phi</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1">#compute ELBO</span>
    <span class="n">ELBOs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elbo</span><span class="p">(</span><span class="n">m_new</span><span class="p">,</span> <span class="n">s_sq_new</span><span class="p">,</span> <span class="n">phi</span><span class="p">))</span>
    
    <span class="c1">#compute variational parameter change</span>
    <span class="n">delta_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">m_new</span> <span class="o">-</span> <span class="n">m_current</span><span class="p">)</span>
    <span class="n">delta_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">s_sq_new</span> <span class="o">-</span> <span class="n">s_sq_current</span><span class="p">)</span>
    
    <span class="n">m_current</span> <span class="o">=</span> <span class="n">m_new</span>
    <span class="n">s_sq_current</span> <span class="o">=</span> <span class="n">s_sq_new</span>
    
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    
<span class="c1">#sample from posterior predictive    </span>
<span class="k">def</span> <span class="nf">posterior_predictive_sampling</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
    <span class="n">posterior_predictive_samples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">variances</span><span class="p">):</span>
        <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
        <span class="n">posterior_predictive_samples</span> <span class="o">+=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">posterior_samples</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">posterior_predictive_samples</span><span class="p">)</span>
    
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;histogram of birth weights&#39;</span><span class="p">)</span>
<span class="n">posterior_predictive_samples</span> <span class="o">=</span> <span class="n">posterior_predictive_sampling</span><span class="p">(</span><span class="n">m_current</span><span class="p">,</span> <span class="n">s_sq_current</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">posterior_predictive_samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;variational posterior predictive&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;GMM for Birth Weights&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4c0db18261d3476928749d9bef4297f60dd9b5bca6b93cf78cc59e226b548338.png" src="../_images/4c0db18261d3476928749d9bef4297f60dd9b5bca6b93cf78cc59e226b548338.png" />
</div>
</div>
</section>
<section id="sanity-check-elbo-during-training">
<h2><span class="section-number">2.14. </span>Sanity Check: ELBO During Training<a class="headerlink" href="#sanity-check-elbo-during-training" title="Permalink to this heading">#</a></h2>
<p>Remember that ploting the posterior predictive against actual data is not always an option (e.g. high-dimensional data).</p>
<p>A sanity check for that your CAVI algorithm has been implemented correctly is to plot the ELBO (or alternatively, the observed data log-likelihood) over the iterations of the algorithm:</p>
<div class="math notranslate nohighlight">
\[
ELBO(\phi, m, s^2) = \mathbb{E}_{Z, \mu\sim q(Z, \mu | \phi, m, s^2)} \left[\log \left( \frac{p(Y_1, \ldots, Y_N, Z_1, \ldots, Z_N, \mu)}{q(Z, \mu | \phi, m, s^2)}\right) \right]
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ELBOs</span><span class="p">)),</span> <span class="n">ELBOs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ELBO over iterations of CAVI&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/354e82728c387e6f0aa30c003fbfb08f3d73f1e1c43a0ef2aa8929fa6183934e.png" src="../_images/354e82728c387e6f0aa30c003fbfb08f3d73f1e1c43a0ef2aa8929fa6183934e.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture_9_notes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">23. </span>Lecture #9: Latent Variable Models and MLE</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture_11_notes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Lecture #11: Hierarchical Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Lecture #10: Bayesian Latent Variable Models and Variational Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">1.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">1.2. Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-latent-variable-models">2. Bayesian Latent Variable Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.1. Bayesian Latent Variable Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-bayesian-inference">2.2. Challenges in Bayesian Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-idea-of-variational-inference">2.3. The Idea of Variational Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-design-of-the-variational-objective">2.4. The Design of the Variational Objective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference-as-optimization">2.5. Variational Inference as Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-of-the-elbo">2.6. Gradients of the ELBO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coordinate-ascent-variational-inference">2.7. Coordinate Ascent Variational Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximizing-the-elbo-via-coordinate-ascent">2.8. Maximizing the ELBO via Coordinate Ascent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-the-update-rule-for-q-psi-i-lambda-text-new-i">2.9. Proof of the Update Rule for <span class="math notranslate nohighlight">\(q(\psi_i | \lambda^{\text{new}}_i)\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-show-that-mathbb-e-psi-sim-q-psi-lambda-ldots-mathbb-e-psi-i-sim-q-psi-i-lambda-i-left-mathbb-e-psi-i-sim-q-psi-i-lambda-i-ldots-right">Step 1: Show that <span class="math notranslate nohighlight">\(\mathbb{E}_{\psi \sim q(\psi|\lambda)}[\ldots] = \mathbb{E}_{\psi_i \sim q(\psi_i|\lambda_i)}\left[\mathbb{E}_{\psi_{-i} \sim q(\psi_{-i}|\lambda_{-i})}[\ldots] \right]\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-show-that-underset-lambda-i-max-mathbb-e-psi-i-sim-q-psi-i-lambda-i-ldots-equiv-underset-lambda-i-min-d-text-kl-ldots">Step 2: Show that <span class="math notranslate nohighlight">\(\underset{\lambda_i}{\max}\mathbb{E}_{\psi_i \sim q(\psi_i|\lambda_i)}[\ldots] \equiv \underset{\lambda_i}{\min} D_{\text{KL}}[\ldots]\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-minimize-the-kl-divergence">Step 3: Minimize the KL-divergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-gaussian-mixture-models">2.10. Bayesian Gaussian Mixture Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference-for-bayesian-gaussian-mixture-models">2.11. Variational Inference for Bayesian Gaussian Mixture Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coordinate-ascent-variational-inference-updates-for-bayesian-gmm">2.12. Coordinate Ascent Variational Inference Updates for Bayesian GMM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-rule-for-q-z-n-phi-n">Update rule for <span class="math notranslate nohighlight">\(q(Z_n | \phi_n)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-rule-for-q-mu-k-m-k-s-k-2">Update rule for <span class="math notranslate nohighlight">\(q(\mu_k | m_k, s_k^2)\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implemenation-of-cavi-for-bayesian-gmm">2.13. Implemenation of CAVI for Bayesian GMM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sanity-check-elbo-during-training">2.14. Sanity Check: ELBO During Training</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yaniv Yacoby
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>