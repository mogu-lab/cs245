

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>31. Lecture #17: Black-box Variational Inference &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/lecture_17_notes';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/lectures/lecture_17_notes.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="35. Lecture #18: Automatic Differentiation" href="lecture_18_notes.html" />
    <link rel="prev" title="25. Lecture #16: Neural Network Models for Regression" href="lecture_16_notes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Probabilistic Foundations of Machine Learning (CS349), Fall 2024
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Exact Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_1_notes.html">1. Lecture #1: Course Overview</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_2_notes.html">7. Lecture #2: Maximimum Likelihood Estimation</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_3_notes.html">14. Lecture #3: Bayesian Modeling</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_4_notes.html">21. Lecture #4: Bayesian versus Frequentist Inference</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Sampling-Based Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_5_notes.html">1. Lecture #5: Sampling for Posterior Simulation</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_6_notes.html">7. Lecture #6: Monte Carlo Integration</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_7_notes.html">14. Lecture #7: Markov Chain Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_8_notes.html">18. Lecture #8: Metropolis-Hastings and Gibbs</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_9_notes.html">23. Lecture #9: Latent Variable Models and MLE</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Gradient-Based Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_10_notes.html">1. Lecture #10: Bayesian Latent Variable Models and Variational Inference</a></li>

<li class="toctree-l1"><a class="reference internal" href="lecture_11_notes.html">3. Lecture #11: Hierarchical Models</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_12_notes.html">7. Lecture #12: Logistic Regression and Gradient Descent</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_13_notes.html">11. Lecture #13: Stochastic Gradient Descent and Simulated Annealing</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_14_notes.html">17. Lecture #14: Hamiltonian Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_15_notes.html">21. Lecture #15: Parallel Tempering and Stochastic HMC</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_16_notes.html">25. Lecture #16: Neural Network Models for Regression</a></li>





<li class="toctree-l1 current active"><a class="current reference internal" href="#">31. Lecture #17: Black-box Variational Inference</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_18_notes.html">35. Lecture #18: Automatic Differentiation</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_19_notes.html">38. Lecture #19: Variational Inference in Context</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_20_notes.html">43. Lecture #20: Variational Autoencoders</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_21_notes.html">46. Lecture #21: Implementation of Variational Autoencoders</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/mogu-lab/cs349-fall-2024/blob/master/lectures/lecture_17_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li><a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fmogu-lab%2Fcs349-fall-2024%2Fblob%2Fmaster%2Flectures/lecture_17_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onDeepnote"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_deepnote.svg">
  </span>
<span class="btn__text-container">Deepnote</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/lecture_17_notes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture #17: Black-box Variational Inference</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">31. Lecture #17: Black-box Variational Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">31.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">31.2. Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-neural-network-models">32. Review of Neural Network Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-would-you-parameterize-a-non-linear-trend">32.1. How Would You Parameterize a Non-linear Trend?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-arbitrarily-complex-functions">32.2. Representing Arbitrarily Complex Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#design-choices-depth-or-width">32.3. Design Choices: Depth or Width</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-regression">32.4. Neural Networks Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-maximum-likelihood-objective-is-non-convex-for-neural-networks">32.5. The Maximum Likelihood Objective is Non-Convex for Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-regression-vs-linear-regression">32.6. Neural Network Regression vs Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-error-and-bias-variance">32.7. Generalization Error and Bias/Variance</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-neural-networks">33. Bayesian Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-polynomial-regression-is-bayesian-linear-regression">33.1. Bayesian Polynomial Regression is Bayesian Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">33.2. Bayesian Neural Networks</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#black-box-variational-inference">34. Black-box Variational Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-variational-inference">34.1. Review of Variational Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference-as-optimization">34.2. Variational Inference as Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference-for-bayesian-neural-networks">34.3. Variational Inference for Bayesian Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-the-gradient-of-the-elbo">34.4. Rewriting the Gradient of the ELBO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">34.5. Black-Box Variational Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-of-the-gradient-estimate">34.6. Variance of the Gradient Estimate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-the-elbo-with-the-reparametrization-trick">34.7. Gradient of the ELBO with the Reparametrization Trick</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#black-box-variational-inference-with-the-reparametrization-trick">34.8. Black-Box Variational Inference with the Reparametrization Trick</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bbvi-for-bayesian-linear-regression-posterior-predictives">34.9. BBVI for Bayesian Linear Regression (Posterior Predictives)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bbvi-for-bayesian-linear-regression-posteriors">34.10. BBVI for Bayesian Linear Regression (Posteriors)</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-17-black-box-variational-inference">
<h1><span class="section-number">31. </span>Lecture #17: Black-box Variational Inference<a class="headerlink" href="#lecture-17-black-box-variational-inference" title="Permalink to this heading">#</a></h1>
<section id="am-207-advanced-scientific-computing">
<h2><span class="section-number">31.1. </span>AM 207: Advanced Scientific Computing<a class="headerlink" href="#am-207-advanced-scientific-computing" title="Permalink to this heading">#</a></h2>
<section id="stochastic-methods-for-data-analysis-inference-and-optimization">
<h3>Stochastic Methods for Data Analysis, Inference and Optimization<a class="headerlink" href="#stochastic-methods-for-data-analysis-inference-and-optimization" title="Permalink to this heading">#</a></h3>
</section>
<section id="fall-2021">
<h3>Fall, 2021<a class="headerlink" href="#fall-2021" title="Permalink to this heading">#</a></h3>
<img src="fig/logos.jpg" style="height:150px;"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Import basic libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">autograd.numpy.random</span> <span class="k">as</span> <span class="nn">npr</span>
<span class="kn">import</span> <span class="nn">autograd.scipy.stats.multivariate_normal</span> <span class="k">as</span> <span class="nn">mvn</span>
<span class="kn">import</span> <span class="nn">autograd.scipy.stats.norm</span> <span class="k">as</span> <span class="nn">norm</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">from</span> <span class="nn">autograd.misc.optimizers</span> <span class="kn">import</span> <span class="n">adam</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.animation</span> <span class="k">as</span> <span class="nn">animation</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">rc</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1">### Import basic libraries</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">numpy</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">autograd.numpy.random</span> <span class="k">as</span> <span class="nn">npr</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="kn">import</span> <span class="nn">autograd.scipy.stats.multivariate_normal</span> <span class="k">as</span> <span class="nn">mvn</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;autograd&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">y_var</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;generate training data with a gap, testing data uniformly sampled&#39;&#39;&#39;</span>
    <span class="c1">#training x</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
    <span class="c1">#function relating x and y</span>
<span class="c1">#     f = lambda x:  0.01 * x**3</span>
    <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
    <span class="c1">#y is equal to f(x) plus gaussian noise</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">y_var</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">N</span><span class="p">)</span>

    <span class="c1">## generate testing data</span>
    <span class="c1">#nubmer of testing points</span>
    <span class="n">N_test</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1">#testing x</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">N_test</span><span class="p">)</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">y_var</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">N_test</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span>

<span class="k">def</span> <span class="nf">bayesian_polynomial_regression</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">prior_var</span><span class="p">,</span> <span class="n">y_var</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">poly_degree</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;visualize the posterior predictive of a Bayesian polynomial regression model&#39;&#39;&#39;</span>
    <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">poly_degree</span><span class="p">)</span>
    <span class="c1">#transform training x: add polynomial features</span>
    <span class="n">x_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="c1">#transform x_test: add polynomial features</span>
    <span class="n">x_test_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="c1">#Gaussian log pdf</span>
    <span class="n">gaussian_log_pdf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma_sq</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">sigma_sq</span><span class="p">)</span>

    <span class="c1">#reshape y into 2D array</span>
    <span class="n">y_matrix</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1">#define the covariance and precision matrices of the prior on the weights</span>
    <span class="n">prior_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">prior_var</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">)))</span>
    <span class="n">prior_precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">prior_variance</span><span class="p">)</span>

    <span class="c1">#defining the posterior variance</span>
    <span class="n">joint_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">prior_precision</span> <span class="o">+</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">y_var</span> <span class="o">*</span> <span class="n">x_poly</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_poly</span><span class="p">))</span>
    <span class="c1">#defining the posterior mean</span>
    <span class="n">joint_mean</span> <span class="o">=</span> <span class="n">joint_variance</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_poly</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_matrix</span><span class="p">))</span> <span class="o">*</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">y_var</span>

    <span class="c1">#sampling S points from the posterior</span>
    <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">joint_mean</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">joint_variance</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
    <span class="c1">#sampling S points from the posterior predictive</span>
    <span class="n">y_predict_noiseless</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_test_poly</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">posterior_samples</span><span class="p">])</span>
    <span class="n">y_predict_bayes</span> <span class="o">=</span> <span class="n">y_predict_noiseless</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">y_var</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">)))</span>
    
    <span class="c1">#compute log likelihood for the test data</span>
    <span class="n">log_likelihood_bayes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)):</span>
        <span class="n">log_likelihood_bayes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gaussian_log_pdf</span><span class="p">(</span><span class="n">y_predict_noiseless</span><span class="p">[:,</span> <span class="n">n</span><span class="p">],</span> <span class="n">y_var</span><span class="p">,</span> <span class="n">y_test</span><span class="p">[</span><span class="n">n</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">log_likelihood_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">log_likelihood_bayes</span><span class="p">)</span>
    
    <span class="c1">#compute the 95 percentiles of the posterior predictives</span>
    <span class="n">ub_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">y_predict_bayes</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">lb_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">y_predict_bayes</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1">#visualize the posterior predictive distribution</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train data&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">ub_bayes</span><span class="p">,</span> <span class="n">lb_bayes</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;test log-likelihood: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_likelihood_bayes</span><span class="p">),</span> <span class="mi">4</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;posterior predictive distribution of bayesian regression model with prior variance of </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prior_var</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">ax</span><span class="p">,</span> <span class="n">joint_variance</span><span class="p">,</span> <span class="n">joint_mean</span>

<span class="k">def</span> <span class="nf">black_box_variational_inference</span><span class="p">(</span><span class="n">logprob</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
<span class="w">    </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements http://arxiv.org/abs/1401.0118, and uses the</span>
<span class="sd">    local reparameterization trick from http://arxiv.org/abs/1506.02557</span>
<span class="sd">    code taken from:</span>
<span class="sd">    https://github.com/HIPS/autograd/blob/master/examples/black_box_svi.py</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">unpack_params</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="c1"># Variational dist is a diagonal Gaussian.</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">log_std</span> <span class="o">=</span> <span class="n">params</span><span class="p">[:</span><span class="n">D</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="n">D</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">log_std</span>

    <span class="k">def</span> <span class="nf">gaussian_entropy</span><span class="p">(</span><span class="n">log_std</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_std</span><span class="p">)</span>

    <span class="n">rs</span> <span class="o">=</span> <span class="n">npr</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">variational_objective</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Provides a stochastic estimate of the variational lower bound.&quot;&quot;&quot;</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">log_std</span> <span class="o">=</span> <span class="n">unpack_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">rs</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_std</span><span class="p">)</span> <span class="o">+</span> <span class="n">mean</span>
        <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">gaussian_entropy</span><span class="p">(</span><span class="n">log_std</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">logprob</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">lower_bound</span>

    <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">variational_objective</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">variational_objective</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">unpack_params</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">variational_inference</span><span class="p">(</span><span class="n">Sigma_W</span><span class="p">,</span> <span class="n">sigma_y</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">max_iteration</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;implements wrapper for variational inference via bbb for bayesian regression&#39;&#39;&#39;</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">Sigma_W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">Sigma_W_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_W</span><span class="p">)</span>
    <span class="n">Sigma_W_det</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">Sigma_W</span><span class="p">)</span>
    <span class="n">variational_dim</span> <span class="o">=</span> <span class="n">D</span>
    
    <span class="c1">#define the log prior on the model parameters</span>
    <span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
        <span class="n">constant_W</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">D</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Sigma_W_det</span><span class="p">))</span>
        <span class="n">exponential_W</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">Sigma_W_inv</span><span class="p">),</span> <span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">log_p_W</span> <span class="o">=</span> <span class="n">constant_W</span> <span class="o">+</span> <span class="n">exponential_W</span>
        <span class="k">return</span> <span class="n">log_p_W</span>

    <span class="c1">#define the log likelihood</span>
    <span class="k">def</span> <span class="nf">log_lklhd</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">constant</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma_y</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">*</span> <span class="n">N</span>
        <span class="n">exponential</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">sigma_y</span><span class="o">**-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span> <span class="o">-</span> <span class="n">forward</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x_train</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">constant</span> <span class="o">+</span> <span class="n">exponential</span>

    <span class="c1">#define the log joint density</span>
    <span class="n">log_density</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">log_lklhd</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_prior</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

    <span class="c1">#build variational objective.</span>
    <span class="n">objective</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">unpack_params</span> <span class="o">=</span> <span class="n">black_box_variational_inference</span><span class="p">(</span><span class="n">log_density</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">callback</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span>  <span class="n">t</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration </span><span class="si">{}</span><span class="s2"> lower bound </span><span class="si">{}</span><span class="s2">; gradient mag: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="n">objective</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradient</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">t</span><span class="p">))))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimizing variational parameters...&quot;</span><span class="p">)</span>
    <span class="c1">#initialize variational parameters</span>
    <span class="n">init_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="n">init_log_std</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="n">init_var_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">init_mean</span><span class="p">,</span> <span class="n">init_log_std</span><span class="p">])</span>
    
    <span class="c1">#perform gradient descent using adam (a type of gradient-based optimizer)</span>
    <span class="n">variational_params</span> <span class="o">=</span> <span class="n">adam</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">init_var_params</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="n">max_iteration</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">variational_params</span> 

<span class="k">def</span> <span class="nf">variational_polynomial_regression</span><span class="p">(</span><span class="n">Sigma_W</span><span class="p">,</span> <span class="n">sigma_y</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">posterior_sample_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">max_iteration</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;perform bayesian regression: infer posterior, visualize posterior predictive, compute log-likelihood&#39;&#39;&#39;</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">Sigma_W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1">#approximate posterior with mean-field gaussian</span>
    <span class="n">variational_params</span> <span class="o">=</span> <span class="n">variational_inference</span><span class="p">(</span><span class="n">Sigma_W</span><span class="p">,</span> <span class="n">sigma_y</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">max_iteration</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
    
    <span class="c1">#sample from the variational posterior</span>
    <span class="n">var_means</span> <span class="o">=</span> <span class="n">variational_params</span><span class="p">[:</span><span class="n">D</span><span class="p">]</span>
    <span class="n">var_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">variational_params</span><span class="p">[</span><span class="n">D</span><span class="p">:])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">var_means</span><span class="p">,</span> <span class="n">var_variance</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">posterior_sample_size</span><span class="p">)</span>

    <span class="c1">#predict on x_test</span>
    <span class="n">y_predict_noiseless</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">posterior_samples</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_predict_noiseless</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_y</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">posterior_sample_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">)))</span>

    <span class="c1">#Gaussian log pdf</span>
    <span class="n">gaussian_log_pdf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma_sq</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">sigma_sq</span><span class="p">)</span>

    <span class="c1">#compute log likelihood for the test data</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)):</span>
        <span class="n">log_likelihood</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gaussian_log_pdf</span><span class="p">(</span><span class="n">y_predict_noiseless</span><span class="p">[:,</span> <span class="n">n</span><span class="p">],</span> <span class="n">sigma_y</span><span class="p">,</span> <span class="n">y_test</span><span class="p">[</span><span class="n">n</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">log_likelihood</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1">#compute the 95 percentiles of the posterior predictives</span>
    <span class="n">ub_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">lb_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1">#visualize the posterior predictive distribution</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train data&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">ub_bayes</span><span class="p">,</span> <span class="n">lb_bayes</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test log-likelihood:</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">log_likelihood</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;mean-field Gaussian variational approximation of the posterior&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">ax</span><span class="p">,</span> <span class="n">var_variance</span><span class="p">,</span> <span class="n">var_means</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="outline">
<h2><span class="section-number">31.2. </span>Outline<a class="headerlink" href="#outline" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Review of Neural Network Models</p></li>
<li><p>Bayesian Neural Networks</p></li>
<li><p>Black-box Variational Inference</p></li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="review-of-neural-network-models">
<h1><span class="section-number">32. </span>Review of Neural Network Models<a class="headerlink" href="#review-of-neural-network-models" title="Permalink to this heading">#</a></h1>
<section id="how-would-you-parameterize-a-non-linear-trend">
<h2><span class="section-number">32.1. </span>How Would You Parameterize a Non-linear Trend?<a class="headerlink" href="#how-would-you-parameterize-a-non-linear-trend" title="Permalink to this heading">#</a></h2>
<p>In non-linear regression, we are interested in modeling observed outcome <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> as a non-linear function of observed covariates <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span>:
\begin{align}
\mu &amp;= g_{\mathbf{w}}(\mathbf{X}^{(n)})\
Y^{(n)}&amp;\sim \mathcal{N}(\mu, \sigma^2)
\end{align}
But it’s not easy to think of a function class <span class="math notranslate nohighlight">\(g(x)\)</span> can capture the trend in the data (e.g. polynomial or sinusoical)?
<img src="./fig/fig12.png" style='height:400px;'></p>
</section>
<section id="representing-arbitrarily-complex-functions">
<h2><span class="section-number">32.2. </span>Representing Arbitrarily Complex Functions<a class="headerlink" href="#representing-arbitrarily-complex-functions" title="Permalink to this heading">#</a></h2>
<p><strong>Motto:</strong> neural netoworks build up a complex function <span class="math notranslate nohighlight">\(g\)</span> by <strong>composing</strong> simple nonlinear functions. We represent neural networks as <strong>layered directed graphs</strong> where each node <span class="math notranslate nohighlight">\(i\)</span> in the <span class="math notranslate nohighlight">\(l\)</span>-th layer represents the function <span class="math notranslate nohighlight">\(f\left(\sum_{j}w^{l-1}_{ji} h^{l-1}_j\right)\)</span>, <span class="math notranslate nohighlight">\(h^{l-1}_j\)</span> being the hidden nodes from the <span class="math notranslate nohighlight">\(l-1\)</span>-th layer.</p>
<img src="./fig/fig5.png" style='height:300px;'>
<p>This is a <em><strong>neural network</strong></em>. We denote the weights of the neural network collectively by <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.
The non-linear function <span class="math notranslate nohighlight">\(f\)</span> is called the <em><strong>activation function</strong></em>.</p>
</section>
<section id="design-choices-depth-or-width">
<h2><span class="section-number">32.3. </span>Design Choices: Depth or Width<a class="headerlink" href="#design-choices-depth-or-width" title="Permalink to this heading">#</a></h2>
<p>Ideally, we want our architecture to be able to express a number of very complex functions, since we don’t know what is appropriate for our data. So what architecture is more effective for expressing complex functions?</p>
<table>
    <tr>
        <td>
            <img src="./fig/wide.jpg" style="height: 350px;" align="center"/>
        </td>
        <td>
            <img src="./fig/deep.jpg" style="height: 350px;" align="center"/>
        </td>
    </tr>
</table>
</section>
<section id="neural-networks-regression">
<h2><span class="section-number">32.4. </span>Neural Networks Regression<a class="headerlink" href="#neural-networks-regression" title="Permalink to this heading">#</a></h2>
<p><strong>Model for Regression:</strong> <span class="math notranslate nohighlight">\(Y^{(n)}\sim \mathcal{N}(\mu, \sigma^2)\)</span>, <span class="math notranslate nohighlight">\(\mu = g_\mathbf{W}(\mathbf{X}^{(n)})\)</span>, where <span class="math notranslate nohighlight">\(g_\mathbf{W}\)</span> is a neural network with parameters <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.</p>
<p><strong>Training Objective:</strong> find <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> to maximize the likelihood of our data. This is equivalent to minimizing the Mean Square Error,
$<span class="math notranslate nohighlight">\(
\min_{\mathbf{W}}\, \mathrm{MSE}(\mathbf{W}) = \frac{1}{N}\sum^N_{n=1} \left(y_n - g_\mathbf{W}(x_n)\right)^2
\)</span>$</p>
<p><strong>Optimizing the Training Objective:</strong> The main challenge of optimizing the objective is computing the gradient of a neural network. Luckily, the gradient can be computed in an algorithmic way using the chain rule, working from the output node <strong>backwards</strong> towards the input. For example, the derivative with respect to the hidden node <span class="math notranslate nohighlight">\(h^l_i\)</span> is:
$<span class="math notranslate nohighlight">\(
\frac{\partial \mathrm{MSE}}{\partial h^l_i} = \sum_{j}\underbrace{\frac{\partial \mathrm{MSE}}{\partial h^{l+1}_j}}_{\text{derivative from layer \)</span>l\quad<span class="math notranslate nohighlight">\(}} \underbrace{\frac{\partial h^{l+1}_j}{\partial h^{l}_i}}_{\text{derivative of \)</span>f\left(\sum_{k}w_k h_k\right)<span class="math notranslate nohighlight">\(}}
\)</span>$
Using this backwards gradient computation, we can optimize a neural network with respect to the MSE using <strong>gradient descent</strong>.</p>
</section>
<section id="the-maximum-likelihood-objective-is-non-convex-for-neural-networks">
<h2><span class="section-number">32.5. </span>The Maximum Likelihood Objective is Non-Convex for Neural Networks<a class="headerlink" href="#the-maximum-likelihood-objective-is-non-convex-for-neural-networks" title="Permalink to this heading">#</a></h2>
<p>Unfortunately, the likelihood and MSE functions for neural network regression models are not convex! This means that <strong>just because your gradient is zero, it doens’t mean you’ve optimized anything</strong>.</p>
<img src="./fig/loss_landscape.jpg" style="height: 350px;" align="center"/></section>
<section id="neural-network-regression-vs-linear-regression">
<h2><span class="section-number">32.6. </span>Neural Network Regression vs Linear Regression<a class="headerlink" href="#neural-network-regression-vs-linear-regression" title="Permalink to this heading">#</a></h2>
<p>Linear models are easy to interpret. Once we’ve found the MLE of the model parameters, we can formulate scientific hypotheses about the relationship between the outcome <span class="math notranslate nohighlight">\(Y\)</span> and the covariates <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>:</p>
<p>\begin{align}
\widehat{\text{income}} = 2 * \text{education (yr)} + 3.1 * \text{married} - 1.5 * \text{gaps in work history}
\end{align}</p>
<p>What do the weights of a neural network tell you about the relationship between the covariates and the outcome?
<img src="./fig/fig5.png" style='height:250px;'></p>
</section>
<section id="generalization-error-and-bias-variance">
<h2><span class="section-number">32.7. </span>Generalization Error and Bias/Variance<a class="headerlink" href="#generalization-error-and-bias-variance" title="Permalink to this heading">#</a></h2>
<p>Complex models have <em><strong>low bias</strong></em> – they can model a wide range of functions, given enough samples.</p>
<p>But complex models like neural networks can use their ‘extra’ capacity to explain non-meaningful features of the training data that are unlikely to appear in the test data (i.e. noise). These models have <em><strong>high variance</strong></em> – they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings.</p>
<table>
    <tr>
        <td>
            <img src="./fig/fig11.png" style="width: 380px;" align="center"/>
        </td>
        <td>
            <img src="./fig/fig12.png" style="width: 380px;" align="center"/>
        </td>
    </tr>
</table></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="bayesian-neural-networks">
<h1><span class="section-number">33. </span>Bayesian Neural Networks<a class="headerlink" href="#bayesian-neural-networks" title="Permalink to this heading">#</a></h1>
<section id="bayesian-polynomial-regression-is-bayesian-linear-regression">
<h2><span class="section-number">33.1. </span>Bayesian Polynomial Regression is Bayesian Linear Regression<a class="headerlink" href="#bayesian-polynomial-regression-is-bayesian-linear-regression" title="Permalink to this heading">#</a></h2>
<p>A Bayesian polynomial regression model uses a polynomial of degree <span class="math notranslate nohighlight">\(D\)</span> to capture the relationship between <span class="math notranslate nohighlight">\(X\in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<p>\begin{align}
\mathbf{W} &amp;\sim \mathcal{N}(0, \sigma^2_W \mathbf{I})\
\mu^{(n)}&amp;= w_0 + w_1 X^{(n)} + w_2 (X^{(n)})^2 + \ldots + w_D(X^{(n)})^D\
Y^{(n)} &amp;\sim \mathcal{N}(\mu^{(n)}, \sigma^2_Y)
\end{align}</p>
<p>Rather than considering the polynomial as a non-linear function of <span class="math notranslate nohighlight">\(X\)</span>, we can see it as a linear function of the vector <span class="math notranslate nohighlight">\(\mathbf{X} = [1, X, X^2, \ldots, X^D] \in \mathbb{R}^{D+1}\)</span>:</p>
<p>\begin{align}
\mathbf{W} &amp;\sim \mathcal{N}(0, \sigma^2_W \mathbf{I})\
\mu^{(n)}&amp;= \mathbf{W}^\top \mathbf{X}^{(n)}\
Y^{(n)} &amp;\sim \mathcal{N}(\mu^{(n)}, \sigma^2_Y)
\end{align}</p>
<p>This means that for a Bayesian polynomial regression model, the posterior <span class="math notranslate nohighlight">\(p(\mathbf{X} | \text{Data})\)</span> is a multivariate Gaussian (just as in the case of Bayesian linear regression, see HW#0).</p>
</section>
<section id="id1">
<h2><span class="section-number">33.2. </span>Bayesian Neural Networks<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>A <strong>Bayesian neural network (BNN)</strong> is a Bayesian model for regression that uses a neural network to capture the relationship between <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<p>\begin{align}
\mathbf{W} &amp;\sim \mathcal{N}(0, \sigma^2_W \mathbf{I})\
\mu^{(n)}&amp;= g_{\mathbf{W}}(\mathbf{X}^{(n)})\
Y^{(n)} &amp;\sim \mathcal{N}(\mu^{(n)}, \sigma^2_Y)
\end{align}</p>
<p>Unfortunately, the posterior of a neural network is multimodal and very complex, posing a challenge for samplers. Furthermore, training data for BNNs are typically large, this makes gradient-based samplers like HMC extremely inefficient – in every leap-frog iteration of HMC, the gradient <span class="math notranslate nohighlight">\(\frac{\partial U(q)}{\partial q}\)</span> requires an evaluation over the entire training data set.</p>
<p><strong>See course project papers:</strong></p>
<ol class="arabic simple">
<li><p>NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport</p></li>
<li><p>Stochastic Gradient Hamiltonian Monte Carlo</p></li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="black-box-variational-inference">
<h1><span class="section-number">34. </span>Black-box Variational Inference<a class="headerlink" href="#black-box-variational-inference" title="Permalink to this heading">#</a></h1>
<section id="review-of-variational-inference">
<h2><span class="section-number">34.1. </span>Review of Variational Inference<a class="headerlink" href="#review-of-variational-inference" title="Permalink to this heading">#</a></h2>
<p><strong>Goal:</strong> given a target posterior distribution <span class="math notranslate nohighlight">\(p(\psi | Y_1, \ldots, Y_N)\)</span>, <span class="math notranslate nohighlight">\(\psi \in \mathbb{R}^I\)</span> we want to find a distribution <span class="math notranslate nohighlight">\(q(\psi |\lambda^*)\)</span> in a family of distributions <span class="math notranslate nohighlight">\(Q = \{q(\psi |\lambda) | \lambda \in \Lambda \}\)</span> such that <span class="math notranslate nohighlight">\(q(\psi |\lambda^*)\)</span> best approximates <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p><strong>Design Choices:</strong> we need to choose:</p>
<p>A. <em><strong>(Variational family)</strong></em> a family <span class="math notranslate nohighlight">\(Q\)</span> of candidate distributions for approximating <span class="math notranslate nohighlight">\(p\)</span>. The members of <span class="math notranslate nohighlight">\(Q\)</span> are called the <em><strong>variational distributions</strong></em>.</p>
<p><strong>Our Choice:</strong>  we assume that the joint <span class="math notranslate nohighlight">\(q(\psi)\)</span> factorizes completely over each dimension of <span class="math notranslate nohighlight">\(\psi\)</span>, i.e. <span class="math notranslate nohighlight">\(q(\psi)= \prod_{i=1}^I q(\psi_i | \lambda_i)\)</span>. This is called the <em><strong>mean field assumption</strong></em>. What can go wrong with this design choice?</p>
<p>B. <em><strong>(Divergence measure)</strong></em> a divergence measure to quantify the difference between <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>.</p>
<p><strong>Our Choice:</strong>
$<span class="math notranslate nohighlight">\(D_{\text{KL}}(q(\psi | \lambda) \| p(\psi | Y_1, \ldots, Y_N)) = \mathbb{E}_{\psi \sim q(\psi|\lambda)}\left[\log\left( \frac{q(\psi | \lambda)}{p(\psi | Y_1, \ldots, Y_N)} \right) \right]\)</span>$
What can go wrong with this design choice?</p>
</section>
<section id="variational-inference-as-optimization">
<h2><span class="section-number">34.2. </span>Variational Inference as Optimization<a class="headerlink" href="#variational-inference-as-optimization" title="Permalink to this heading">#</a></h2>
<p>Let’s formalize variational inference for a target distribution <span class="math notranslate nohighlight">\(p(\psi)\)</span>: find a <span class="math notranslate nohighlight">\(q(\psi|\lambda^*)\)</span> such that</p>
<p>\begin{aligned}
\lambda^* = \underset{\lambda}{\text{argmin}}; D_{\text{KL}}(q(\psi|\lambda) | p(\psi|Y_1, \ldots, Y_N))) = \underset{\lambda}{\text{argmin}}; \mathbb{E}_{\psi \sim q(\psi|\lambda)}\left[\log\left(\frac{q(\psi | \lambda)}{p(\psi|Y_1, \ldots, Y_N))}\right) \right]
\end{aligned}</p>
<p>Recall that for EM, we had proved that minimizing the KL is equivalent to maximizing the ELBO (for which it is easier to compute the gradient). We will do the same here:</p>
<p>\begin{aligned}
\underset{\lambda}{\min}D_{\text{KL}}(q(\psi|\lambda) | p(\psi|Y_1, \ldots, Y_N))) \overset{\text{equiv}}{\equiv}&amp; \underset{\lambda}{\max} -D_{\text{KL}}(q(\psi|\lambda) | p(\psi|Y_1, \ldots, Y_N))) \
=&amp; \underset{\lambda}{\max} -\mathbb{E}<em>{\psi \sim q(\psi|\lambda)}\left[\log\left(\frac{q(\psi | \lambda)}{p(\psi|Y_1, \ldots, Y_N))} \right)\right] \
=&amp; \underset{\lambda}{\max}\underbrace{\mathbb{E}</em>{\psi \sim q(\psi|\lambda)}\left[\log\left(\frac{p(\psi, Y_1, \ldots, Y_N))}{q(\psi | \lambda)} \right)\right]}_{ELBO(\lambda)} \
&amp;- \log p(Y_1, \ldots, Y_N).
\end{aligned}</p>
<p>Thus, the variational objective can be rephrased as maximizing the <span class="math notranslate nohighlight">\(ELBO\)</span>.</p>
</section>
<section id="variational-inference-for-bayesian-neural-networks">
<h2><span class="section-number">34.3. </span>Variational Inference for Bayesian Neural Networks<a class="headerlink" href="#variational-inference-for-bayesian-neural-networks" title="Permalink to this heading">#</a></h2>
<p>Consider a Bayesian neural network:
\begin{align}
\mathbf{W} &amp;\sim \mathcal{N}(0, \sigma^2_W \mathbf{I})\
Y^{(n)} &amp;\sim \mathcal{N}(g_{\mathbf{W}}(\mathbf{X}^{(n)}), \sigma^2_Y)
\end{align}
and a mean-field Gaussian variational family:
$<span class="math notranslate nohighlight">\(
q(\mathbf{W} | \mu, \Sigma) = \mathcal{N}(\mathbf{W}; \mu, \Sigma)
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\Sigma$ is a diagonal matrix.</p>
<p>The ELBO is:
\begin{align}
ELBO(\mu, \Sigma) &amp;= \mathbb{E}<em>{\mathbf{W} \sim q(\mathbf{W} | \mu, \Sigma)} \left[ \log \left( \frac{p(\mathbf{W}) \prod</em>{n=1}^N p(Y^{(n)} | \mathbf{X}^{(n)}, \mathbf{W})}{q(\mathbf{W} | \mu, \Sigma)} \right) \right]\
&amp;= \mathbb{E}<em>{\mathbf{W} \sim q(\mathbf{W} | \mu, \Sigma)} \left[ \log \left( \frac{\mathcal{N}(\mathbf{W}; 0, \sigma^2_W \mathbf{I}) \prod</em>{n=1}^N \mathcal{N}(Y^{(n)}; g_{\mathbf{W}}(\mathbf{X}^{(n)}), \sigma^2_Y)}{\mathcal{N}(\mathbf{W}; \mu, \Sigma)} \right) \right].
\end{align}</p>
<p>To find the optimal variational parameters such that <span class="math notranslate nohighlight">\(\mu^*, \Sigma^* = \mathrm{argmax}\, ELBO(\mu, \Sigma)\)</span>, we need to compute the gradient of the ELBO:</p>
<p>\begin{aligned}
\nabla_{\mu, \Sigma},\underbrace{\mathbb{E}<em>{\mathbf{W} \sim q(\mathbf{W} | \mu, \Sigma)} \left[ \log \left( \frac{p(\mathbf{W}) \prod</em>{n=1}^N p(Y^{(n)} | \mathbf{X}^{(n)}, \mathbf{W})}{q(\mathbf{W} | \mu, \Sigma)} \right) \right]}_{ELBO(\mu, \Sigma)}.
\end{aligned}</p>
</section>
<section id="rewriting-the-gradient-of-the-elbo">
<h2><span class="section-number">34.4. </span>Rewriting the Gradient of the ELBO<a class="headerlink" href="#rewriting-the-gradient-of-the-elbo" title="Permalink to this heading">#</a></h2>
<p>To find the optimal variational parameters such that <span class="math notranslate nohighlight">\(\mu^*, \Sigma^* = \mathrm{argmax}\, ELBO(\mu, \Sigma)\)</span>, we need to compute the gradient of the ELBO:</p>
<p>\begin{aligned}
\nabla_{\mu, \Sigma}, ELBO(\mu, \Sigma) = \nabla_{\mu, \Sigma},\mathbb{E}<em>{\mathbf{W} \sim q(\mathbf{W} | \mu, \Sigma)} \left[ \log \left( \frac{p(\mathbf{W}) \prod</em>{n=1}^N p(Y^{(n)} | \mathbf{X}^{(n)}, \mathbf{W})}{q(\mathbf{W} | \mu, \Sigma)} \right) \right],
\end{aligned}</p>
<p>The main problem in computing this gradient is that we can’t take the gradient of a complex integral in closed form, and we can’t push the gradient inside the expectation to make the gradient computation easier (since the expectation is over a distribution that involves <span class="math notranslate nohighlight">\(\mu, \Sigma\)</span>). Recall that in Coordinate Ascent Variational Inference (CAVI), we avoided computing this gradient all together, but this required extensive model-specific derivations. Today, instead, we will adopt the approach of performing gradient descent taking MC estimates of expectations in the gradient.</p>
<p>But before we estimate the expectation with a sum, we need to push the gradient inside the expectation! To do so, we rewrite the expectation as an integral:</p>
<p>\begin{align}
\nabla_{\mu, \Sigma}, ELBO(\mu, \Sigma) = \nabla_{\mu, \Sigma}, \int_{\mathcal{W}} \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] q(\mathbf{W} | \mu, \Sigma) d\mathbf{W}
\end{align}</p>
<p>Now, in general, integration and differentiation <strong>do not commute</strong> meaning <span class="math notranslate nohighlight">\(\nabla \int \neq \int \nabla\)</span>. However, in this case, we leverage the properties of the functions inside the integrand to prove that:</p>
<p>\begin{align}
\nabla_{\mu, \Sigma}, ELBO(\mu, \Sigma) &amp;= \nabla_{\mu, \Sigma}, \int_{\mathcal{W}} \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] q(\mathbf{W} | \mu, \Sigma) d\mathbf{W}\
&amp;= \int_{\mathcal{W}} \nabla_{\mu, \Sigma}, \left( \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] q(\mathbf{W} | \mu, \Sigma) \right)d\mathbf{W}\
\end{align}</p>
<p>Using the product rule, we can differentiate the integrand:</p>
<p>\begin{align}
\nabla_{\mu, \Sigma}, ELBO(\mu, \Sigma) =&amp; \nabla_{\mu, \Sigma}, \int_{\mathcal{W}} \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] q(\mathbf{W} | \mu, \Sigma) d\mathbf{W}\
=&amp; \int_{\mathcal{W}} \nabla_{\mu, \Sigma}, \left( \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] q(\mathbf{W} | \mu, \Sigma) \right)d\mathbf{W}\
=&amp; \int_{\mathcal{W}} q(\mathbf{W} | \mu, \Sigma)\nabla_{\mu, \Sigma}, \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] + \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] \nabla_{\mu, \Sigma}, q(\mathbf{W} | \mu, \Sigma) d\mathbf{W}\
\end{align}</p>
<p>Now, we can do a bit of algebra to break up the integral into pieces and evaluate some easy pieces:</p>
<p>\begin{align}
\nabla_{\mu, \Sigma}, ELBO(\mu, \Sigma) =&amp;  \int_{\mathcal{W}} q(\mathbf{W} | \mu, \Sigma)\nabla_{\mu, \Sigma}, \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] + \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] \nabla_{\mu, \Sigma}, q(\mathbf{W} | \mu, \Sigma) d\mathbf{W}\
=&amp; \int_{\mathcal{W}} q(\mathbf{W} | \mu, \Sigma)\nabla_{\mu, \Sigma}, \left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] d\mathbf{W},\quad \text{(Distribute integral over sum)} \
&amp;+ \int_{\mathcal{W}}\left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] \nabla_{\mu, \Sigma}, q(\mathbf{W} | \mu, \Sigma) d\mathbf{W}\
=&amp; \int_{\mathcal{W}}  q(\mathbf{W} | \mu, \Sigma)\nabla_{\mu, \Sigma}, \log p(\mathbf{W}, \text{Data}) d\mathbf{W} - \int_{\mathcal{W}}q(\mathbf{W} | \mu, \Sigma)\nabla_{\mu, \Sigma}, \log q(\mathbf{W} | \mu, \Sigma)  d\mathbf{W} ,\quad \text{(Distribute over difference)}\
&amp;+ \int_{\mathcal{W}}\left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] \nabla_{\mu, \Sigma}, q(\mathbf{W} | \mu, \Sigma) d\mathbf{W}\
\end{align}</p>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[
\int_{\mathcal{W}}  q(\mathbf{W} | \mu, \Sigma)\nabla_{\mu, \Sigma}\, \log p(\mathbf{W}, \text{Data}) d\mathbf{W} = 0
\]</div>
<p>since <span class="math notranslate nohighlight">\(\nabla_{\mu, \Sigma}\, \log p(\mathbf{W}, \text{Data}) =\mathbf{0}\)</span> because <span class="math notranslate nohighlight">\(p(\mathbf{W}, \text{Data})\)</span> depends neither on <span class="math notranslate nohighlight">\(\mu\)</span> nor <span class="math notranslate nohighlight">\(\Sigma\)</span>.</p>
<p>Similarly, we can prove that</p>
<div class="math notranslate nohighlight">
\[
\int_{\mathcal{W}}q(\mathbf{W} | \mu, \Sigma)\nabla_{\mu, \Sigma}\, \log q(\mathbf{W} | \mu, \Sigma)  d\mathbf{W} =  \mathbb{E}_{\mathbf{W} \sim q(\mathbf{W} | \mu, \Sigma)}  \left[\nabla_{\mu, \Sigma}\,\log q(\mathbf{W} | \mu, \Sigma) \right] = 0
\]</div>
<p>The proof is not difficult, it only requires you to write out the gradient for <span class="math notranslate nohighlight">\(\nabla_{\mu, \Sigma}\,\log q(\mathbf{W} | \mu, \Sigma)\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mu, \Sigma}\,\log q(\mathbf{W} | \mu, \Sigma)  = \frac{\nabla_{\mu, \Sigma}\,q(\mathbf{W} | \mu, \Sigma)}{q(\mathbf{W} | \mu, \Sigma)}
\]</div>
<p>and then evaluate the expectation as an integral.</p>
<p>Finally, this mean that we are left with:</p>
<p>\begin{align}
\nabla_{\mu, \Sigma}, ELBO(\mu, \Sigma) =&amp; \int_{\mathcal{W}}\left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] \nabla_{\mu, \Sigma}, q(\mathbf{W} | \mu, \Sigma), d\mathbf{W}\
\end{align}</p>
<p>But the problem is that this doesn’t look like an expectation! So we can’t use MC estimatation to approximate this integral! Luckily, we observe the following algebra fact: since we know that
<span class="math notranslate nohighlight">\(
\nabla_{\mu, \Sigma}\,\log q(\mathbf{W} | \mu, \Sigma)  = \frac{\nabla_{\mu, \Sigma}\,q(\mathbf{W} | \mu, \Sigma)}{q(\mathbf{W} | \mu, \Sigma)}
\)</span>,
this gives us</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mu, \Sigma}\,q(\mathbf{W} | \mu, \Sigma) = \nabla_{\mu, \Sigma}\,\log q(\mathbf{W} | \mu, \Sigma)\cdot q(\mathbf{W} | \mu, \Sigma).
\]</div>
<p>That is, knowing the derivative of the log of a function allows us to compute the gradient of that function! This trick is called the <em><strong>log-derivative trick</strong></em>.</p>
<p>Now, if we replace the gradient of <span class="math notranslate nohighlight">\(q\)</span> in our integral with the gradient of log of <span class="math notranslate nohighlight">\(q\)</span>, we get</p>
<p>\begin{align}
\nabla_{\mu, \Sigma}, ELBO(\mu, \Sigma) =&amp; \int_{\mathcal{W}}\left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] \nabla_{\mu, \Sigma}, \log q(\mathbf{W} | \mu, \Sigma) \cdot q(\mathbf{W} | \mu, \Sigma), d\mathbf{W}\
\end{align}</p>
<p>and this integral can be written as an expectation:</p>
<p>\begin{align}
\nabla_{\mu, \Sigma}, ELBO(\mu, \Sigma) =&amp; \int_{\mathcal{W}}\left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] \nabla_{\mu, \Sigma}, \log q(\mathbf{W} | \mu, \Sigma) \cdot q(\mathbf{W} | \mu, \Sigma), d\mathbf{W} = \mathbb{E}<em>{\mathbf{W}\sim q(\mathbf{W} | \mu, \Sigma)}\left[\left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] \nabla</em>{\mu, \Sigma}, \log q(\mathbf{W} | \mu, \Sigma)\right]
\end{align}</p>
<p>Recombining the two log terms, we get:</p>
<p>\begin{align}
\nabla_{\mu, \Sigma}, ELBO(\mu, \Sigma) = \mathbb{E}<em>{\mathbf{W}\sim q(\mathbf{W} | \mu, \Sigma)}\left[\left[\log p(\mathbf{W}, \text{Data}) - \log q(\mathbf{W} | \mu, \Sigma) \right] \nabla</em>{\mu, \Sigma}, \log q(\mathbf{W} | \mu, \Sigma)\right] = \underbrace{\mathbb{E}<em>{\mathbf{W} \sim q(\mathbf{W} | \mu, \Sigma)}\left[ \nabla</em>{\mu, \Sigma}, q(\mathbf{W} | \mu, \Sigma) * \log \left( \frac{p(\mathbf{W}) \prod_{n=1}^N p(Y^{(n)} | \mathbf{X}^{(n)}, \mathbf{W})}{q(\mathbf{W} | \mu, \Sigma)} \right) \right]}_{\text{score function gradient}}
\end{align}</p>
<p>That is, we have successfully rewritten the gradient of the ELBO as an expectation of a gradient! The resulting expectation of a gradient is called the <em><strong>score function gradient</strong></em> of the ELBO. Finally, to approximate the gradient, we can perform MC estimation to estmiate the expectation and use <code class="docutils literal notranslate"><span class="pre">autograd</span></code> to compute <span class="math notranslate nohighlight">\( \nabla_{\mu, \Sigma}\, q(\mathbf{W} | \mu, \Sigma)\)</span>; and since the gradient is now easily approximated we can maximize the ELBO (minimize the negative ELBO) using gradient descent.</p>
</section>
<section id="id2">
<h2><span class="section-number">34.5. </span>Black-Box Variational Inference<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>The <em><strong>Black-box Variational Inference (BBVI)</strong></em> algorithm for BNN’s:
0. <strong>Initialization:</strong> pick an intial value <span class="math notranslate nohighlight">\(\mu^{(0)}, \Sigma^{(0)}\)</span></p>
<ol class="arabic simple">
<li><p><strong>Gradient Ascent:</strong> repeat:</p>
<ol class="arabic simple">
<li><p>Approximate the gradient
\begin{align}
\nabla_{\mu, \Sigma} , ELBO(\mu, \Sigma) &amp;= \mathbb{E}<em>{\mathbf{W} \sim q(\mathbf{W} | \mu, \Sigma)}\left[ \nabla</em>{\mu, \Sigma}, q(\mathbf{W} | \mu, \Sigma) * \log \left( \frac{p(\mathbf{W}) \prod_{n=1}^N p(Y^{(n)} | \mathbf{X}^{(n)}, \mathbf{W})}{q(\mathbf{W} | \mu, \Sigma)} \right) \right]\
&amp;\approx\underbrace{\frac{1}{S}\sum_{s=1}^S \nabla_{\mu, \Sigma}, \log q(\mathbf{W}^s | \mu, \Sigma) * \log \left( \frac{p(\mathbf{W}^s) \prod_{n=1}^N p(Y^{(n)} | \mathbf{X}^{(n)}, \mathbf{W}^s)}{q(\mathbf{W}^s | \mu, \Sigma)} \right)}_},
\end{align}
where <span class="math notranslate nohighlight">\(\mathbf{W}^s\sim q(\mathbf{W} | \mu^{\text{current}}, \Sigma^{\text{current}})\)</span>.</p></li>
<li><p>Update parameters <span class="math notranslate nohighlight">\((\mu^{\text{current}}, \Sigma^{\text{current}}) \leftarrow (\mu^{\text{current}}, \Sigma^{\text{current}}) + \eta * {\text{score function gradient}}\)</span></p></li>
</ol>
</li>
</ol>
</section>
<section id="variance-of-the-gradient-estimate">
<h2><span class="section-number">34.6. </span>Variance of the Gradient Estimate<a class="headerlink" href="#variance-of-the-gradient-estimate" title="Permalink to this heading">#</a></h2>
<p>In Black-Box Variational Inference, we estimate the gradient using Monte Carlo estimation (i.e. the score function gradient approximation):</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mu, \Sigma} \, ELBO(\mu, \Sigma) \approx \frac{1}{S} {\sum_{s=1}^S \nabla_{\mu, \Sigma}\,\log q(\mathbf{W}^s | \mu, \Sigma) * \log \left( \frac{p(\mathbf{W}^s) \prod_{n=1}^N p(Y^{(n)} | \mathbf{X}^{(n)}, \mathbf{W}^s)}{q(\mathbf{W}^s | \mu, \Sigma)} \right)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{W}^s\sim q(\mathbf{W} | \mu^{\text{current}}, \Sigma^{\text{current}})\)</span>.</p>
<p>As in the case with every MC estimate, we worry about variance. As it turns out, the score function gradient approximation has very high variance. This leads to slow convergence for gradient descent. To mitigate this, we need to employ a number of <strong>variance reduction methods</strong>.</p>
<p>In the original paper “Black-Box Variational Inference”, the authors implement control variates and Rao-Blackwellization.</p>
</section>
<section id="gradient-of-the-elbo-with-the-reparametrization-trick">
<h2><span class="section-number">34.7. </span>Gradient of the ELBO with the Reparametrization Trick<a class="headerlink" href="#gradient-of-the-elbo-with-the-reparametrization-trick" title="Permalink to this heading">#</a></h2>
<p>An alternative to using the log-derivative trick to computing the gradient of the ELBO is to use the reparametrization trick.</p>
<p>We note that since <span class="math notranslate nohighlight">\(q(\mathbf{W} | \mu, \Sigma) = \mathcal{N}(\mathbf{W};\mu, \Sigma )\)</span>, sampling <span class="math notranslate nohighlight">\(W\sim q(\mathbf{W} | \mu, \Sigma)\)</span> is equivalent to sampling <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \mathbf{I})\)</span> and then transforming the sample <span class="math notranslate nohighlight">\(\mathbf{W} = \epsilon^\top \Sigma^{1/2} + \mu\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> and <span class="math notranslate nohighlight">\(\Sigma\)</span> have the same dimensions.</p>
<p>Thus, we can rewrite the ELBO:
<img src="./fig/reparametrized_grad.png" style='height:300px;'></p>
</section>
<section id="black-box-variational-inference-with-the-reparametrization-trick">
<h2><span class="section-number">34.8. </span>Black-Box Variational Inference with the Reparametrization Trick<a class="headerlink" href="#black-box-variational-inference-with-the-reparametrization-trick" title="Permalink to this heading">#</a></h2>
<p>The <em><strong>Black-box Variational Inference (BBVI) with the reparametrization trick</strong></em> or <em><strong>Bayes By Backprop</strong></em> algorithm for BNN’s:
0. <strong>Initialization:</strong> pick an intial value <span class="math notranslate nohighlight">\(\mu^{(0)}, \Sigma^{(0)}\)</span></p>
<ol class="arabic simple">
<li><p><strong>Gradient Ascent:</strong> repeat:</p>
<ol class="arabic simple">
<li><p>Approximate the gradient
\begin{align}
\nabla_{\mu, \Sigma} , ELBO(\mu, \Sigma) \approx&amp; \small\frac{1}{S} \sum_{s=1}^S \nabla_{\mu, \Sigma} \log \left[p(\epsilon_s^\top \Sigma^{1/2} + \mu) \prod_{n=1}^N p(Y^{(n)} | \mathbf{X}^{(n)}, \epsilon_s^\top \Sigma^{1/2} + \mu)\right] \
&amp;+ \nabla_{\mu, \Sigma}\underbrace{-\mathbb{E}<em>{\mathbf{W} \sim \mathcal{N}(\mu, \Sigma )}\left[\log \mathcal{N}(\mathbf{W};\mu, \Sigma ) \right]}</em>{\text{Guassian entropy: has closed form}},
\end{align}
where <span class="math notranslate nohighlight">\(\epsilon_s \sim \mathcal{N}(0, \mathbf{I})\)</span>.</p></li>
<li><p>Update parameters <span class="math notranslate nohighlight">\((\mu^{\text{current}}, \Sigma^{\text{current}}) \leftarrow (\mu^{\text{current}}, \Sigma^{\text{current}}) + \eta * {\text{reparametrization gradient}}\)</span></p></li>
</ol>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##set random seed</span>
<span class="n">rand_state</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">rand_state</span><span class="p">)</span>

<span class="c1">##generate training data</span>
<span class="c1">#number of points in each of the two segments of the domain</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="c1">#output variance</span>
<span class="n">y_var</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">y_var</span><span class="p">)</span>

<span class="c1">##transform covariates for polynomial regression</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#transform x: add polynomial features</span>
<span class="n">x_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="c1">#transform x_test: add polynomial features</span>
<span class="n">x_test_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1">##define dimensions</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">x_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">x_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1">##define variances</span>
<span class="n">sigma_y</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">**</span><span class="mi">2</span>
<span class="n">weight_noise</span> <span class="o">=</span> <span class="mi">5</span><span class="o">**</span><span class="mi">2</span>
<span class="n">Sigma_W</span> <span class="o">=</span> <span class="n">weight_noise</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>

<span class="c1">##polynomial function</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x_poly</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="bbvi-for-bayesian-linear-regression-posterior-predictives">
<h2><span class="section-number">34.9. </span>BBVI for Bayesian Linear Regression (Posterior Predictives)<a class="headerlink" href="#bbvi-for-bayesian-linear-regression-posterior-predictives" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_sample_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1">##visualize the posterior predictive of a bayesian polynomial regression model</span>
<span class="c1">#prior variance</span>
<span class="n">prior_var</span> <span class="o">=</span> <span class="mi">5</span><span class="o">**</span><span class="mi">2</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">joint_variance</span><span class="p">,</span> <span class="n">joint_mean</span> <span class="o">=</span> <span class="n">bayesian_polynomial_regression</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">prior_var</span><span class="p">,</span> <span class="n">y_var</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">S</span><span class="o">=</span><span class="n">posterior_sample_size</span><span class="p">,</span> <span class="n">poly_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">var_variance</span><span class="p">,</span> <span class="n">var_mean</span> <span class="o">=</span> <span class="n">variational_polynomial_regression</span><span class="p">(</span><span class="n">Sigma_W</span><span class="p">,</span> <span class="n">sigma_y</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">posterior_sample_size</span><span class="o">=</span><span class="n">posterior_sample_size</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">max_iteration</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimizing variational parameters...
</pre></div>
</div>
<img alt="../_images/5a4ed04ee41011d3147442dcb70648211b9e61d52d2682c2d133f27770ba432a.png" src="../_images/5a4ed04ee41011d3147442dcb70648211b9e61d52d2682c2d133f27770ba432a.png" />
</div>
</div>
</section>
<section id="bbvi-for-bayesian-linear-regression-posteriors">
<h2><span class="section-number">34.10. </span>BBVI for Bayesian Linear Regression (Posteriors)<a class="headerlink" href="#bbvi-for-bayesian-linear-regression-posteriors" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#define points that include most of the probability mass of the pdf</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mf">0.33</span><span class="p">:</span><span class="mf">0.4</span><span class="p">:</span><span class="mf">.005</span><span class="p">,</span> <span class="mf">1.925</span><span class="p">:</span><span class="mf">2.025</span><span class="p">:</span><span class="mf">.005</span><span class="p">]</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="c1">#get the value of the target pdf at those points</span>
<span class="n">z_p</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">joint_mean</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">joint_variance</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>
<span class="c1">#get the value of the variational pdf at those points</span>
<span class="n">z_q</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">var_mean</span><span class="p">,</span> <span class="n">var_variance</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#plot the target density against variational densities</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z_p</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Reds&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z_p</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Reds&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z_q</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Approximate posterior (blue) vs actual posterior (red)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2815a4f62269da0c8c92208752d6d700a1cc0ec9614e485f039e7cc658df1822.png" src="../_images/2815a4f62269da0c8c92208752d6d700a1cc0ec9614e485f039e7cc658df1822.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##set random seed</span>
<span class="n">rand_state</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">rand_state</span><span class="p">)</span>

<span class="c1">##generate training data</span>
<span class="c1">#number of points in each of the two segments of the domain</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="c1">#output variance</span>
<span class="n">y_var</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">y_var</span><span class="p">)</span>

<span class="c1">##transform covariates for polynomial regression</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="c1">#transform x: add polynomial features</span>
<span class="n">x_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="c1">#transform x_test: add polynomial features</span>
<span class="n">x_test_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1">##define dimensions</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">x_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">x_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1">##define variances</span>
<span class="n">sigma_y</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">**</span><span class="mi">2</span>
<span class="n">weight_noise</span> <span class="o">=</span> <span class="mi">5</span><span class="o">**</span><span class="mi">2</span>
<span class="n">Sigma_W</span> <span class="o">=</span> <span class="n">weight_noise</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>

<span class="c1">##polynomial function</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x_poly</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_sample_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1">##visualize the posterior predictive of a bayesian polynomial regression model</span>
<span class="c1">#prior variance</span>
<span class="n">prior_var</span> <span class="o">=</span> <span class="mi">5</span><span class="o">**</span><span class="mi">2</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">joint_variance</span><span class="p">,</span> <span class="n">joint_mean</span> <span class="o">=</span> <span class="n">bayesian_polynomial_regression</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">prior_var</span><span class="p">,</span> <span class="n">y_var</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">S</span><span class="o">=</span><span class="n">posterior_sample_size</span><span class="p">,</span> <span class="n">poly_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">var_variance</span><span class="p">,</span> <span class="n">var_mean</span> <span class="o">=</span> <span class="n">variational_polynomial_regression</span><span class="p">(</span><span class="n">Sigma_W</span><span class="p">,</span> <span class="n">sigma_y</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">posterior_sample_size</span><span class="o">=</span><span class="n">posterior_sample_size</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">max_iteration</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimizing variational parameters...
</pre></div>
</div>
<img alt="../_images/1c6c7c79185acbc51808222a0cb365f6a49a20bcb8b63949d2c28da4963c33cc.png" src="../_images/1c6c7c79185acbc51808222a0cb365f6a49a20bcb8b63949d2c28da4963c33cc.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture_16_notes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">25. </span>Lecture #16: Neural Network Models for Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture_18_notes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">35. </span>Lecture #18: Automatic Differentiation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">31. Lecture #17: Black-box Variational Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">31.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">31.2. Outline</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-neural-network-models">32. Review of Neural Network Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-would-you-parameterize-a-non-linear-trend">32.1. How Would You Parameterize a Non-linear Trend?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-arbitrarily-complex-functions">32.2. Representing Arbitrarily Complex Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#design-choices-depth-or-width">32.3. Design Choices: Depth or Width</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-regression">32.4. Neural Networks Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-maximum-likelihood-objective-is-non-convex-for-neural-networks">32.5. The Maximum Likelihood Objective is Non-Convex for Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-regression-vs-linear-regression">32.6. Neural Network Regression vs Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-error-and-bias-variance">32.7. Generalization Error and Bias/Variance</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-neural-networks">33. Bayesian Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-polynomial-regression-is-bayesian-linear-regression">33.1. Bayesian Polynomial Regression is Bayesian Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">33.2. Bayesian Neural Networks</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#black-box-variational-inference">34. Black-box Variational Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-variational-inference">34.1. Review of Variational Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference-as-optimization">34.2. Variational Inference as Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference-for-bayesian-neural-networks">34.3. Variational Inference for Bayesian Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-the-gradient-of-the-elbo">34.4. Rewriting the Gradient of the ELBO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">34.5. Black-Box Variational Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-of-the-gradient-estimate">34.6. Variance of the Gradient Estimate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-the-elbo-with-the-reparametrization-trick">34.7. Gradient of the ELBO with the Reparametrization Trick</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#black-box-variational-inference-with-the-reparametrization-trick">34.8. Black-Box Variational Inference with the Reparametrization Trick</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bbvi-for-bayesian-linear-regression-posterior-predictives">34.9. BBVI for Bayesian Linear Regression (Posterior Predictives)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bbvi-for-bayesian-linear-regression-posteriors">34.10. BBVI for Bayesian Linear Regression (Posteriors)</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yaniv Yacoby
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>