

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>46. Lecture #21: Implementation of Variational Autoencoders &#8212; Probabilistic Foundations of Machine Learning (CS349)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/lecture_21_notes';</script>
    <link rel="canonical" href="https://mogu-lab.github.io/cs349-fall-2024/lectures/lecture_21_notes.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="43. Lecture #20: Variational Autoencoders" href="lecture_20_notes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Probabilistic Foundations of Machine Learning (CS349), Fall 2024
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Exact Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_1_notes.html">1. Lecture #1: Course Overview</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_2_notes.html">7. Lecture #2: Maximimum Likelihood Estimation</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_3_notes.html">14. Lecture #3: Bayesian Modeling</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_4_notes.html">21. Lecture #4: Bayesian versus Frequentist Inference</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Sampling-Based Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_5_notes.html">1. Lecture #5: Sampling for Posterior Simulation</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_6_notes.html">7. Lecture #6: Monte Carlo Integration</a></li>






<li class="toctree-l1"><a class="reference internal" href="lecture_7_notes.html">14. Lecture #7: Markov Chain Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_8_notes.html">18. Lecture #8: Metropolis-Hastings and Gibbs</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_9_notes.html">23. Lecture #9: Latent Variable Models and MLE</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models and Gradient-Based Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture_10_notes.html">1. Lecture #10: Bayesian Latent Variable Models and Variational Inference</a></li>

<li class="toctree-l1"><a class="reference internal" href="lecture_11_notes.html">3. Lecture #11: Hierarchical Models</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_12_notes.html">7. Lecture #12: Logistic Regression and Gradient Descent</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_13_notes.html">11. Lecture #13: Stochastic Gradient Descent and Simulated Annealing</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_14_notes.html">17. Lecture #14: Hamiltonian Monte Carlo</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_15_notes.html">21. Lecture #15: Parallel Tempering and Stochastic HMC</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_16_notes.html">25. Lecture #16: Neural Network Models for Regression</a></li>





<li class="toctree-l1"><a class="reference internal" href="lecture_17_notes.html">31. Lecture #17: Black-box Variational Inference</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture_18_notes.html">35. Lecture #18: Automatic Differentiation</a></li>


<li class="toctree-l1"><a class="reference internal" href="lecture_19_notes.html">38. Lecture #19: Variational Inference in Context</a></li>




<li class="toctree-l1"><a class="reference internal" href="lecture_20_notes.html">43. Lecture #20: Variational Autoencoders</a></li>


<li class="toctree-l1 current active"><a class="current reference internal" href="#">46. Lecture #21: Implementation of Variational Autoencoders</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/mogu-lab/cs349-fall-2024/blob/master/lectures/lecture_21_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li><a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fmogu-lab%2Fcs349-fall-2024%2Fblob%2Fmaster%2Flectures/lecture_21_notes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onDeepnote"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_deepnote.svg">
  </span>
<span class="btn__text-container">Deepnote</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/lecture_21_notes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture #21: Implementation of Variational Autoencoders</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">46.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">46.2. Outline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-structure-of-vae-implementation">46.3. Overall Structure of VAE Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-make-objective">46.4. Defining <code class="docutils literal notranslate"><span class="pre">.make_objective</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-log-likelihood">46.5. Defining <code class="docutils literal notranslate"><span class="pre">.log_likelihood</span></code></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-21-implementation-of-variational-autoencoders">
<h1><span class="section-number">46. </span>Lecture #21: Implementation of Variational Autoencoders<a class="headerlink" href="#lecture-21-implementation-of-variational-autoencoders" title="Permalink to this heading">#</a></h1>
<section id="am-207-advanced-scientific-computing">
<h2><span class="section-number">46.1. </span>AM 207: Advanced Scientific Computing<a class="headerlink" href="#am-207-advanced-scientific-computing" title="Permalink to this heading">#</a></h2>
<section id="stochastic-methods-for-data-analysis-inference-and-optimization">
<h3>Stochastic Methods for Data Analysis, Inference and Optimization<a class="headerlink" href="#stochastic-methods-for-data-analysis-inference-and-optimization" title="Permalink to this heading">#</a></h3>
</section>
<section id="fall-2021">
<h3>Fall, 2021<a class="headerlink" href="#fall-2021" title="Permalink to this heading">#</a></h3>
<img src="fig/logos.jpg" style="height:150px;"></section>
</section>
<section id="outline">
<h2><span class="section-number">46.2. </span>Outline<a class="headerlink" href="#outline" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Overall Structure of an VAE Implementation</p></li>
<li><p>Implementing the ELBO</p></li>
<li><p>Implementing the Log-Likelihood</p></li>
</ol>
</section>
<section id="overall-structure-of-vae-implementation">
<h2><span class="section-number">46.3. </span>Overall Structure of VAE Implementation<a class="headerlink" href="#overall-structure-of-vae-implementation" title="Permalink to this heading">#</a></h2>
<p>A VAE implementation requires us to keep track of two models: the generative model and the inference model. We need to update the parameters of these models during training, we will eventually need to evaluate the model by generating synthetic data and by computing the log-likelihood.</p>
<p>Passing the two models back and forth between functions might get complicated. So we choose to implement a VAE class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VAE</span><span class="p">:</span>
    <span class="c1"># Initialization </span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">generative_architecture</span><span class="p">,</span> <span class="n">inference_architecture</span><span class="p">):</span>
        
        <span class="c1"># We use the Feedforward neural network class to make the generative and inference models</span>
        <span class="c1"># 1. Initialize weights for the generative and inference models</span>
        <span class="c1"># 2. Instantiate generative model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generative</span> <span class="o">=</span> <span class="n">Feedforward</span><span class="p">(</span><span class="n">decoder_architecture</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">generative_weights</span><span class="p">)</span>
        <span class="c1"># 3. Instantiate inference model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inference</span> <span class="o">=</span> <span class="n">Feedforward</span><span class="p">(</span><span class="n">encoder_architecture</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">inference_weights</span><span class="p">)</span>
        <span class="c1"># 4. Do other initializations...</span>
        
    <span class="c1"># Define the variational objective: the negative ELBO, using S number of samples for the Monte Carlo estimte in the ELBO</span>
    <span class="k">def</span> <span class="nf">make_objective</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
        <span class="c1"># ...</span>
        <span class="k">return</span> <span class="n">variational_objective</span><span class="p">,</span> <span class="n">grad</span><span class="p">(</span><span class="n">variational_objective</span><span class="p">)</span>
    
    <span class="c1"># Generate synthetic data from the generative model</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="c1"># ...</span>
        <span class="k">return</span> <span class="n">synthetic_samples</span>
    
    <span class="c1"># Infer the variational parameters of the gaussian approximation of p(z|y) given y</span>
    <span class="k">def</span> <span class="nf">infer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># ...</span>
        <span class="k">return</span> <span class="n">variational_means</span><span class="p">,</span> <span class="n">variational_variances</span>
    
    <span class="c1"># Compute the log-likelihood of the data y using S number of samples for the Monte Carlo estimte in the log-likelihood</span>
    <span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="c1"># ...</span>
        <span class="k">return</span> <span class="n">lkhd</span>
    
    <span class="c1"># Fit the model given training data y, using S number of samples for the Monte Carlo estimte in the ELBO</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
        <span class="c1"># ...</span>
        <span class="k">return</span> <span class="kc">None</span>
        
</pre></div>
</div>
</section>
<section id="defining-make-objective">
<h2><span class="section-number">46.4. </span>Defining <code class="docutils literal notranslate"><span class="pre">.make_objective</span></code><a class="headerlink" href="#defining-make-objective" title="Permalink to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">.make_objective</span></code> function of the VAE class takes in parameters: <code class="docutils literal notranslate"><span class="pre">y</span></code>, the training data, and <code class="docutils literal notranslate"><span class="pre">S</span></code>, the number of samples used for the Monte Carlo estimte in the ELBO. This function returns the negative ELBO and the gradient of the ELBO function.</p>
<p>Recall that the ELBO for an VAE, whose generative model has parameters <span class="math notranslate nohighlight">\(w\)</span> and whose inference model has parameters <span class="math notranslate nohighlight">\(v\)</span>, is written as follows:</p>
<p>\begin{align}
\mathrm{ELBO}(w, q_v) &amp;= \sum_{n=1}^N \left[\mathbb{E}<em>{q_v}\left[\log p_w(y_n|z_n) \right] - \mathrm{D}</em>{\mathrm{KL}}\left[q_v(z_n)|p(z_n) \right]\right]\
&amp;= \sum_{n=1}^N \left[\mathbb{E}<em>{q_v}\left[\log p_w(y_n|z_n) \right] - \mathbb{E}</em>{q_v}\left[\log \frac{q_v(z_n)}{p(z_n)} \right]\right]\
&amp;= \sum_{n=1}^N \left[\mathbb{E}<em>{q_v}\left[\log p_w(y_n|z_n) \right] - \mathbb{E}</em>{q_v}\left[\log q_v(z_n)\right] + \mathbb{E}<em>{q_v} \left[{p(z_n)} \right]\right]\
&amp;\approx \sum</em>{n=1}^N \left[\frac{1}{S}\sum_{s=1}^S \log p_w(y_n|z^s_n)  - \frac{1}{S}\sum_{s=1}^S\log q_v(z^s_n) + \frac{1}{S}\sum_{s=1}^S {p(z^s_n)}\right], \quad z^s_n \sim q_v(z).
\end{align}</p>
<p>We see that we need to implement three terms:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{q_v}\left[\log p_w(y_n|z_n) \right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{q_v}\left[\log q_v(z_n)\right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{q_v} \left[{p(z_n)} \right]\)</span></p></li>
</ol>
<p>When we implement <span class="math notranslate nohighlight">\(\mathbb{E}_{q_v}\left[\log p_w(y_n|z_n) \right]\)</span>, we need to first sample <span class="math notranslate nohighlight">\(z_n\)</span> from the inference model given the current inference parameters, <span class="math notranslate nohighlight">\(q_v\)</span>. Recall that <span class="math notranslate nohighlight">\(q_v\)</span> is the Gaussian distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_v(y_n), \sigma_v(y_n))\)</span>, where <span class="math notranslate nohighlight">\(\mu_v(y_n)\)</span> and <span class="math notranslate nohighlight">\(\sigma_v(y_n)\)</span> are the variational mean and variance for the variational posterior for <span class="math notranslate nohighlight">\(y_n\)</span>.</p>
<p>Recall that <span class="math notranslate nohighlight">\(p_w(y_n|z_n)\)</span> is the Gaussian distribution <span class="math notranslate nohighlight">\(\mathcal{N}(y_n; f_w(z_n), \sigma_y^2I)\)</span>, where <span class="math notranslate nohighlight">\(f_w\)</span> is the generative model given the current generative parameters. So we will need to pass the samples <span class="math notranslate nohighlight">\(z_n\)</span> through the inference model and obtain <span class="math notranslate nohighlight">\(f_w(z_n)\)</span>, then we evaluate the Gaussian pdf, with mean <span class="math notranslate nohighlight">\(f_w(z_n)\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_y^2\)</span>, at <span class="math notranslate nohighlight">\(y_n\)</span>.</p>
<p>When we implement <span class="math notranslate nohighlight">\(\mathbb{E}_{q_v}\left[\log q_v(z_n)\right]\)</span>, we evaluate the Gaussian <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_v(x_n), \sigma_v(x_n))\)</span> at the samples <span class="math notranslate nohighlight">\(z_n\)</span>, drawn from the variational posterior <span class="math notranslate nohighlight">\(q_z\)</span> for <span class="math notranslate nohighlight">\(x_n\)</span>.</p>
<p>Finally, when we implement <span class="math notranslate nohighlight">\(\mathbb{E}_{q_v} \left[{p(z_n)} \right]\)</span>, we evaluate the standard Gaussian <span class="math notranslate nohighlight">\(\mathcal{N}(0, I)\)</span> at the samples <span class="math notranslate nohighlight">\(z_n\)</span>, drawn from the variational posterior <span class="math notranslate nohighlight">\(q_z\)</span> for <span class="math notranslate nohighlight">\(x_n\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_objective</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_var</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
           
        <span class="k">def</span> <span class="nf">variational_objective</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Definition of the ELBO</span>
<span class="sd">            params: the parameters of the generative and inference models concatenated into a single vecor</span>
<span class="sd">            t: the current iteration number, required by autograd</span>
<span class="sd">            &#39;&#39;&#39;</span>
            
            <span class="c1">#unpack the generative and inference model parameters</span>
            <span class="n">inference_weights</span><span class="p">,</span> <span class="n">generative_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unpack_weights</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

            <span class="c1">#infer z&#39;s: this returns the variational means and variances concatenated</span>
            <span class="n">z_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inference_weights</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            
            <span class="c1">#unpack the variational means and variances</span>
            <span class="n">mean</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unpack_params</span><span class="p">(</span><span class="n">z_params</span><span class="p">)</span>
            
            <span class="c1">#sample z&#39;s</span>
            <span class="n">z_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">std</span> <span class="o">+</span> <span class="n">mean</span>
            
            <span class="c1">#predict y&#39;s</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generative</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">decoder_weights</span><span class="p">,</span> <span class="n">z_samples</span><span class="p">)</span>
            
            <span class="c1">#evaluate term 1: log-likelihood</span>
            <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            
            <span class="c1">#evaluate term 2: evaluate sampled z&#39;s under variational distribution</span>
            <span class="n">log_qz_given_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">z_samples</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
                        
            <span class="c1">#evaluate term 3: evaluate sampled z&#39;s under prior</span>
            <span class="n">log_pz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">z_samples</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            
            <span class="c1">#compute the elbo</span>
            <span class="n">elbo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_likelihood</span> <span class="o">-</span> <span class="n">log_qz_given_y</span> <span class="o">+</span> <span class="n">log_pz</span><span class="p">)</span>
            
            <span class="c1">#return the negative elbo to be minimized</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">elbo</span>
            
            
        <span class="k">return</span> <span class="n">variational_objective</span><span class="p">,</span> <span class="n">grad</span><span class="p">(</span><span class="n">variational_objective</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="defining-log-likelihood">
<h2><span class="section-number">46.5. </span>Defining <code class="docutils literal notranslate"><span class="pre">.log_likelihood</span></code><a class="headerlink" href="#defining-log-likelihood" title="Permalink to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">.log_likelihood</span></code> function computes the log-likelihood of the observed data given a learned generative model <span class="math notranslate nohighlight">\(f_w\)</span>. Recall that the log-likelihood can be written as:
\begin{align}
\ell_v(y) &amp;= \frac{1}{N}\sum_{n=1}^N \log \left[\mathbb{E}<em>{z_n \sim p(z)} \mathcal{N}(y_n; f_w(z_n), \sigma_y^2I) \right]\
&amp;\approx \frac{1}{N}\sum</em>{n=1}^N \log \left[\frac{1}{S}\sum_{s=1}^S \mathcal{N}(y_n; f_w(z^s_n), \sigma_y^2I) \right], \quad z^s_n \sim p(z)
\end{align}</p>
<p>Since the Gaussian pdf will tend to be small, in higher dimensions, the log-likelihood will be numerically unstable to compute. We instead write the log-likelihood in an equivalent but more computationally stable way:</p>
<p>\begin{align}
\ell_v(y) &amp;\approx \sum_{n=1}^N \log \frac{1}{S}\sum_{s=1}^S \mathcal{N}(y_n; f_w(z^s_n), \sigma_y^2I) , \quad z^s_n \sim p(z)\
&amp;= \sum_{n=1}^N \log\frac{1}{S} + \sum_{n=1}^N \underbrace{\log \sum_{s=1}^S \mathrm{exp}}_{\texttt{log-sum-exp function}}\left{ \log \mathcal{N}(y_n; f_w(z^s_n), \sigma_y^2I)\right}, \quad z^s_n \sim p(z).
\end{align}
where the <span class="math notranslate nohighlight">\(\log \sum_{s=1}^S \mathrm{exp}\)</span> portion of the expression is computed using <code class="docutils literal notranslate"><span class="pre">scipy</span></code>’s <code class="docutils literal notranslate"><span class="pre">logsumexp</span></code> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="c1"># Define the covariance matrix for the Gaussian likelihood</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">y_var</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># We reshape the observations into a 3D array</span>
        <span class="n">y_tile</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">N</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
        <span class="c1"># We sample from the prior</span>
        <span class="n">z_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">S</span><span class="p">))</span>
        <span class="c1"># We feed the samples through the generative model </span>
        <span class="n">y_synthetic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generative</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generative</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">z_samples</span><span class="p">)</span>

        <span class="c1"># Compute the constant term of the log Gaussian pdf</span>
        <span class="n">const</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_dim</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">cov</span><span class="p">)))</span>
        <span class="c1"># Compute the exponential term of the log Gaussian pdf</span>
        <span class="n">exponential</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">y_synthetic</span> <span class="o">-</span> <span class="n">y_tile</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">cov</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Add the constant and the exponential terms</span>
        <span class="n">llkhd</span> <span class="o">=</span> <span class="n">const</span> <span class="o">+</span> <span class="n">exponential</span>
        <span class="c1"># Compute the log-likelihood using the logsumexp trick for numeric stability</span>
        <span class="n">lkhd</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">S</span><span class="p">)</span> <span class="o">+</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">llkhd</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture_20_notes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">43. </span>Lecture #20: Variational Autoencoders</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#am-207-advanced-scientific-computing">46.1. AM 207: Advanced Scientific Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-methods-for-data-analysis-inference-and-optimization">Stochastic Methods for Data Analysis, Inference and Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fall-2021">Fall, 2021</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">46.2. Outline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-structure-of-vae-implementation">46.3. Overall Structure of VAE Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-make-objective">46.4. Defining <code class="docutils literal notranslate"><span class="pre">.make_objective</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-log-likelihood">46.5. Defining <code class="docutils literal notranslate"><span class="pre">.log_likelihood</span></code></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yaniv Yacoby
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>