{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian Mixture Models (Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** Sometimes our data contains hidden structure---structure that we'd like to uncover in order to answer some scientific question. For example, recall the data set we analyzed in the unit on continuous probability from IHH’s Center for Telekinesis Research (CTR). The researchers at the IHH’s CTR study the propensity of intergalactic beings for telekinesis—the ability. They were interested in understanding how different physiological conditions affect a being’s telekinetic abilities. That is, they observed each patient's telekinetic-ability and wanted to understand how it related to some underlying condition (allergic reaction, intoxication, and entangled antennas). In their specific case, their data did contain the underlying condition. However, often times, our data doesn't contain this information. In such cases, our goal is to *uncover* the underlying types of patients. Doing so may help identify patients that benefit from different treatments. For example, in addition to each patient's underlying physiological condition, their telekinetic ability could have been impacted by environmental factors growing up, their genetics, etc. It's hard to know a priori which of these factors are truly important, so its not worth investing in collecting all this data (which is expensive). \n",
    "\n",
    "**Challenge:** But how can we possibly uncover a variable that's not in the data? By making assumptions about the distribution of this variable, as well as how it relates to the other variables in the data, we can! In statistical lingo, such *unobserved* variables are called *latent* variables. As we will show here, there's only one rule of probability we need to learn in order to use our existing toolkit to model latent variables. \n",
    "\n",
    "**Outline:** \n",
    "* Introduce latent variable models, as well as our first latent variable model (LVM)---the Gaussian Mixture Model (GMM)\n",
    "* Introduce the law of total probability (in the descrete case), which will allow us to compute the MLE for LVMs\n",
    "* Compute the MLE for the GMM\n",
    "* Implement a GMM in `NumPyro`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data:** We will start by modeling the data introduced in the chapter on continuous probability. The data includes two variables---the patient's telekinetic ability, and their underlying condition. We will *pretend* that we did not observe their underlying condition. Our goal will then be to *infer* it given their telekinetic ability. Let's remind ourselves what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Telekinetic-Ability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Allergic Reaction</td>\n",
       "      <td>0.510423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>Allergic Reaction</td>\n",
       "      <td>0.479960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>Intoxication</td>\n",
       "      <td>2.043218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>Allergic Reaction</td>\n",
       "      <td>-0.443333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>Intoxication</td>\n",
       "      <td>1.423190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>Intoxication</td>\n",
       "      <td>1.392568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>Intoxication</td>\n",
       "      <td>2.110151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>Intoxication</td>\n",
       "      <td>2.102866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Intoxication</td>\n",
       "      <td>1.865081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>Allergic Reaction</td>\n",
       "      <td>0.401414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4070</th>\n",
       "      <td>Intoxication</td>\n",
       "      <td>2.271342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Allergic Reaction</td>\n",
       "      <td>-0.455159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>Entangled Antennas</td>\n",
       "      <td>-1.713834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Intoxication</td>\n",
       "      <td>2.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>Intoxication</td>\n",
       "      <td>1.693633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Condition  Telekinetic-Ability\n",
       "Patient ID                                         \n",
       "398          Allergic Reaction             0.510423\n",
       "3833         Allergic Reaction             0.479960\n",
       "4836              Intoxication             2.043218\n",
       "4572         Allergic Reaction            -0.443333\n",
       "636               Intoxication             1.423190\n",
       "2545              Intoxication             1.392568\n",
       "1161              Intoxication             2.110151\n",
       "2230              Intoxication             2.102866\n",
       "148               Intoxication             1.865081\n",
       "2530         Allergic Reaction             0.401414\n",
       "4070              Intoxication             2.271342\n",
       "1261         Allergic Reaction            -0.455159\n",
       "4682        Entangled Antennas            -1.713834\n",
       "333               Intoxication             2.000120\n",
       "906               Intoxication             1.693633"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a bunch of libraries we'll be using below\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "csv_fname = 'data/IHH-CTR.csv'\n",
    "data = pd.read_csv(csv_fname, index_col='Patient ID')\n",
    "\n",
    "# Print a random sample of patients, just to see what's in the data\n",
    "data.sample(15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Models (LVMs)\n",
    "\n",
    "**Overview.** Latent variable models allow us to model variables we actually did not observe. We will do this using the very same toolkit we've used so far; we'll write down a joint distribution for all variables---observed and latent---as well as a directed graphical model. We will then perform MLE on the resultant model. We will instantiate everything with a specific model---Gaussian Mixture Model (GMM)---which will help us find patient types in the above IHH data.\n",
    "\n",
    "**Gaussian Mixture Models (GMMs).** In our IHH example, we assume the patients' underlying type in some way \"explains\" their observed data. We can encode this into a model by saying that each patient's data is generated by:\n",
    "1. Sampling their *patient type* from some distribution. We'll call the latent type $z$, and assume it's drawn from a Categorical distribution with parameter $\\pi$:\n",
    "    \\begin{align}\n",
    "    z_n &\\sim \\mathrm{Cat}(\\pi)\n",
    "    \\end{align}\n",
    "2. Given the type, we can now sample the *observed data*, $x$. As the name suggests, we'll set this distribution to be a Gaussian. The Gaussian's mean and variance will be selected by $z_n$. By this we mean that if the patient has underlying type 1 (i.e. $z_n = 1$), then their observed data is sampled from $\\mathcal{N}(\\mu_1, \\sigma^2_1)$. Similarly, if they have underlying type 2, their observed data is sampled from $\\mathcal{N}(\\mu_2, \\sigma^2_2)$. Putting this together, we have:\n",
    "    \\begin{align}\n",
    "    x_n | z_n &\\sim \\mathcal{N}(\\mu_{z_n}, \\sigma^2_{z_n})\n",
    "    \\end{align}\n",
    "\n",
    "For 1-dimensional $x$, the final data-generating process is then:\n",
    "\\begin{align}\n",
    "z_n &\\sim p_Z(\\cdot; \\pi) = \\mathrm{Cat}(\\pi) \\quad (\\text{mixture})\\\\\n",
    "x_n | z_n &\\sim p_{X | Z}(\\cdot | x_n; \\mu_0, \\dots, \\mu_{K-1}, \\sigma_0, \\dots, \\sigma_{K-1}) = \\mathcal{N}(\\mu_{z_n}, \\sigma^2_{z_n}) \\quad (\\text{components})\n",
    "\\end{align}\n",
    "The distribution over the latent variable is often called the \"mixture,\" and each Gaussian is called a \"component\" in the mixture. From here on, we'll use $\\theta = \\{ \\pi, \\mu_0, \\dots, \\mu_{K-1}, \\sigma_0, \\dots, \\sigma_{K-1}\\}$ to refer to the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directed Graphical Model.** Graphically, we can depict a GMM as follows:\n",
    "\n",
    "<div class=\"canva-centered-embedding\">\n",
    "  <div class=\"canva-iframe-container\">\n",
    "    <iframe loading=\"lazy\" class=\"canva-iframe\"\n",
    "      src=\"https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGIs1vP9i0&#x2F;-IWykjjF-dWy5DOBnqfudA&#x2F;view?embed\">\n",
    "    </iframe>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "As you can see, our observed data, $x_n$, depends on the latent patient type, $z_n$. Since $z_n$ is not observed, *its circle is left white* (i.e. not shaded in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are GMMs useful for?** To better understand what GMMs are useful for, let's visualize them. Here's an example GMM:\n",
    "\n",
    "```{figure} _static/figs/example_1d_gmm.png\n",
    "---\n",
    "name: fig-gmm-1d\n",
    "align: center\n",
    "---\n",
    "The PDF of a GMM's mixture components (left) and data marginal (right).\n",
    "```\n",
    "\n",
    "On the left, you can see the GMM's mixture components (i.e. each Gaussian), and on the right, you can see the probability of the *observed* data, $p_X(\\cdot; \\theta)$. Looking at the above figure, you can see two things:\n",
    "1. *Clustering.* Looking at the left plot, you can see that GMMs can \"cluster\" the observed data; every observation likely belongs to one of three Gaussians---we just need to figure out which observation belongs to which cluster.\n",
    "2. *Complicated Distributions.* Looking at the right plot, you can see that GMMs can describe more complicated distributions. Unlikely the continuous distributions we've used so far, which all have one mode (or one \"bump\"), using a GMM we can easily describe a distribution with multiple bumps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges Deriving the MLE for GMMs.** Now that we have our directed graphical model and our data generating process, we can try to derive the MLE for GMMs. Unfortunately, as you will see, we'll run into some issues. Then our joint data likelihood (which we'd like to maximize) is:\n",
    "\\begin{align}\n",
    "p(\\mathcal{D}; \\theta) &= \\prod\\limits_{n=1}^N p(\\mathcal{D}_n; \\theta) \\\\\n",
    "&= \\prod\\limits_{n=1}^N p_X(x_n; \\theta)\n",
    "\\end{align}\n",
    "Looking at the above, what is $p_X(x_n; \\theta)$? Our data-generating process gives us the following joint distribution:\n",
    "\\begin{align}\n",
    "p_{X | Z}(x_n, z_n; \\theta) &= p_{X | Z}(x_n | z_n; \\theta) \\cdot p_Z(z_n; \\theta)\n",
    "\\end{align}\n",
    "Somehow, we need to compute $p_X(x_n; \\theta)$ from $p_{X | Z}(x_n, z_n; \\theta)$. \n",
    "\n",
    "As we will show next, we can compute $p_X(x_n; \\theta)$ as follows:\n",
    "\\begin{align}\n",
    "p_X(x_n; \\theta) &= \\sum\\limits_{z_n \\in S} p_{X, Z}(x_n, z_n; \\theta),\n",
    "\\end{align}\n",
    "where $S = \\{0, \\dots, K - 1 \\}$ is the support of $Z$, and $K$ is the number of clusters. This formula shows that we can compute $p_X(x_n; \\theta)$ by summing the joint over every value of $z_n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Law of Total Probability (Discrete)\n",
    "\n",
    "**Definition.** Suppose you have two random variables, $A$ and $B$, and suppose that $A$ is discrete with support $S$. Then the law of total probability says we can compute the marginal $p_B(b)$ from the joint $p_{A, B}(a, b)$ as follows:\n",
    "\\begin{align}\n",
    "p_B(b) &= \\sum\\limits_{a \\in S} p_{A, B}(a, b)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition.** To get intuition, let's depict $A$ and $B$ as follows, each with support $S = \\{0, 1\\}$:\n",
    "\n",
    "<div class=\"canva-centered-embedding\">\n",
    "<div class=\"canva-iframe-container\">\n",
    "  <iframe loading=\"lazy\" class=\"canva-iframe\"\n",
    "    src=\"https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMdzJ3f3k&#x2F;PhWElQiQSXIIPklC6ED39w&#x2F;view?embed\">\n",
    "  </iframe>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "In this diagram, each shaded area represents the probability of an event---i.e. area is proportional to probability. The marginal probability of $B = 1$ is therefore the ratio of the blue square relative to the whole space (the gray square):\n",
    "\n",
    "<div class=\"canva-centered-embedding\">\n",
    "<div class=\"canva-iframe-container\">\n",
    "  <iframe loading=\"lazy\" class=\"canva-iframe\"\n",
    "    src=\"https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMdw0ybko&#x2F;3Aap1TJZMrHHD8kPFhfEzQ&#x2F;view?embed\">\n",
    "  </iframe>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Using the law of total probability, we can equivalently compute the marginal probability $p_B(1)$ as follows:\n",
    "\\begin{align}\n",
    "p_B(1) &= \\sum\\limits_{a \\in S} p_{A, B}(a, 1) \\\\\n",
    "&= p_{A, B}(0, 1) + p_{A, B}(1, 1) \\\\\n",
    "\\end{align}\n",
    "Re-writing this equation visually, we get:\n",
    "\n",
    "<div class=\"canva-centered-embedding\">\n",
    "<div class=\"canva-iframe-container\">\n",
    "  <iframe loading=\"lazy\" class=\"canva-iframe\"\n",
    "    src=\"https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAGMdzAtxTY&#x2F;406MzXJ_IH4tJuHLwoYFEg&#x2F;view?embed\">\n",
    "  </iframe>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the diagram, this formula holds. Now let's add to our pictoral intuition by assigning meaning to $A$ and $B$. Suppose $B = 1$ is the event in which a patient has pneumonia ($B = 0$ implies they don't have pneumonia), and suppose that $A = 1$ is the event of rain. We can attribute meaning to the law of total probability as follows:\n",
    "\\begin{align}\n",
    "\\underbrace{p_B(1)}_{\\text{has pneumonia}} &= \\underbrace{p_{A, B}(0, 1)}_{\\text{has pneumonia and no rain}} + \\underbrace{p_{A, B}(1, 1)}_{\\text{has pneumonia and rain}}\n",
    "\\end{align}\n",
    "Looking at this formula, we want to aggregate the probability of a patient having pneumonia across all possible scenarios---rain or no rain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood for GMMs\n",
    "\n",
    "**MLE Objective.** Using the law of total probability, we can now write our MLE objective:\n",
    "\\begin{align}\n",
    "\\theta^\\text{MLE} &= \\mathrm{argmax}_\\theta \\log p(\\mathcal{D}; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_\\theta \\log \\prod\\limits_{n=1}^N p(\\mathcal{D}_n; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_\\theta \\sum\\limits_{n=1}^N \\log p(\\mathcal{D}_n; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_\\theta \\sum\\limits_{n=1}^N \\log p(x_n; \\theta) \\\\\n",
    "&= \\mathrm{argmax}_\\theta \\sum\\limits_{n=1}^N \\log \\sum\\limits_{z_n \\in S} p(x_n, z_n; \\theta) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intractability.**\n",
    "\n",
    "**Expectations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate GMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMMs in `NumPyro`\n",
    "\n",
    "* Ask to generate samples\n",
    "* Ask to infer clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
